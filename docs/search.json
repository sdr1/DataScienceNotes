[
  {
    "objectID": "SQL-published-version.html",
    "href": "SQL-published-version.html",
    "title": "SQL",
    "section": "",
    "text": "First we’re going to create a few datasets that we’ll use to show what SQL can do\n\n\nhere() starts at /Users/stevenrashin/Documents/GitHub/DataScienceNotes\n\n\n\nsample_data &lt;- tibble(\n  id = 1:1000,\n  x1 = rnorm(1000, 0, 1),\n  x2 = rnorm(1000, 10, 15),\n  y = 3 * x1 + 4 * x2 + rnorm(1000, 0, 10),\n  g = rbinom(1000, size = 1, prob = 0.3)\n)\nsample_data2 &lt;- tibble(\n  id = 1:1000,\n  x3 = rnorm(1000, 15, 30),\n  x4 = rnorm(1000, 20, 5),\n  y2 = 10 * x3 + 40 * x4 + rnorm(1000, 0, 40),\n  g2 = rbinom(1000, size = 1, prob = 0.3)\n)\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbWriteTable(conn = mydb, name = \"sample\", value = sample_data, overwrite = T)\ndbWriteTable(conn = mydb, name = \"other_sample\", value = sample_data2, overwrite = T)\n\n#### Try to load in postgresql - doesn't currently work\n\n# https://caltechlibrary.github.io/data-carpentry-R-ecology-lesson/05-r-and-databases.html\n# https://jtr13.github.io/cc21fall2/how-to-integrate-r-with-postgresql.html\n# https://solutions.posit.co/connections/db/databases/postgresql/\n#https://medium.com/geekculture/a-simple-guide-on-connecting-rstudio-to-a-postgresql-database-9e35ccdc08be\n\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbListTables(mydb) # returns a list of tables in your database\n\ncharacter(0)"
  },
  {
    "objectID": "SQL-published-version.html#preliminaries",
    "href": "SQL-published-version.html#preliminaries",
    "title": "SQL",
    "section": "",
    "text": "First we’re going to create a few datasets that we’ll use to show what SQL can do\n\n\nhere() starts at /Users/stevenrashin/Documents/GitHub/DataScienceNotes\n\n\n\nsample_data &lt;- tibble(\n  id = 1:1000,\n  x1 = rnorm(1000, 0, 1),\n  x2 = rnorm(1000, 10, 15),\n  y = 3 * x1 + 4 * x2 + rnorm(1000, 0, 10),\n  g = rbinom(1000, size = 1, prob = 0.3)\n)\nsample_data2 &lt;- tibble(\n  id = 1:1000,\n  x3 = rnorm(1000, 15, 30),\n  x4 = rnorm(1000, 20, 5),\n  y2 = 10 * x3 + 40 * x4 + rnorm(1000, 0, 40),\n  g2 = rbinom(1000, size = 1, prob = 0.3)\n)\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbWriteTable(conn = mydb, name = \"sample\", value = sample_data, overwrite = T)\ndbWriteTable(conn = mydb, name = \"other_sample\", value = sample_data2, overwrite = T)\n\n#### Try to load in postgresql - doesn't currently work\n\n# https://caltechlibrary.github.io/data-carpentry-R-ecology-lesson/05-r-and-databases.html\n# https://jtr13.github.io/cc21fall2/how-to-integrate-r-with-postgresql.html\n# https://solutions.posit.co/connections/db/databases/postgresql/\n#https://medium.com/geekculture/a-simple-guide-on-connecting-rstudio-to-a-postgresql-database-9e35ccdc08be\n\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbListTables(mydb) # returns a list of tables in your database\n\ncharacter(0)"
  },
  {
    "objectID": "SQL-published-version.html#basic-syntax",
    "href": "SQL-published-version.html#basic-syntax",
    "title": "SQL",
    "section": "Basic Syntax",
    "text": "Basic Syntax\nThe basic syntax is you SELECT variables FROM a database.\n\\[\\underbrace{\\text{SELECT }}_{\\text{Select vars}} \\underbrace{\\text{*}}_{\\text{* is all variables}}\\]\n\\[\\underbrace{\\text{FROM }}_{\\text{from where}} \\underbrace{\\text{db\\_name}}_{\\text{name}}\\]\nSee e.g.,\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\nSELECT *\nFROM sample\n\nYou can run these commands in r using the following syntax. Since you can’t run the sql commands in quarto without compiling, I’ve used this method to check that my sql commands actually work.\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select *\n  from \"sample\"\n  limit 10\n')\n\n   id           x1         x2           y g\n1   1 -0.292322973  17.622036   79.569812 0\n2   2  1.103428094  -3.226083  -12.985231 0\n3   3  0.178622761  63.534728  245.547251 0\n4   4 -0.837106572  17.725579   50.542776 1\n5   5  1.219368664 -25.444891 -102.049791 0\n6   6  0.330327419   9.982296   44.160825 0\n7   7 -0.006858368  13.432985   44.459063 0\n8   8 -0.006800915  -5.060463    5.424679 1\n9   9 -0.142207830  23.659236   97.476765 0\n10 10  0.129038201   6.329206   25.812063 0\n\n\nThis can be modified (obviously!). Suppose you need two variables, x1 and x2.\n\nSELECT x1, x2\nFROM sample\n\n\nDisplaying records 1 - 10\n\n\nx1\nx2\n\n\n\n\n-0.2923230\n17.622036\n\n\n1.1034281\n-3.226083\n\n\n0.1786228\n63.534728\n\n\n-0.8371066\n17.725579\n\n\n1.2193687\n-25.444891\n\n\n0.3303274\n9.982296\n\n\n-0.0068584\n13.432985\n\n\n-0.0068009\n-5.060463\n\n\n-0.1422078\n23.659236\n\n\n0.1290382\n6.329206\n\n\n\n\n\nHere’s a more advanced query where we select rows where var_1 \\(&gt; 10\\)\n\nSELECT x1, x2\nFROM sample\nWHERE x2 &gt;= 10\n\n\nDisplaying records 1 - 10\n\n\nx1\nx2\n\n\n\n\n-0.2923230\n17.62204\n\n\n0.1786228\n63.53473\n\n\n-0.8371066\n17.72558\n\n\n-0.0068584\n13.43298\n\n\n-0.1422078\n23.65924\n\n\n-1.4434837\n29.94631\n\n\n-0.7600478\n34.21095\n\n\n-1.6875069\n18.19395\n\n\n-1.2902722\n25.84801\n\n\n-0.3086429\n19.90835\n\n\n\n\n\nOften you need summaries or operations by group var_1 \\(&gt; 10\\):\n\nSELECT avg(var1)\nFROM db_name\nGROUP BY group_var\nHAVING var2 &gt; 0\n\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select avg(\"x1\")\n  from \"sample\"\n  group by \"g\"\n  having x2 &gt;= 0\n')\n\n    avg(\"x1\")\n1 0.001398952\n2 0.008783227\n\n\nWhat is the difference? HAVING applies to groups as a whole whereas WHERE applies to individual rows. If you have both, the WHERE clause is applied first, the GROUP BY clause is second - so only the individual rows that meet the WHERE clause are grouped. The HAVING clause is then applied to the output. Then only the groups that meet the HAVING condition will appear.\nSuppose you need both:\n\nSELECT AVG(var_3)\nFROM db_name\nWHERE var1 &gt;= 10\nGROUP BY group_var\nHAVING var_2 &gt; 5\n\nAbove you’ll get the average of variable 3 from \\(db\\_name\\) only for the individual rows where \\(var\\_1\\) is greater than 10 grouped by group_var where, within the groups their associated \\(var\\_2\\) value is greater than 5.\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select avg(\"y\")\n  from \"sample\"\n  where \"x1\" &gt;= 3 \n  group by \"g\"\n  having x2 &gt;= 0\n')\n\n   avg(\"y\")\n1  43.20164\n2 158.00006"
  },
  {
    "objectID": "SQL-published-version.html#data-types",
    "href": "SQL-published-version.html#data-types",
    "title": "SQL",
    "section": "Data types",
    "text": "Data types\nHere are a few common data types. For a full list go to https://www.postgresql.org/docs/current/datatype.html.\n\nData Types\n\n\nData Type\nWhat does it do?\n\n\n\n\nint\nsigned four-byte integer\n\n\nnumeric\nexact number. use when dealing with money\n\n\nvarchar\nvariable-length character string\n\n\ntime\ntime of day (no time zone)\n\n\ntimestamp\ndate and time (no time zone)\n\n\ndate\ncalendar date (year, month, day)\n\n\n\nFor a technical discussion of the difference between float4 and float8 see this post: https://stackoverflow.com/questions/16889042/postgresql-what-is-the-difference-between-float1-and-float24."
  },
  {
    "objectID": "SQL-published-version.html#nulls",
    "href": "SQL-published-version.html#nulls",
    "title": "SQL",
    "section": "NULLS",
    "text": "NULLS\nUse IS NOT NULL to get rid of nulls. Usually used after the WHERE clause."
  },
  {
    "objectID": "SQL-published-version.html#aliasing",
    "href": "SQL-published-version.html#aliasing",
    "title": "SQL",
    "section": "Aliasing",
    "text": "Aliasing\nSometimes you need to alias variables. This is especially necessary when merging as you can overwrite columns that have the same name that aren’t explicitly part of the merge.\n\nSELECT var AS new_var_name\nFROM ...\n\nYou can also alias data frames - this is useful when you have multiple data frames.\n\nSELECT var AS new_var_name\nFROM df1 a \n\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select \"g\" as \"group\", ROUND(avg(\"y\"),2) as \"new_average\"\n  from \"sample\" \"b\"\n  group by \"g\"\n  having \"x2\" &gt;= 0\n')\n\n  group new_average\n1     0       39.29\n2     1       46.45"
  },
  {
    "objectID": "SQL-published-version.html#converting-data-types",
    "href": "SQL-published-version.html#converting-data-types",
    "title": "SQL",
    "section": "Converting Data Types",
    "text": "Converting Data Types\nSometimes the data is in one format and you need it in another. You can use CAST to do this\n\nCAST(variable AS int/numeric/varcar/time)\n\nSometimes you need to get rid of nulls, to do that use COALESCE\n\nCOALESCE(variable, 0)"
  },
  {
    "objectID": "SQL-published-version.html#extracting",
    "href": "SQL-published-version.html#extracting",
    "title": "SQL",
    "section": "Extracting",
    "text": "Extracting\nA lot of times when dealing with dates you’ll need a range or only part of the information given. To extract this data, you need the command extract.\nThis extracts a year from a date:\n\nEXTRACT(Year from date_var)\n\nThis extracts an epoch (i.e. the time difference) between the end date and the start date:\n\nEXTRACT(EPOCH from endvar-startvar)\n\nSuppose, however, that you only want the days in the epoch. That’s surprisingly easy with the following code:\n\nEXTRACT(Day from endvar-startvar)"
  },
  {
    "objectID": "SQL-published-version.html#sec-aggregate",
    "href": "SQL-published-version.html#sec-aggregate",
    "title": "SQL",
    "section": "Aggregate Functions",
    "text": "Aggregate Functions\nNote that in the table below all of the functions EXCEPT count ignore null values.\n\nAggregate Types\n\n\n\n\n\n\nAggregate Fcn\nWhat does it do?\n\n\n\n\nMIN()\nreturns the smallest value within the selected column\n\n\nMAX()\nreturns the largest value within the selected column\n\n\nCOUNT()\nreturns the number of rows in a set\n\n\nSUM()\nreturns the total sum of a numerical column\n\n\nAVG()\nreturns the mean value of a numerical column. Getting the median requires PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY var1). See below for more info\n\n\nGREATEST(var1, var2)\nGreatest rowwise among var1 and var2\n\n\nLEAST(var1, var2)\nLeast rowwise among var1 and var2\n\n\n\nBefore we go on, a brief digression on getting the median. For reasons known only to the creators of SQL, getting the median is fantastically difficult. Suppose you want the median as a decimal rounded to the second significant digit. You’d need to write ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY var1)::DECIMAL, 2).1\nThe functions MIN() to AVG() operate globally but sometimes you need the biggest/smallest out of a single record (i.e. locally/within a row). You can do that with GREATEST() and LEAST(). Note that these also work on characters. An example will help clarify.\nSuppose you want to see the number of flights between a pair of cities (e.g. Austin and Dallas) but you don’t care about where the plane begins. In this case the command CONCAT(departure_city, '-' arrival_city)2 will create 2 separate entries for Austin-Dallas and Dallas-Austin which is not what the question asked for. So you have to use GREATEST and LEAST.\nGreatest gets the highest alphabetically/greatest date/number out of a variable PER RECORD (i.e per row). Max gets the most OVER ALL RECORDS. Why does this difference matter? Suppose Abilene and Amarillo are in the data. Then if you used MAX() every row would be in the Abilene and Amarillo group.\nGoing back to our Dallas/Austin example, GREATEST(departure_city, arrival_city) would give us Austin and LEAST(departure_city, arrival_city) gives us Dallas in a row with a flight from Austin to Dallas. In a row with a flight from Austin to London the command would give us Austin and London. So to combine these to create a unique ID, we could type CONCAT(GREATEST(destination_location, source_location),'-',LEAST(destination_location, source_location)) and that would give us Austin-Dallas whenever the these two cities appeared in destination location and source location.\n\nPercentiles\nAs you could see from above, extracting the median is difficult. If you want a bunch of percentiles, the problem is even worse.\n\nSELECT \nUNNEST(array[0, 0.25, 0.5, 0.75, 1]) AS percentile, \nUNNEST(PERCENTILE_CONT(array[0, 0.25, 0.5, 0.75, 1]) within group (order by var1)) &lt;---- Note that unnest wraps the whole thing!  \nFROM ...\n\nYou have two options for calculating percentiles PERCENTILE_CONT and PERCENTILE_DISC. PERCENTILE_CONT will interpolate values while PERCENTILE_DISC will give you values only from your data. Suppose you have 2,3,4,5. PERCENTILE_CONT would give you 3.5, PERCENTILE_DISC gives you 3."
  },
  {
    "objectID": "SQL-published-version.html#merging",
    "href": "SQL-published-version.html#merging",
    "title": "SQL",
    "section": "Merging",
    "text": "Merging\nThere are four types of joins:\n\nINNER JOIN/JOIN\n\nJoins all common records between tables\n\nLEFT JOIN\n\nJoins the matching records in the right frame (i.e. the one after the LEFT JOIN clause) with all the records in the left frame (i.e. the one after the FROM clause)\n\nRIGHT JOIN\n\nJoins all of the records in the right frame (i.e. the one after the RIGHT JOIN clause) with all the matching records in the left frame (i.e. the one after the FROM clause)\n\nFULL JOIN\n\nAll the records in both frames\n\n\nThese joins are all on some variable.\n\nSELECT a.var1, a.var2, a.id, b.var3, b.var4, b.id1 &lt;---- note the aliases up here.  This is good practice to show where you're getting each variable from\nFROM df1 a &lt;---- alias dataframe 1 as a\nINNER JOIN df2 b &lt;----- alias dataframe 2 as b\nON a.id = b.id\n\nYou can use OR in the join to join on either value. AND works too.\n\nSELECT a.var1, a.var2, a.id, a.id2, b.var3, b.var4, b.id1, b.alt_id &lt;---- note the aliases up here.  This is good practice to show where you're getting each variable from\nFROM df1 a &lt;---- alias dataframe 1 as a\nINNER JOIN df2 b &lt;----- alias dataframe 2 as b\nON a.id = b.id OR a.var1 = b.alt_id &lt;---- this joins if EITHER is true\n\n\nUNION\n\nConcatenates queries. Does not allow duplicates\n\n\n\nSELECT ...\nFROM ...\nUNION\nSELECT ...\nFROM ...\n\n\nUNION ALL\n\nConcatenates queries. Allows duplicates"
  },
  {
    "objectID": "SQL-published-version.html#window-functions",
    "href": "SQL-published-version.html#window-functions",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\nSuppose you need to do something within a window like find all flights within 3 days. Here you need a window function. The basic syntax is as follows:\n\\(\\underbrace{\\dots}_{\\text{Some fcn}} \\text{OVER} (\\)\n\\(\\hspace{0.5cm}\\underbrace{\\text{PARTITION BY} {\\color{blue}{\\text{var1}}}}_{\\text{group by }{\\color{blue}{\\text{var1}}}}\\)\n\\(\\hspace{0.5cm}\\underbrace{\\text{ORDER BY} {\\color{green}{\\text{var2}}}}_{\\text{order by }{\\color{green}{\\text{var2}}}}\\) \\() \\text{ AS newvar}\\)\nIn addition to the functions in Section 8, here are a bunch of useful functions. For a more comprehensive list, go to https://www.postgresql.org/docs/current/functions-window.html\n\nWindow Functions\n\n\n\n\n\n\nWindow Function\nWhat does it do?\n\n\n\n\nlag()\nlags the data 1, lag(2) would lag 2 rows\n\n\nlead()\nopposite of lag()\n\n\nrank()\nranks rows. Suppose you have two rows tied for first, the rankings would go 1,1,3\n\n\ndense_rank()\nranks rows without skipping.Suppose you have two rows tied for first, the rankings would go 1,1,2\n\n\nntile()\nsplits the data into n groups, indexed by an integer, as equally as possible. ntile(4) for example, gives us quartiles if ordered properly\n\n\ncume_dist()\ncumulative distribution\n\n\n\n\nBounding window functions\nSometimes you need to search within a certain window in a group as opposed to within an entire group. Suppose we wanted a moving average within the last three years. We could do that by properly bounding our query. The bounds come after the ORDER BY clause.\n\nSELECT AVG(var1) OVER  &lt;---- give us the mean of variable 1\n (  \n PARTITION BY country &lt;---- group by country\n ORDER BY year desc &lt;---- order by year descending\n ROWS BETWEEN 2 PRECEDING AND CURRENT ROW &lt;---- gives us the last 3 years.  includes the current row.\n)\nFROM df1\n\nWe can be fairly creative with the bounds using the following building blocks:\n\n\n\n\n\n\n\nBounds\nWhat does it do?\n\n\n\n\nn PRECEDING\n2 Preceding gives us the 2 prior rows not including the current row\n\n\nUNBOUND PRECEDING\nAll rows up to but not including the current row\n\n\nCURRENT ROW\nJust the current row\n\n\nn PRECEDING AND CURRENT ROW\nn preceding and the current row\n\n\nn FOLLOWING\nn following but not including the current row\n\n\nUNBOUND FOLLOWING\nn preceding and the current row"
  },
  {
    "objectID": "SQL-published-version.html#conditionals",
    "href": "SQL-published-version.html#conditionals",
    "title": "SQL",
    "section": "Conditionals",
    "text": "Conditionals\nSometimes you don’t just need an average, you need a conditional average or sum. This can be done with the FILTER command\n\nsum(var) FILTER(WHERE ...) AS ...\n\nSometimes you need to create a new variable based on the values of other variables. You do that with CASE WHEN\n\nCASE \n  WHEN [condition] THEN [result]\n  WHEN [condition2] THEN [result2]\n  ELSE [result3] END AS new_conditional_variable  \n\nNote that the CASE WHEN ... can be used in the GROUP BY command to create groups."
  },
  {
    "objectID": "SQL-published-version.html#dates",
    "href": "SQL-published-version.html#dates",
    "title": "SQL",
    "section": "Dates",
    "text": "Dates\nDates are difficult to deal with. You just have to memorize these commands.\n\nDatetime Functions\n\n\n\n\n\n\nFunction\nWhat does it do?\n\n\n\n\nMAKE_DATE(year,month,day)\nmakes dates\n\n\nMAKE_TIMESTAMP(year,month,day, hour, minute, second)\nmakes timestamps\n\n\nMAKE_INTERVAL(year,month,day, hour, minute, second)\nmakes intervals\n\n\nDATE(datetime_var)\nextracts dates from datetimes\n\n\n\nThere are variations. Suppose you wanted to find all processes that lasted less than 10 days. You could use the command MAKE_INTERVAL(days &lt;= 10)"
  },
  {
    "objectID": "SQL-published-version.html#sec-cte",
    "href": "SQL-published-version.html#sec-cte",
    "title": "SQL",
    "section": "Common Table Expressions",
    "text": "Common Table Expressions\nSometimes you need to create a separate table that you can then extract data from to avoid conflicts like using a window function in the WHERE clause. You do this with a common table expression (CTE).\nThe basic syntax is as follows:\n\nWITH cte1 AS (\nSELECT ...\nFROM db1\n)\n\nSELECT ...\nFROM ... (likely db1 or db2)\n\nYou’re not limited to one CTE, you can have multiple if you want:\n\nWITH RECURSIVE cte1 AS (\nSELECT ...\nFROM db1\n), &lt;---- need the parenthesis and comma here otherwise you get an error!\ncte2 AS (\nSELECT ...\nFROM db2\n)\n\nSELECT ...\nFROM ... (likely db1 or db2)\n\nBelow is an an example of using CTEs to avoid a window function in the WHERE clause.3 The query is from a problem that asks you to find the second longest flight between two cities. This is a bit of a tricky problem because it requries a window function, concatentaion, ordering text strings, and a common table expression.\n\nWITH tmp AS (SELECT \nid, \ndestination_location,\nsource_location,\nflight_start,\nflight_end,\ndense_rank() OVER (\n    PARTITION BY CONCAT(GREATEST(destination_location, source_location),'.',LEAST(destination_location, source_location))\n    ORDER BY extract(epoch from flight_end - flight_start) desc\n) AS flight_duration\nFROM flights)\n\nSELECT id, destination_location, source_location, flight_start, flight_end\nFROM tmp\nwhere flight_duration = 2\norder by id\n\nLet’s go through this, starting with the structure. Here we’re creating a common table expression called cte and then using it\n\\(\\text{WITH } {\\color{red}{\\text{tmp}}} \\text{ AS (} \\leftarrow \\text{Create CTE called } \\color{red}{\\text{tmp}}\\)\n\\(\\hspace{1cm}\\text{SELECT} \\dots \\leftarrow \\text{Standard SQL commands in here}\\)\n\\(\\hspace{1cm}\\text{FROM} \\dots\\)\n\\() \\leftarrow \\text{Close out CTE}\\)\n\\(\\text{SELECT} \\dots\\)\n\\(\\text{FROM } {\\color{red}{\\text{tmp}}} \\leftarrow \\text{Use } {\\color{red}{\\text{tmp}}} \\text{ as a normal table}\\)\nThe other tricky bit is the window function. Here we’re using a dense rank (i.e. ranking everything sequentially so if two are tied for first the ranks are 1,1,2) by city pair (that’s the variable after partition by) ordering those groups by flight time descending."
  },
  {
    "objectID": "SQL-published-version.html#subqueries",
    "href": "SQL-published-version.html#subqueries",
    "title": "SQL",
    "section": "Subqueries",
    "text": "Subqueries\nSuppose you need a one off query within a query. You can use a sub query! The basic syntax is below. You can put them anywhere. For example, here’s one in the FROM clause. Note that you need to alias your subqueries or else they’ll fail\n\nSELECT ...\nFROM (\n  SELECT *\n  FROM db1\n) AS db2\n\nYou could, however, do one in the SELECT clause if you wanted\n\nSELECT (\n  SELECT ...\n  FROM ...\n)\nFROM db1\n\nSo when to use common table expressions versus subtables? Common table expressions are, generally, preferred because they’re more readable and can be used multiple times. See, e.g., below, where we havethe same query as Section 13 but with a subquery instead of a common table expression.\n\n\nSELECT \nid, \ndestination_location,\nsource_location,\nflight_start,\nflight_end\nFROM (\n    SELECT \n    *,  \ndense_rank() OVER (\n    PARTITION BY CONCAT(GREATEST(destination_location, source_location),'.',LEAST(destination_location, source_location))\n    ORDER BY extract(epoch from flight_end - flight_start) desc) AS duration_rank\n    FROM flights\n    ) AS subquery\n\nWHERE duration_rank = 2\norder by id\n\n\nSubquery Joins\nSometimes you need to do something to a database before joining it. Here a subquery join is helpful\n\nSELECT user_id, var1, var2 \nFROM  db1\nLEFT JOIN \n    ( &lt;---- begin subquery \n        SELECT id, var3, var4\n        FROM db2\n        WHERE ... &lt;-----  you're using a subquery because you need to do something so I've included the where in here \n    ) AS a &lt;--- end and alias subquery \nON db1.user_id = a.id &lt;---- join on subquery name!"
  },
  {
    "objectID": "SQL-published-version.html#sec-text",
    "href": "SQL-published-version.html#sec-text",
    "title": "SQL",
    "section": "Text",
    "text": "Text\nSometimes you’re faced with a task where you have to concatenate (i.e. join) or split variables. I’ve seen this problem when trying to match pairs of cities for flights when the question only cares about the pair and not the ordering.\n\\(\\text{CONCAT(var1, var2)}\\)\n\\(\\underbrace{\\text{SPLIT\\_PART}}_{\\text{split apart}}(\\underbrace{{\\color{red}{\\text{var}}, {\\color{blue}{\\text{'.'}}}, {\\color{green}{\\text{1}}}}}_{\\text{split apart}{\\color{red}{\\text{ var}}} \\text{ on}{\\color{blue}{\\text{ a period}}} \\text{ taking }{\\color{green}{\\text{the first instance}}} })\\)"
  },
  {
    "objectID": "SQL-published-version.html#random-sample",
    "href": "SQL-published-version.html#random-sample",
    "title": "SQL",
    "section": "Random Sample",
    "text": "Random Sample\nThe method below selects 10% of the data.\n\nSELECT ...\nFROM ...\nTABLESAMPLE BERNOULLI(10)"
  },
  {
    "objectID": "SQL-published-version.html#summary-statistics",
    "href": "SQL-published-version.html#summary-statistics",
    "title": "SQL",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nGetting summary statistics is a pain the ass. So here is some code that will do it for a variable called \\(a\\) in database \\(t1\\). The difficulty of the code is that it requires two separate common table expressions. Here’s a sketch of what that looks like. Note that adding RECURSIVE to the with query allows you to select from db1 in the db2 query.\n\nWITH RECURSIVE cte1 AS (\nSELECT ...\nFROM db1\n),\ncte2 AS (\nSELECT ...\nFROM db2\n)\n\nSELECT ...\nFROM ... (likely db1 or db2)\n\n\nWITH RECURSIVE\nsummary_stats AS\n(\n SELECT \n  ROUND(AVG(a), 2) AS mean,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY a) AS median,\n  MIN(a) AS min,\n  MAX(a) AS max,\n  MAX(a) - MIN(a) AS range,\n  ROUND(STDDEV(a), 2) AS standard_deviation,\n  ROUND(VARIANCE(a), 2) AS variance,\n  PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY a) AS q1,\n  PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY a) AS q3\n   FROM t1\n),\nrow_summary_stats AS\n(\nSELECT \n 1 AS id, \n 'mean' AS statistic, \n mean AS value \n  FROM summary_stats\nunion all\nSELECT \n 2, \n 'median', \n median \n  FROM summary_stats\nUNION\nSELECT \n 3, \n 'minimum', \n min \n  FROM summary_stats\nUNION\nSELECT \n 4, \n 'maximum', \n max \n  FROM summary_stats\nUNION\nSELECT \n 5, \n 'range', \n range \n  FROM summary_stats\nUNION\nSELECT \n 6, \n 'standard deviation', \n standard_deviation \n  FROM summary_stats\nUNION\nSELECT \n 7, \n 'variance', \n variance \n  FROM summary_stats\nUNION\nSELECT \n 9, \n 'Q1', \n q1 \n  FROM summary_stats\nUNION\nSELECT \n 10, \n 'Q3', \n q3 \n  FROM summary_stats\nUNION\nSELECT \n 11, \n 'IQR', \n (q3 - q1) \n  FROM summary_stats\nUNION\nSELECT \n 12, \n 'skewness', \n ROUND(3 * (mean - median)::NUMERIC / standard_deviation, 2) AS skewness \n  FROM summary_stats\n)\nSELECT * \n FROM row_summary_stats\n  ORDER BY id;"
  },
  {
    "objectID": "SQL-published-version.html#execution-order",
    "href": "SQL-published-version.html#execution-order",
    "title": "SQL",
    "section": "Execution Order",
    "text": "Execution Order\n\nJOIN\n\nIf no JOIN, then we start at FROM\n\nFROM\nWHERE\nGROUP BY\nHAVING\nSELECT\nDISTINCT\nORDER BY\nLIMIT/OFFSET\n\n\nMore Efficient Code\n\nhttps://nodeteam.medium.com/how-to-optimize-postgresql-queries-226e6ff15f72\nProperly index* columns used in WHERE and JOIN conditions.\n\nIndex will create a pointer to the actual rows in the specified table.\nhttps://www.postgresql.org/docs/current/sql-createindex.html\n\nUse appropriate data types and avoid unnecessary data type conversions.\nLimit the use of SELECT * and only retrieve the columns you need.\nMinimize the use of subqueries and consider JOIN alternatives.\nMonitor and analyze query performance using database-specific tools and profiling.”"
  },
  {
    "objectID": "SQL-published-version.html#footnotes",
    "href": "SQL-published-version.html#footnotes",
    "title": "SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you ask why, I’ll give you my favorite coding answer.↩︎\nSee Section 15 for more details↩︎\nSee https://www.interviewquery.com/questions/second-longest-flight for full problem details↩︎"
  },
  {
    "objectID": "OLS.html",
    "href": "OLS.html",
    "title": "Regression",
    "section": "",
    "text": "Get summary stats for the variables. Check for outliers or weird patterns\nPlot the independent variables against the dependent variable. Same goal as above.\nFigure out the model and standard errors - you most likely want robust or cluster robust standard errors\nRun the regression. If you have over 5 fixed effects (FEs), you can use a couple of approaches - if you don’t care about the FEs themselves, demean the dependent and independent variable of interest by group (i.e. \\(y_{\\text{demeaned}}=\\alpha + \\beta_1 x_\\text{1 demeaned} + \\beta_2 x_\\text{2 demeaned}\\)) , or use plm or some other package\nCheck regression diagnostics\n\nResiduals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary.\nNormal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required.\nScale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic.\nResiduals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model’s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots. You can also do this using the The Hat Matrix!\nCan also use https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html\n\nInterpret the coefficients (see Section 12)\n\nNo logged DV, no logged IV\n\nA one unit increase in x, increases y by coefficient (\\(\\beta\\)) units\n\nNo logged DV, logged IV\n\nDivide the coefficient by 100, this tells us that a 1% increase in the independent variable increases (decreases) the dependent variable by coefficient/100 units . For an 10% increase multiply the coefficient by log(1.1) (for x increase log(1.x)). Suppose our coefficient is 5. The interpretation is that a one percent increase in x increases the dependent variable by 0.05. For every 10% increase in the independent variable, the dependent variable increases by 5 * log(1.1) = 0.47\n\nLogged DV, No logged IV\n\nExponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the y for every one-unit increase in the independent variable. exp(coef) – 1) * 100. Suppose your coefficient is 0.5, (exp(0.5)-1)*100 = 64.9, a one unit increase in your independent variable increases your dependent variably by 64.9%. If our coefficient were 5, the result would be 14,741.32%. This is unlikely!\n\nLogged DV, logged IV\n\nInterpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. Suppose the coefficient is, again, 5. For every 1% increase in the independent variable, the dependent variable increases by 5%. Suppose, instead, we wanted an x percent increase - use the formula \\((1.x^5 – 1) * 100\\). For example, a 10% increase in our independent variable increases our dependent variable by \\((1.1^5 - 1) * 100 = 61.05\\%\\)\n\n\nPut regression into table form\n\n\nlibrary(ggfortify)\nlibrary(gridExtra)\n\n# simulate some data:\nx1 &lt;- rnorm(1000, 15, 5)\nx2 &lt;- rnorm(1000, 30, 10)\n#group &lt;- ifelse(x1&gt;50,1,0)\ny &lt;- 5 + x1 + x2 + rnorm(1000, 0, 10)\n\nsim_data &lt;- tibble(\n  x1 = x1,\n  x2 = x2,\n  y = y\n)\n\n# summaries of variables\nsim_data %&gt;%\n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx1\n0\n1\n15.11\n5.07\n0.82\n11.76\n15.10\n18.51\n36.16\n▂▇▇▂▁\n\n\nx2\n0\n1\n30.08\n10.45\n-2.99\n23.25\n30.23\n37.37\n62.21\n▁▃▇▅▁\n\n\ny\n0\n1\n50.13\n14.89\n3.78\n39.47\n50.24\n60.01\n93.30\n▁▅▇▅▁\n\n\n\n\n# plot variables against DV to check for weirdness\nsim_data %&gt;%\n  gather(key = \"other_variable\", value = \"value\",-y) %&gt;%\n  ggplot(., aes(x = value, y = y)) +\n  geom_point() +\n  facet_wrap(~other_variable)+\n  scale_color_viridis_d() +\n  labs(caption = \"Check that each x is linearly related to the y\")\n\n\n\n# model, can also do cluster robust\nstd &lt;- lm(y ~ x1 + x2)\nrobust &lt;- estimatr::lm_robust(y ~ x1+x2)\n\n# Output the diagnostic plots.  Look for weirdness (won't find it here because we're in a nice simualted sandbox)\n\nresidvfitted &lt;- ggplot2::autoplot(std, which = c(1)) + labs(caption = \"Residuals vs Fitted - checks linear relationship assumption \n        of linear regression. A linear relationship will demonstrate a\n        horizontal red line here. Deviations from a horizontal line\n        suggest nonlinearity and that a different approach may be\n        necessary.\")\n\nqq &lt;- ggplot2::autoplot(std, which = c(2)) + labs(caption = \"Normal Q-Q - checks whether or not the residuals (the difference\n        between the observed and predicted values) from the model are\n        normally distributed. The best fit models points fall along the\n        dashed line on the plot. Deviation from this line suggests that\n        a different analytical approach may be required.\")\n\nscalevlocation &lt;-   ggplot2::autoplot(std, which = c(3)) + labs(caption = \" Scale-Location - checks the homoscedasticity of the model. A\n        horizontal red line with points equally spread out indicates a\n        well-fit model. A non-horizontal line or points that cluster\n        together suggests that your data are not homoscedastic.\")\n        \nresidvleeverage &lt;- ggplot2::autoplot(std, which = c(4)) + labs(caption = \"Residuals vs Leverage - helps to identify outlier or extreme\n        values that may disproportionately affect the model's results.\n        Their inclusion or exclusion from the analysis may affect the\n        results of the analysis. Note that the top three most extreme\n        values are identified with numbers next to the points in all\n        four plots. You can also do this using the [The Hat Matrix!]\")\n\n \nresidvfitted + qq \n\n\n\nscalevlocation + residvleeverage\n\n\n\n# Lots of diagnostic options, see https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html\n\n# Model Summary for the output\nmodelsummary::modelsummary(std)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n6.517\n\n\n\n(1.333)\n\n\nx1\n0.978\n\n\n\n(0.062)\n\n\nx2\n0.959\n\n\n\n(0.030)\n\n\nNum.Obs.\n1000\n\n\nR2\n0.562\n\n\nR2 Adj.\n0.561\n\n\nAIC\n7421.8\n\n\nBIC\n7441.4\n\n\nLog.Lik.\n−3706.885\n\n\nF\n638.701\n\n\nRMSE\n9.85\n\n\n\n\n\n\nmodelsummary::modelsummary(robust)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n6.517\n\n\n\n(1.345)\n\n\nx1\n0.978\n\n\n\n(0.067)\n\n\nx2\n0.959\n\n\n\n(0.029)\n\n\nNum.Obs.\n1000\n\n\nR2\n0.562\n\n\nR2 Adj.\n0.561\n\n\nAIC\n7421.8\n\n\nBIC\n7441.4\n\n\nRMSE\n9.85\n\n\n\n\n\n\n\n\n\nThese are the sources I used to create this!\n\n\n\nA visual introduction to probability https://seeing-theory.brown.edu/\nConditional probability visual guide https://setosa.io/conditional/\nBiltzstein and Hwang’s probability textbook https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view\nEthan Bueno de Mesquita and Anthony Fowler’s Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis\nAll Models are Wrong (a good visual guide to OLS) https://allmodelsarewrong.github.io/\nSection 5.7 of https://jhudatascience.org/tidyversecourse/model.html#model-diagnostics\nThe bias-variance tradeoff http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\nA guide to OLS via matrices https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\nBrandon Stewart’s class on applied social science stats - this is the best econometrics class I’ve seen from a slide perspective https://bstewart.scholar.princeton.edu/soc500\nMauricio Romero’s lecture on Ordinary Least Squares https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf\nhttp://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf\nAngrist and Pischke’s Mostly Harmless Econometrics\nNotes on MHE with a good derivation of the law of iterated expectations https://www.leonardgoff.com/resources/mostlyharmlesslecturenotes.pdf"
  },
  {
    "objectID": "OLS.html#resources",
    "href": "OLS.html#resources",
    "title": "Regression",
    "section": "",
    "text": "These are the sources I used to create this!\n\n\n\nA visual introduction to probability https://seeing-theory.brown.edu/\nConditional probability visual guide https://setosa.io/conditional/\nBiltzstein and Hwang’s probability textbook https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view\nEthan Bueno de Mesquita and Anthony Fowler’s Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis\nAll Models are Wrong (a good visual guide to OLS) https://allmodelsarewrong.github.io/\nSection 5.7 of https://jhudatascience.org/tidyversecourse/model.html#model-diagnostics\nThe bias-variance tradeoff http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\nA guide to OLS via matrices https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf\nBrandon Stewart’s class on applied social science stats - this is the best econometrics class I’ve seen from a slide perspective https://bstewart.scholar.princeton.edu/soc500\nMauricio Romero’s lecture on Ordinary Least Squares https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf\nhttp://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf\nAngrist and Pischke’s Mostly Harmless Econometrics\nNotes on MHE with a good derivation of the law of iterated expectations https://www.leonardgoff.com/resources/mostlyharmlesslecturenotes.pdf"
  },
  {
    "objectID": "OLS.html#goals",
    "href": "OLS.html#goals",
    "title": "Regression",
    "section": "Goals",
    "text": "Goals\nThere are a few different ways to describe the goals of social science research:\n\nOur goal is to learn about the data generating process that generated the sample (Brandon Stewart). Another way DGP are also described as real world process(es) that creates/generates the data we’re interested in. In other words, the DGP describes the rules that created variation in the population itself and in our data. DGPs are usually unknown, unless you’re simulating.\nThe goal of quantitative social science is not limited to uncovering causally identified facts; the goal is to harness (many) pieces of evidence to obtain an inference to the best explanation—both within and across studies. (Stewart and Spirling)\nThe goal of statistical inference is to learn about the unobserved population distribution, which can be characterized by parameters. We want to estimate these population parameters. (What is a parameter? It’s a particular aspect of a population distribution)\n\nNote that sometimes the unobserved population distribution and the DGP are sometimes considered the same. Matt Blackwell and Brandon Stewart, for example, sometimes call the population distribution the data generating process (DGP)."
  },
  {
    "objectID": "OLS.html#sec-terms",
    "href": "OLS.html#sec-terms",
    "title": "Regression",
    "section": "Terms",
    "text": "Terms\nEstimands are the parameters that we aim to estimate (in other words “the estimand is the object of inquiry—it is the precise quantity about which we marshal data to draw an inference”). These are often written in Greek letters. Think of these as the truth. They are the result of the true data generating process.\nEstimators are functions of sample data (i.e. statistics/rules) which we use to learn about the estimands. They are often written using modified Greek letters (e.g., \\(\\hat{\\beta}\\)). These are procedures (e.g. means, variances, OLS). In concrete terms, Ordinary Least Squares is an estimator of some estimand, which was created by some data generating process (DGP).\nEstimates are particular values of estimators realized in a given sample. They are denoted by English letters (e.g., X, \\(x_i\\)) and are data from our sample. For example, the mean wealth of a person who completes college may be an estimate we care about.\nNote that, theoretically, estimate = estimand + bias + noise (see e.g. Fowler and Bueno de Mesquita’s book). Where bias is a systematic error and noise is an idiosyncratic one that is particular to any observation. So our population estimate is a function of the true estimand, systematic bias, and irreducible noise. Our goal is for the bias to be as close to 0 as possible. This, however, is hard!\n\nDGP \\(\\rightarrow\\) Sample \\(\\rightarrow\\) DGP\nPutting this together:\n\n(Unobserved) There is a data-generating process that creates the data/parameters we care about. E.g., some people get cancer because of a mutation, candidates for office spend money strategically to win elections, lobbyists argue that policy should change, etc… These processes create data that we’re interested in such as cancer rates/election spending/lobbying results\nWe decide to care about a population parameter - the estimand - the true effect of an intervention (e.g. the true effect of a cancer drug on mortality or the true effect of spending on campaign outcomes). ``Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g. a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not… The distinction between the theoretical and empirical estimands is subtle but important: the former may involve unobservable quantities such as counterfactuals while the latter involves only observable data.”\nWe then gather a sample of the population (or, maybe the whole population itself but then we say this population is a realization of a super-population), and measure the effect of this intervention on that sample. Say gather 1000 cancer patients or all congressional elections from 2000 - 2022.\nOnce we have sample, we select an estimator (e.g. mean, regression, etc…), which we use to learn about the estimand. Particular values of the estimator are estimates (e.g., population mean).\nIf we’ve done this well, we’re able to use our sample to talk about the population.\n\nSo we have: data \\(\\rightarrow\\) calculation \\(\\rightarrow\\) estimate \\(\\rightarrow\\) (maybe) truth\nSources:\n\nhttps://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture3handout.pdf\nhttps://twitter.com/nickchk/status/1272993322395557888\nhttps://jamanetwork.com/journals/jama/fullarticle/2783611\nhttps://doi.org/10.1177%2F00031224211004187\nFowler and Bueno de Mesquita Book\nWhat Good is a Regression? paper by Spirling and Stweart"
  },
  {
    "objectID": "OLS.html#ols-big-picture",
    "href": "OLS.html#ols-big-picture",
    "title": "Regression",
    "section": "OLS Big Picture",
    "text": "OLS Big Picture\n\nRegression provides the best linear predictor for the dependent variable in the same way that the conditional expectation function (CEF) is the best unrestricted predictor of the dependent variable\nIf we prefer to think of approximating \\(E(y_i|x_i)\\) as opposed to predicting \\(y_i\\), even if the CEF is nonlinear, regression provides the best linear approximation to it (Angrist and Pischke)\nIf the CEF is linear (i.e. if the process that produces the population distribution is linear), then it makes the most sense to use linear regression to estimate it. Restated, the population regression function is the best we can do in the class of all linear functions to approximate \\(E(y_i|x_i)\\) (Angrist and Pischke)\nLinear regression may be interesting even if the underlying CEF is not linear. \\(E(y_i|x_i)\\), is the minimum mean squared error predictor of \\(y_i\\) given \\(x_i\\) in the class of all functions of \\(x_i\\) (Angrist and Pischke)\n\\(\\beta_{OLS}\\) is an estimator of a parameter we do not observe\nThe standard error is the standard deviation of the estimator\nA confidence interval is an interval where we know with some probability the true estimate lives\nA p-value is the largest probability of obtaining results at least as extreme as those actually observed, under the assumption that the null hypothesis is correct.\nRegression anatomy helps us understand OLS as a “matching estimator” (try to compare observations that are alike in the Xs). (Note that this comes from MHE’s regression anatomy theorem. Suppose you have a multivariate regression \\(Y_i = \\beta_0 +\\beta_1X_{1i}+\\beta_2X_{2i}+\\epsilon_i\\) and two auxillary regressions of \\(X_{1i}\\) on \\(X_{2i}\\), and vice versa. The regression anatomy theorem states that \\(\\beta_1\\) captures the effect of \\(\\tilde{x}_{1i}\\) (the residuals from the regression of \\(X_{1i}\\) on \\(X_{2i}\\)) - the part of \\(X_{1i}\\) not explained by \\(X_2\\).). Similarly \\(\\beta_2\\) captures the effect of \\(\\tilde{x}_{2i}\\) (the residuals from the regression of \\(X_{2i}\\) on \\(X_{ii}\\)) - the part of \\(X_{2i}\\) not explained by \\(X_1\\).)\n\nSource\n\nhttps://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf - http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf"
  },
  {
    "objectID": "OLS.html#method-of-moments",
    "href": "OLS.html#method-of-moments",
    "title": "Regression",
    "section": "Method of Moments",
    "text": "Method of Moments\nThis is the method used in Scott Cunningham’s Causal Inference Mixtape and Angrist and Pischke’s Mostly Harmless Econometrics.\nThink of the population regression as a moment of the population distribution. This proceeds in two steps:\n\nFirst we derive an estimator for the population regression coefficient\nThen we replace it with the sample analog.\n\nLet’s assume that the data generating process in the sky follows the following form:\n\\[ y_i = \\beta_0 + \\beta_1x_i + u_i\\] That is, every y is a function of an intercept \\(\\beta_0\\), a coefficient \\(\\beta_1\\) on \\(x_i\\) and a random error \\(u_i\\). To make it concrete, assume that your earnings at age 40 are only systematically determined by your education and some random error \\(u_i\\). So nothing else matters systematically. That is, majoring in electrical engineering has the exact same returns as majoring in underwater basket weaving if they both take four years to complete. Except for a random error (e.g., sometimes electrical engineers decide to live as artists). This is a very strong assumption because we have to believe that everyone’s ability is the same.\nHow do we go from the above to a regression?\nWe need two assumptions, laid out below:\n\nWe need to assume \\(E(u_i)=0\\). That is, the expected value of any error term is 0. This doesn’t mean that every error term is 0 for each i. Quite the contrary! Some errors are positive, some are negative, but the mean of the distribution is 0.\nWe also need to assume \\(E(u_i|x_i)=E(u_i)\\) This assumption - mean independence - that states that the disturbances average out to 0 for any value of X. Put differently, no observations of the independent variables convey any information about the expected value of the disturbance.\n\nThis implies that: \\(E(u_i | x_i)=E(u_i)=0\\) and \\(E(u_ix_i)=E(E(u_i|x_i))=0\\). From these we can derive the population regression coefficients.\nAt a population level:\n\n\n\\[E[Y|X] = \\beta_0 + \\beta_1x_i\\] \\(E[Y|X]\\) is a population level regression function or conditional expectation function (i.e. mean). We solve for \\(\\beta_0\\) and \\(\\beta_1\\)\nSolving for \\(\\beta_0\\)\n\\[E[u_i|x_i] = E[y_i-\\beta_0-\\beta_1x_i] = 0\\] \\[E[y_i]-\\beta_0-E[\\beta_1x_i] = 0\\] Above, note that \\(\\beta_0\\) is a constant so \\(E[\\beta_0]=\\beta_0\\)\n\\[E[y_i]-\\beta_1 E[x_i] = \\beta_0\\] Now do \\(\\beta_1\\):\n\\[E[u_ix_i] = 0 \\] Replace \\(u_i\\) with \\(y-\\beta_0-\\beta_1x_i\\). A note about this proof - be careful with expectations and with minus signs\n\\[E[(y_i-\\beta_0-\\beta_1x_i)x_i] = 0  \\] We note \\(x_i\\) is a scaler, so we can move it to the front.\n\\[E [x_i(y_i-\\beta_0-\\beta_1 x_i)]= 0 \\] Now replace \\(\\beta_0\\) with what we derived above \\(E[y_i]-\\beta_1 E[x_i] = \\beta_0\\)\nRemember that since the term is \\(-\\beta_1\\) we need to distribute the minus!\n\\[E[x_i(y_i - E[y_i]+\\beta_1 E[x_i] - \\beta_1 x_i)] = 0 \\]\n\\[E[x_i(y_i - E[y_i] - \\beta_1 x_i  + \\beta_1 E[x_i]) ] = 0 \\] Factor out \\(\\beta_1\\). Watch the minus sign on the \\(E[x_i]\\)!\n\\[E[x_i(y_i - E[y_i] - \\beta_1 (x_i  - E[x_i])) ] = 0 \\] \\[E[x_i(y_i - E[y_i])] = \\beta_1 E [x_i  (x_i  - E[x_i]) ] \\] On the right hand side, we note that \\(E[(x_i - E[x_i])^2] = E[x_i^2 - E[x]^2]\\). Given that this took me a few hours to figure out, here’s the proof. Note that we start with \\(E[(x_i - E[x_i])^2] = E[x_i^2 - 2 x_i E[x_i]-E[x_i]^2]\\). Now we distribute the expectations to all the terms. \\(E[x_i^2] - E[2 x_i E[x_i]]+E[E[x_i]^2]]\\). Now clean up the expectations. \\(E[x_i^2] - 2 E[ x_i^2 ]+E[x_i]^2 = E[x_i^2]-E[x_i]^2 = E[(x_i-E[x_i])(x_i-E[x_i])]\\)\n\\[E[x_i(y_i - E[y_i])] = \\beta_1 E[(x_i-E[x_i])(x_i-E[x_i])]  \\] Note that we can do the same thing to the left hand side too. So \\(E[x_i(y_i - E[y_i])] = E[(x_i-E[x_i])(y_i-E[y_i])]\\)\nNow, putting it all together:\n\\[\\frac{E[(x_i-E[x_i])(y_i-E[y_i])]}{E[(x_i-E[x_i])(x_i-E[x_i])]} = \\beta_1 \\] \\[\\frac{\\text{population covariance}}{\\text{population variance  }} = \\beta_1 \\] But! we don’t have the population x or y or E[x] or E[y]. So we have to use the sample analogs.\n\\[\\frac{1}{N}\\sum_{i=1}^N [Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1 X_i] = 0 \\] and\n\\[\\frac{1}{N}\\sum_{i=1}^N [X_i(Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1 X_i)] = 0 \\]\nWe can rewrite the solution for \\(\\beta_0\\) as \\[ \\beta_0 = \\bar{Y}-\\hat{\\beta_1}\\bar{X} \\]\nIf we plug that into the second equation, we get:\n\\[\\frac{1}{N}\\sum_{i=1}^N X_i[Y_i - \\bar{Y}-\\hat{\\beta_1}\\bar{X} -\\hat{\\beta}_1 X_i] = 0 \\]\nNow separate the \\(\\beta_1\\)’s to the right hand side\n\\[\\frac{1}{N}\\sum_{i=1}^N X_i(Y_i - \\bar{Y}) = \\hat{\\beta_1} \\frac{1}{N}\\sum_{i=1}^N X_i (X_i - \\bar{X}) \\] \\[ \\hat{\\beta_1} = \\frac{\\frac{1}{N}\\sum_{i=1}^N X_i(X_i - \\bar{Y}) }{\\frac{1}{N}\\sum_{i=1}^N X_i (X_i - \\bar{X})} \\] Which is equivalent to:\n\\[\\hat{\\beta_1} = \\frac{\\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})(Y_i-\\bar{Y}) }{\\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X}) (X_i - \\bar{X})} = \\frac{\\text{Covariance(X,Y)}}{\\text{Variance(X)}}  \\]"
  },
  {
    "objectID": "OLS.html#linear-algebra",
    "href": "OLS.html#linear-algebra",
    "title": "Regression",
    "section": "Linear Algebra",
    "text": "Linear Algebra\nIn this section we derive the OLS model using linear algebra. Before getting into the derivatives, note the properties of transposes and matrix derivatives necessary to do this in the section below.\n\nPreliminaries\n\nX is an \\(x \\times k + 1\\) matrix with k variables and n observations. The plus 1 means we have an intercept as denoted by a vector of ones in the X matrix. This means \\(X'\\) or \\(X^T\\) is \\((k + 1) \\times n\\)\ne/u are \\(n \\times 1\\) vectors\ny is an \\(n \\times 1\\) vector\n\\(\\beta\\) is an \\(n \\times 1\\) vector\nA and B are matrices, a and b are vectors\nn is the number of observations, k is the number of predictors\n\\(y_i\\) denotes the i’th value of \\(y\\)\n\n\n\nMatrix Transpose and Derivative Rules\nTranspose rules:\n\n(AB)’ = B’A’\n\n(a’Bc)’ = c’B’a\n\na’b = b’a - (A + B)C = AC + BC\n\n(a + b)’C = a’C + b’C\n\n\\(AB \\neq BA\\)\n\nDerivative rules for taking the derivative with respect to X\n\n\\(X'B \\rightarrow B\\)\n\\(x'b \\rightarrow b\\)\n\\(x'x \\rightarrow 2x\\)\n\\(x'Bx \\rightarrow 2Bx\\)\n\nSee http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf\n\n\nThe Derivation\nOur goal here is to derive the coefficient \\(\\beta\\).\nSo we start with argmin \\(e'e\\)\n\\[e'e = (y-X\\hat{\\beta})'(y-X\\hat{\\beta})\\]\n\\[= y'y - y' X \\hat{\\beta} - \\hat{\\beta}'X'y + \\hat{\\beta}'X'X\\hat{\\beta} \\]\nNote above that \\(y'\\) is \\(1 \\times n\\). So \\(y'X\\hat{\\beta}\\) is \\((1 \\times n) (k \\times n) (n \\times 1)\\) also \\(\\hat{\\beta}'X'y\\) is \\((1 \\times k) (k \\times n) (n \\times 1) \\rightarrow 1 \\times 1\\). We choose \\(\\hat{\\beta}'X'y\\) to make the final step of the proof work. (We could also rearrange because X’y = y’X)\n\\[ =  y'y - 2 \\hat{\\beta}'X'y +    \\hat{\\beta}'X'X\\hat{\\beta}         \\] We minimize this quantity by taking the partial derivative with respect to \\(\\hat{\\beta}\\) and setting it equal to 0 and solving for \\(\\hat{\\beta}\\)\n\\[\\frac{\\partial e'e}{\\partial \\beta} = -2X'y + 2X'X\\hat{\\beta}               \\]\n\\[0 = -2X'y + 2X'X\\hat{\\beta}               \\]\n\\[2X'y = 2X'X\\hat{\\beta}               \\] \\[  X'y = X'X\\hat{\\beta}                \\] \\[ (X'X)^{-1} X'y = \\hat{\\beta}   \\]\nThe best guide for what this looks like in practice is https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf slides 99-105 (page numbers are n plus 1)"
  },
  {
    "objectID": "OLS.html#maximum-likelihood-derivation-of-ols",
    "href": "OLS.html#maximum-likelihood-derivation-of-ols",
    "title": "Regression",
    "section": "Maximum Likelihood Derivation of OLS",
    "text": "Maximum Likelihood Derivation of OLS\nAssume that \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). That means we know that \\(y_i \\sim N(\\hat{b}'x_i,\\sigma^2)\\)\nWe start with the joint distribution of \\(y_1,y_2,y_3 \\dots y_n\\)\n\\[P(y|X,b,\\sigma^2) = \\prod_{i=1}^nf(y_i;X,b,\\sigma^2) \\] \\[P(y|X,b,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sigma^2 \\sqrt{2 \\pi} } \\text{exp}(-\\frac{1}{2\\sigma^2}(y_i-b'x_i)^2) \\] Distribute the \\(\\prod\\) to the exponent, but remember that it replicates the constant \\(\\frac{1}{2\\pi\\sigma^2}\\) \\(c^n\\) times where c is the constant and n are the number of replications of the product notation.\n\\[(2 \\pi \\sigma^2)^{\\frac{-n}{2}} \\text{exp}(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b'x_i)^2)  \\]\nNow we take the logarithm of each side. This gives us the log likelihood which we’re going to… maximize!\n\\[ l = \\ln[P(y|X,b,\\sigma^2)]\\] \\[\\ln(l) = \\ln((2 \\pi \\sigma^2)^{\\frac{-n}{2}} \\text{exp}(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b'x_i)^2) ) \\]\n\\[l = -\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b'x_i)^2  \\] Here’s a neat little step - replace the row-notation above with matrix notation.\n\\[\\ln l = -\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2} (\\textbf{y}-\\textbf{Xb})'(\\textbf{y}-\\textbf{Xb})  \\]\nNow we have an expression for the log likelihood - so we want to … maximize it (i.e. for maximum likelihood!). What we’re going to do in a few steps is take the partial derivative with respect to b (i.e. \\(\\frac{\\partial l}{\\partial b}\\)) and set the whole thing to 0. This means that, at this point, we don’t care about terms that don’t have a b in them. I.e. the first term \\(-\\frac{n}{2}\\ln(2\\pi\\sigma^2)\\) does not have a b in it at all, so we can just call that \\(c\\).\n\\[\\ln l = c - \\frac{1}{2\\sigma^2} (\\textbf{y}-\\textbf{Xb})'(\\textbf{y}-\\textbf{Xb})    \\] \\[\\ln l = c - \\frac{1}{2\\sigma^2} (\\textbf{y'y} + \\textbf{b'X'Xb} - \\textbf{2b'X'y})    \\] Now do the partial derivative and set them equal to 0.\n\\[ \\frac{\\partial l}{\\partial b} =  2\\textbf{X'Xb} - \\textbf{2X'y} = 0\\]\n\\[ \\frac{\\partial l}{\\partial b} \\rightarrow 2\\textbf{X'Xb} = \\textbf{2X'y} \\] \\[\\hat{\\textbf{b}}_{ML} =  (\\textbf{X'X})^{-1}\\textbf{X'y}\\] To get the variance - take the partial with respect to \\(\\sigma\\)\nSource:\n\nhttps://allmodelsarewrong.github.io/olsml.html"
  },
  {
    "objectID": "OLS.html#gradient-descent-for-ols",
    "href": "OLS.html#gradient-descent-for-ols",
    "title": "Regression",
    "section": "Gradient Descent for OLS",
    "text": "Gradient Descent for OLS\nSee Gradient Descent help file! Here we’re not deriving OLS but we’re estimating the coefficients analytically.\nSource:\n\nhttps://allmodelsarewrong.github.io/gradient.html\nhttps://www.ocf.berkeley.edu/~janastas/stochastic-gradient-descent-in-r.html"
  },
  {
    "objectID": "OLS.html#are-methods-equivalent",
    "href": "OLS.html#are-methods-equivalent",
    "title": "Regression",
    "section": "Are Methods Equivalent?",
    "text": "Are Methods Equivalent?\n\nlibrary(haven)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\nlibrary(sjPlot)\n\n#DGP for x\na &lt;- 50000\nx &lt;- abs(rnorm(10000,mean = 13, sd = 5))\nb &lt;- 4000\nu &lt;- rnorm(10000, mean = 0, sd = 40000)\ny &lt;- a + b * x + u\n\nDGP &lt;- tibble(\n  x = x, \n  y = y\n  )\n\nggplot(DGP, aes(x=x, y=y)) +\n  geom_point() + \n  theme_minimal() +\n  labs(\n    title = \"Truth\",\n    x = \"Simulated X\",\n    y = \"Simulated Y\",\n  ) + stat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\")\n\n\n\n# Method of moments \ncov(x, y) / var(x)\n\n[1] 3978.392\n\n# Linear Algebra \nx_mat &lt;- as.matrix(cbind(1, x)) # remember to add column of ones!\nsolve( (t(x_mat) %*% x_mat) ) %*% (t(x_mat) %*% y) \n\n       [,1]\n  49825.042\nx  3978.392\n\n# MLE\nloglikMLE &lt;- function(par, y){\n  x = as.vector(x)\n  sum(dnorm(y, mean = par[1] + par[2]*x, sd = par[3], log = T))\n}\n\nMLE &lt;- optim(c(intercept = 9000, x = 400, sigma = 1), fn = loglikMLE, #parameters to be estimated, function to optimise\n               y = as.vector(y), control = list(fnscale = -1, reltol = 1e-16)\n               #we specify -1 in the controls so that we MAXIMIZE rather than minimize, which is the default\n               )\nround(MLE$par,1)\n\nintercept         x     sigma \n  49857.6    3976.0   40023.9 \n\n# Via Gradient Descent \ncost &lt;- function(X, y, theta) {\n  sum( (X %*% theta - y)^2 ) / (2*length(y))\n}\n\n#We must also set two additional parameters: learning rate and iteration limit\nalpha &lt;- 0.01\nnum_iters &lt;- 10000\n\n# keep history\ncost_history &lt;- double(num_iters)\ntheta_history &lt;- list(num_iters)\n\n# initialize coefficients\ntheta &lt;- matrix(c(0,0), nrow=2)\n\n# add a column of 1's for the intercept coefficient\nX &lt;- cbind(1, matrix(x))\n\n# gradient descent\nfor (i in 1:num_iters) {\n  error &lt;- (X %*% theta - y)\n  delta &lt;- t(X) %*% error / length(y)\n  theta &lt;- theta - alpha * delta\n  cost_history[i] &lt;- cost(X, y, theta)\n  theta_history[[i]] &lt;- theta\n}\n\ngradient_descent_ols_estimates &lt;- theta[,1] %&gt;% tibble()\n\n# how close is this to truth\nest &lt;- lm(y ~ x, DGP)\ntruth_vec &lt;- c(a, b)\n\nest$coefficients %&gt;% \n  tibble() %&gt;% \n  dplyr::bind_cols(truth_vec) %&gt;% \n  dplyr::bind_cols(c(\"a\",\"b\")) %&gt;%\n  dplyr::bind_cols(gradient_descent_ols_estimates) %&gt;%\n  dplyr::bind_cols(MLE$par[1:2]) %&gt;%\n  rename(\"OLS Estimate\" = 1, \"Truth\" = 2, \"Parameter\" = 3, \"Gradient Descent\" = 4, \"MLE\" = 5) %&gt;%\n  relocate(Parameter, .before = `OLS Estimate`) %&gt;%\n  relocate(Truth, .before = `OLS Estimate`)\n\nNew names:\n• `` -&gt; `...2`\n\n\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...3`\n\n\n# A tibble: 2 × 5\n  Parameter Truth `OLS Estimate` `Gradient Descent`    MLE\n  &lt;chr&gt;     &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 a         50000         49825.             49825. 49858.\n2 b          4000          3978.              3978.  3976."
  },
  {
    "objectID": "OLS.html#errors-in-matrix-form-in-r",
    "href": "OLS.html#errors-in-matrix-form-in-r",
    "title": "Regression",
    "section": "Errors in Matrix Form in R",
    "text": "Errors in Matrix Form in R\n\nx1 &lt;- rnorm(1000, 10, 5)\nx2 &lt;- rnorm(1000, 30, 10)\n\ny1 &lt;- 4 + 5 * x1 + 7 * x2 + rnorm(1000, 0, 10) \n\nsummary(m &lt;- lm(y1 ~ x1 + x2))\n\n\nCall:\nlm(formula = y1 ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.138  -6.981  -0.159   6.607  33.963 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.70414    1.21907   2.218   0.0268 *  \nx1           5.07799    0.06580  77.169   &lt;2e-16 ***\nx2           7.01436    0.03249 215.863   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.07 on 997 degrees of freedom\nMultiple R-squared:  0.981, Adjusted R-squared:  0.981 \nF-statistic: 2.576e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\nX &lt;- data.frame(1, x1, x2) \nX &lt;- as.matrix(X)\ny &lt;- as.matrix(y1)\n\nbeta = solve(t(X) %*% X) %*% (t(X) %*% y)\n\n# vcov in R\ne = y - X %*% beta\n\n# this is as above\nstd_error &lt;- ( t(e) %*% e  / (dim(X)[1] - dim(X)[2]) )[1,1] * (solve(t(X) %*% X))\n\n# Robust standard errors\n#https://library.virginia.edu/data/articles/understanding-robust-standard-errors\n# n / (n-k) * u_hat^2\nHC1 &lt;-  (dim(X)[1] /  (dim(X)[1] - dim(X)[2]) ) * (e %*% t(e))\n\nvce_hc1 &lt;- (solve(t(X) %*% X)) %*% t(X) %*% ((HC1 * diag(dim(X)[1])) %*% X) %*% (solve(t(X) %*% X)) \n\n# show all results are equivalent\ntibble(\n  our_beta = beta,\n  their_beta = lm(y1 ~ x1 + x2)$coefficients,\n  our_error = sqrt(diag(std_error)),\n  their_error = sqrt(diag(vcov(lm(y1 ~ x1 + x2)))),\n  our_robust_ses = sqrt(diag(vce_hc1)),\n  their_robust_ses = coeftest(m, vcov. = vcovHC(m, type = 'HC1'))[,2]\n)\n\n# A tibble: 3 × 6\n  our_beta[,1] their_beta our_error their_error our_robust_ses their_robust_ses\n         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1         2.70       2.70    1.22        1.22           1.14             1.14  \n2         5.08       5.08    0.0658      0.0658         0.0665           0.0665\n3         7.01       7.01    0.0325      0.0325         0.0310           0.0310"
  },
  {
    "objectID": "OLS.html#what-if-the-homoskedasticity-assumption-is-wrong",
    "href": "OLS.html#what-if-the-homoskedasticity-assumption-is-wrong",
    "title": "Regression",
    "section": "What if the homoskedasticity assumption is wrong?",
    "text": "What if the homoskedasticity assumption is wrong?\nWe can use Huber-White standard errors. These state that instead of replacing the \\(ee'\\) with \\(\\sigma^2I\\), we let the errors vary \\((X'X)^{-1}X'ee'(X'X)^{-1}X'\\)\n\n# Robust standard errors\n#https://library.virginia.edu/data/articles/understanding-robust-standard-errors\n# n / (n-k) * u_hat^2\nHC1 &lt;-  (dim(X)[1] /  (dim(X)[1] - dim(X)[2]) ) * (e %*% t(e))\n\nvce_hc1 &lt;- (solve(t(X) %*% X)) %*% t(X) %*% ((HC1 * diag(dim(X)[1])) %*% X) %*% (solve(t(X) %*% X)) \n\n# calculate using sqrt of diagonal\nsqrt(diag(vce_hc1)"
  },
  {
    "objectID": "OLS.html#mechanical-properties-of-ols",
    "href": "OLS.html#mechanical-properties-of-ols",
    "title": "Regression",
    "section": "Mechanical Properties of OLS",
    "text": "Mechanical Properties of OLS\n\nThe observed values of X are uncorrelated with the residuals. (Note we’re just assuming that X is uncorrelated with the unobserved disturbances)\n\n\nfit1 &lt;- lm(price ~ mpg + weight, data = sysuse::auto)\n\ncor(fit1$residuals, sysuse::auto$mpg)\n\n[1] 8.338145e-17\n\ncor(fit1$residuals, sysuse::auto$weight)\n\n[1] 9.020833e-17\n\n\n\nThe sum of the residuals is 0\n\n\nsum(fit1$residuals)\n\n[1] -3.637979e-12\n\n\n\nThe sample mean of the residuals is 0\n\n\nmean(fit1$residuals)\n\n[1] -9.832375e-14\n\n\n\nThe regression hyperplane passes through the means of the observed values \\(\\bar{X}\\) and \\(\\bar{y}\\). We know this because we know that \\(\\bar{e} = 0\\) and \\(e = y-X\\beta\\). If we divide by n - the number of observations, we’re left with \\(0=\\bar{e}=\\bar{y}-\\bar{X}\\hat{\\beta}\\) therefore \\(\\bar{y}=\\bar{X}\\hat{\\beta}\\)\n\n\n# We have a regression price = intercept + weight + mpg + error\n\n# mean of covariate mpg\nmean(sysuse::auto$mpg)\n\n[1] 21.2973\n\n# mean of covariate weight\nmean(sysuse::auto$weight)\n\n[1] 3019.459\n\n# mean of dependent variable price\nmean(sysuse::auto$price)\n\n[1] 6165.257\n\n# show equal\nround(mean(sysuse::auto$price),3) == round(predict(fit1, newdata = data.frame(weight = mean(sysuse::auto$weight), mpg = mean(sysuse::auto$mpg))), 3)\n\n   1 \nTRUE \n\n\n\nThe predicted values of y are uncorrelated with the residuals\n\n\ncor(fit1$residuals, fit1$fitted.values)\n\n[1] -5.475433e-17\n\n\n\nThe mean of the predicted Y’s for the sample will equal the mean of the observed y’s\n\n\nmean(fit1$fitted.values) == mean(sysuse::auto$price)\n\n[1] TRUE"
  },
  {
    "objectID": "OLS.html#showing-unbiasdness",
    "href": "OLS.html#showing-unbiasdness",
    "title": "Regression",
    "section": "Showing Unbiasdness",
    "text": "Showing Unbiasdness\nWhat does this mean? That \\(E[\\hat{\\beta}]=\\beta\\). In words, our estimated \\(\\hat{\\beta}\\) is equal to the population \\(\\beta\\)\nTo begin, we start with our definition of \\(\\beta\\) from above. We note that this requires the assumption of linearity in parameters AND no multicollinearity.\n\\[\\hat{\\beta} = (X'X)^{-1} X'y \\]\nNow we replace y with the definition of y (that is \\(y = X\\beta\\) + u). Note the lack of a hat, this is important! \\[\\hat{\\beta} = (X'X)^{-1} X'(X \\beta + u) \\] Distribute the \\((X'X)^{-1} X'\\)\n\\[\\hat{\\beta} = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'u \\] Note that \\((X'X)^{-1} X'X = I\\)\n\\[ \\hat{\\beta} = I\\beta + (X'X)^{-1} X'u \\] We’re not done yet BUT we can see that unless \\(E[u|X]=0\\), \\(\\hat{\\beta} \\neq \\beta\\).\nNow, take conditional expectation of each side\n\\[ E[\\hat{\\beta}|X] =  E[\\beta|X] + E[(X'X)^{-1} X'u|X] \\] \\[ E[\\hat{\\beta}|X] =  \\beta + (X'X)^{-1} X'E[u|X] \\] As long as we have the zero conditional mean assumption \\(E[u|X]=0\\) then \\(E[\\hat{\\beta}|X]=\\beta\\)"
  },
  {
    "objectID": "OLS.html#sec-FWL",
    "href": "OLS.html#sec-FWL",
    "title": "Regression",
    "section": "Regression Anatomy or Frisch-Waugh-Lowell",
    "text": "Regression Anatomy or Frisch-Waugh-Lowell\nRegression anatomy theorem helps us interpret a single slope coefficient in a multiple regression model\nAlso, help us understand “OLS” as a “matching estimator” (try to compare observations that are alike in the Xs)\nSee https://econ.lse.ac.uk/staff/spischke/ec533/Griliches_measurement%20error.pdf and https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107"
  },
  {
    "objectID": "OLS.html#what-is-it",
    "href": "OLS.html#what-is-it",
    "title": "Regression",
    "section": "What is it?",
    "text": "What is it?\nThis theorem tells us how to interpret a regression coefficient. Suppose we have a model \\[Y_{i} = \\alpha + \\beta_1 X_{1i} + \\beta_2X_{2i} + e_i\\]\nThe theorem says that \\[\\beta_{1i}=\\frac{COV(Y_i,\\tilde{X}_{1i})}{VAR(\\tilde{X}_{1i})}\\] where \\(\\tilde{X}_{1i}\\) are the residuals from a regression of \\(X_{1i}\\) on \\(X_{2i}\\):\n\\[X_{i1}=\\pi_0+\\pi_1X_{2i}+\\tilde{X}_{1i}\\] Suppose we care about \\(X_{1i}\\) it tells us that controlling for \\(X_{ki}\\) for all \\(k \\neq 1\\) that we’re looking at just the variation in \\(y\\) explained by \\(X_{1i}\\) and not by the variation in \\(X_{ki}\\).\nThe auxillary regression of \\(X_{1i}\\) on \\(X_{2i}\\) contains the term \\(\\tilde{X}_{1i}\\) which partials out (i.e. removes) the influence of \\(X_{2i}\\) on \\(X_{1i}\\)\nWe can also use it to explain the omitted variable bias formula (Short equals long plus the effect of omitted in long times the regression of omitted on included). Suppose we have two regressions \\(Y_{i} = \\beta_0 + \\beta_1 X_{1i} + e_i\\) and \\(Y_{i} = \\beta_0^* + \\beta_1^* X_{1i} + \\beta_2^*X_{2i} + v_i\\), then \\(\\beta_1 = \\frac{Cov(X_{1i},Y_i)}{Var(X_i)}=\\beta_{1i}^*+\\gamma \\delta_{X_2X_1}\\) where \\(\\delta_{X_2X_1}\\) is the regression of \\(X_2\\) on \\(X_1\\). Note that this is the opposite! order of regression anatomy. The auxillary regression in regression anatomy is \\(X_1\\) on \\(X_2\\)"
  },
  {
    "objectID": "OLS.html#why-is-it-useful",
    "href": "OLS.html#why-is-it-useful",
    "title": "Regression",
    "section": "Why is it useful?",
    "text": "Why is it useful?\nWe can break a multivariate regression with K regressors into K simpler bivariate models.\n\nIt allows you to construct a bi-dimensional scatterplot of a dependent variable an independent variable of interest using the coefficients from a multiple regression. I.e. you run the regressions above and then plot the coefficient of interest from the multiple regression with the original \\(y_i\\) on the y-axis and the residuals (\\(\\tilde{X}_{1i}\\)) on the x-axis.\nIt shows us why multicollinearity is a problem in regression - it means that most of the variation is between the regressors and not between the variable of interest (or the residual variable \\(\\tilde{X_{1i}}\\)) and the dependent variable. This means that the coefficient is unlikely to be significant because most of the variation between the multi-co-linear coefficients is between the coefficients and not between the coefficients and y.\nWe can use it in a multivariate OLS model to decompose the variance of each individual variable into three components (although we only really care about b and c):\n\n\n\nVariance not associated with y\nVariance associated with y and shared with other regressors\nVariance associated with y and not shared with other regressors\n\nWhen you construct an OLS model, the inclusion of a new regressor is valuable when the additional explaining power contained in it is not already fully captured by the other K regressors. Accordingly, the new variable must mainly provide the kind of variance denoted with (c). (https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107)"
  },
  {
    "objectID": "OLS.html#how-do-we-use-it",
    "href": "OLS.html#how-do-we-use-it",
    "title": "Regression",
    "section": "How do we use it?",
    "text": "How do we use it?\n\nScatterplots and analysis of the contribution of each independent variable\n\n\nlibrary(tidyverse)\nlibrary(sysuse)\n\n# see https://github.com/scunning1975/mixtape/blob/master/R/reganat.R\n\n# load data\nauto &lt;- sysuse::auto %&gt;%\n  mutate(length = length - mean(length))\n\n# show regression anatomy\nlong_regression &lt;- lm(formula = price ~ length + weight + headroom + mpg,data = auto)\n\n# below to show reg anatomy and FWL result in same estimate\nlong_without_coef_interest &lt;- lm(formula = price ~ weight + headroom + mpg,data = auto)\n\n# we care about effect of length on price\nx_residuals_regression &lt;- lm(formula = length ~ weight + headroom + mpg,data = auto)\n\nx_residuals &lt;- x_residuals_regression$residuals\n\n# show regression anatomy - all are the same! So\nlong_regression$coefficients[2]\n\n   length \n-94.49651 \n\n# this is regression anatomy\ncov(auto$price, x_residuals)/var(x_residuals)\n\n[1] -94.49651\n\n# this is also regression anatomy\nlm(formula = price ~ x_residuals, data = auto)$coefficients[2]\n\nx_residuals \n  -94.49651 \n\nlong_expressed_as_short &lt;- lm(formula = price ~ x_residuals, data = auto)\n\n# can also do Frisch–Waugh–Lovell.  It's regression anatomy but with the residuals from a regression of y on x2 and x3 (i.e without x1)\ncov(long_without_coef_interest$residuals, x_residuals)/var(x_residuals)\n\n[1] -94.49651\n\n# Now for the plotting!\n\n# show bias in short regression first\nbivariate_incorrect &lt;- lm(formula = price ~ length, data = auto)\n\n# how do we plot?  For geom_smooth you need a y and an x that's linear.\n# the x's are just the x residuals for everything - so those are easy\n# the y's are different!  the main difference between the two is that short uses the bivariate coefficient (times the value of the data itself), and the long uses the 'correct' coefficient from the long regression.  you could also use the coefficient from the bivariate regression of y on the residuals from the auxillary regression (the x1 on x2 through xn).  To keep the data consistent with the plotted points, we use the intercept from the bivariate regression of the y regressed on the residuals from the auxillary regression.\n\n# want a whole bunch of x,y pairs to plot\nshort &lt;- tibble(\n  price = long_expressed_as_short$coefficients[1] + bivariate_incorrect$coefficients[2] * auto$length , \n  x_residuals = x_residuals, \n)\n\nlong &lt;- tibble(\n  price = long_expressed_as_short$coefficients[1] + long_expressed_as_short$coefficients[2] * auto$length , \n  x_residuals = x_residuals, \n  # note could also have price as price = long_regression$coefficients[1] + long_expressed_as_short$coefficients[2] * auto$length \n)\n\nauto %&gt;%\n  ggplot(aes(x=x_residuals, y = price)) +\n  geom_point() +\n  geom_smooth(data = short, color = \"blue\",method = \"lm\", se = F) +\n  geom_smooth(data = long, color = \"red\",method = \"lm\", se = F) +\n    labs(\n    title = \"Regression Anatomy\", \n    caption = \"\n    We've fixed these lines to have the same intercept \\n and just let the slope vary to show the difference between the long and the short \\n\n    The x, y pairs are the original y's and the x residuals \\n\n    Blue is the effect of y on x1, not controlling for x2 \\n\n    Red is the effect of y on x1 controlling for x2\") \n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# To plot correctly for future use\nauto %&gt;%\n  ggplot(aes(x=x_residuals, y = price)) +\n  geom_point() +\n  geom_smooth(data = long, color = \"red\",method = \"lm\", se = F) +\n  labs(\n    title = \"Regression Anatomy\", \n    caption = \"Your long reference footnote goes in here\") +\n  theme_minimal() +\n  theme(\n    legend.position=\"none\",\n    plot.title = element_text(hjust = 0.5, #title\n                                #family = font,           #set font family\n                                size = 18,                #set font size\n                                face = 'bold',            #bold typeface\n                                vjust = 0))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nContribution of the variance of each variable to the model\n\nFor the above note that a partial correlation is computed between two residuals. A semipartial is computed between one residual and another raw or unresidualized variable.\n\nlibrary(magrittr)\n#reganat price length mpg weight, dis(length) semip\n\n# change back to non-demeaned length so that we can check our results with https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107#:~:text=The%20regression%20anatomy%20theorem%20is,issue%20in%20time%2Dseries%20econometrics.\nauto &lt;- sysuse::auto\n\n# need for semi partial correlation\nlong_regression &lt;- lm(formula = price ~ length + mpg + weight, data = auto)\n\n# need for partial correlation\nlong_without_coef_interest &lt;- lm(formula = price ~ mpg + weight, data = auto)\n\n# we care about effect of length on price\nx_residuals_regression &lt;- lm(formula = length ~ mpg + weight, data = auto)\n\n# regression anatomy works\n# bivariate &lt;- lm(formula = price ~ x_residuals_regression$residuals, data = auto)\n\n#### PARTIAL AND SEMI-PARTIAL COEFFICIENTS ####\n\n# partial of price with length\npartial &lt;- cor(x_residuals_regression$residuals, long_without_coef_interest$residuals)\n\n# semipartial correlations of price with length\nsemi_partial &lt;- cor(x_residuals_regression$residuals, auto$price)\n\n# the below is the contribution to the r^2 or the variance explained by x1\nsemi_partial^2\n\n[1] 0.06398732\n\n# make sure first x is the coef of interest\n# see page 14 https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107\nimportance_of_coef &lt;- function(df,y,xvars,...){\n  # select variables\n  df %&lt;&gt;%\n    select(all_of(y), all_of(xvars))\n  \n  y_numeric &lt;- df %&gt;%\n    select(y) %&gt;% pull()\n  \n  # run long regression\n  long_formula &lt;- as.formula(\n    paste(y, paste(xvars, collapse = \" + \"), sep = \" ~ \")\n    )\n  \n  # Saturated model \n  long_regression &lt;- lm(long_formula, data = df)\n  \n  # Saturated model without coef of interest\n  long_formula_without_coef_interest &lt;- as.formula(\n    paste(y, paste(xvars[-1], collapse = \" + \"), sep = \" ~ \")\n    )\n  \n  # long residuals without coefficient of interest\n  long_residuals_wo_ci &lt;- lm(formula = long_formula_without_coef_interest, data = df)\n\n  \n  # X residuals\n  x_residuals_formula &lt;- as.formula(\n      paste(xvars[1], paste(xvars[-1], collapse = \" + \"), sep = \" ~ \")\n  )\n  \n  x_residuals &lt;- lm(x_residuals_formula, data = df)\n  \n  partial &lt;- cor(x_residuals$residuals, long_residuals_wo_ci$residuals)\n  \n  semi_partial &lt;- cor(x_residuals$residuals, y_numeric)\n  \n  semi_partial_squared &lt;- semi_partial^2\n  \n   #Under normal conditions, the sum of the squared semipartials can be subtracted from the overall R2 for the complete OLS regression to get the value of common variance shared by the independent variables with y.\n  # the semi-partial formula here doens't work\n  # numerator &lt;- summary(long_regression)$r.squared - summary(long_residuals_wo_ci)$r.squared\n  # denominator &lt;- (1 - summary(long_regression)$r.squared)*(length(summary(long_regression)$residuals)-summary(long_regression)$df[1]-1)\n  # contribution &lt;- numerator/denominator\n  #   contribution\n  \n  #INTERPRETATION - semipartial squared is the variance explained by an x individually\n  # if you sum up the semi-partials of all the x's, you get all of their individual contributions to the model\n  # if you then subtract that number from the overall r^2 you get the amount of variance common to the x's \n  return(list(partial = partial, semi_partial = semi_partial, semi_partial_squared = semi_partial_squared))\n}\n\nlenth_cont &lt;- importance_of_coef(auto, y = \"price\", xvars = c(\"length\",\"weight\",\"mpg\"))$semi_partial_squared\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(y)\n\n  # Now:\n  data %&gt;% select(all_of(y))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\nweight_cont &lt;- importance_of_coef(auto, y = \"price\", xvars = c(\"weight\",\"length\",\"mpg\"))$semi_partial_squared\nmpg_cont &lt;- importance_of_coef(auto, y = \"price\", xvars = c(\"mpg\",\"weight\",\"length\"))$semi_partial_squared\n\n# Variance of x's individually\nlenth_cont + weight_cont + mpg_cont\n\n[1] 0.2021244\n\n# Variance common to the x's\nsummary(lm(price ~ length + weight + mpg, auto))$r.squared - (lenth_cont + weight_cont + mpg_cont)\n\n[1] 0.155252"
  },
  {
    "objectID": "OLS.html#what-is-it-1",
    "href": "OLS.html#what-is-it-1",
    "title": "Regression",
    "section": "What is it?",
    "text": "What is it?\nSuppose you have a short regression with K regressors and a long regression with K + n regressors where n is less than the number of observations in your sample:\nShort equals long plus the effect(s) of omitted times the regression(s) of omitted on included, all computed in a model maintaining the set of controls included in both short and long"
  },
  {
    "objectID": "OLS.html#why-is-it-useful-1",
    "href": "OLS.html#why-is-it-useful-1",
    "title": "Regression",
    "section": "Why is it useful?",
    "text": "Why is it useful?\nIt shows us the effect of omitting variables from our regression."
  },
  {
    "objectID": "OLS.html#how-do-we-use-it-1",
    "href": "OLS.html#how-do-we-use-it-1",
    "title": "Regression",
    "section": "How do we use it?",
    "text": "How do we use it?\nIt shows the impact of omitting variables in a regression. It’s mostly theoretical since you’d use variables if you had them!\n\n# see http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n08MRU_R2.pdf\n\nauto &lt;- sysuse::auto\n\nshort &lt;- lm(formula = price ~ mpg, data = auto)\n\nlong &lt;- lm(formula = price ~ mpg + length, data = auto)\n\nommitted_on_included &lt;- lm(length ~ mpg, data = auto)\n\n#Show this works\nshort$coefficients[2]\n\n      mpg \n-238.8943 \n\nlong$coefficients[2] + ommitted_on_included$coefficients[2] * long$coefficients[3]\n\n      mpg \n-238.8943 \n\n# round(as.numeric(short$coefficients[2]),5) == round(as.numeric(long$coefficients[2]) + as.numeric(ommitted_on_included$coefficients[2]) * as.numeric(long$coefficients[3]),5)\n\n# Now do OVB for a regression with more than 2 variables\n\nshort &lt;- lm(formula = price ~ mpg + weight, data = auto)\nlong &lt;- lm(formula = price ~ mpg + weight + length + trunk, data = auto)\n\n# Need to do two of these regressions! then take effect of MPG on each\nommitted_on_included_1 &lt;- lm(length ~ mpg + weight, data = auto)\nommitted_on_included_2 &lt;- lm(trunk ~ mpg + weight, data = auto)\n\n# Coefficient on MPG in short \nshort$coefficients[2]\n\n      mpg \n-49.51222 \n\nlong$coefficients[2] + ommitted_on_included_1$coefficients[2] * long$coefficients[4] + ommitted_on_included_2$coefficients[2] * long$coefficients[5]\n\n      mpg \n-49.51222"
  },
  {
    "objectID": "OLS.html#regression-in-machine-learning",
    "href": "OLS.html#regression-in-machine-learning",
    "title": "Regression",
    "section": "Regression in Machine Learning",
    "text": "Regression in Machine Learning\nHere we care about prediction instead of explaining.\nSum of Squared Error \\(= \\sum_{i=1}^n(\\hat{Y_i}-Y_i)^2\\)\nMean Squared Error \\(=\\frac{1}{n}\\sum_{i=1}^n(\\hat{Y_i}-Y_i)^2\\)\nHere \\(SSE = n \\times MSE\\)\nWe can derive OLS using MSE instead or SSE if we want. That is, we can minimize MSE instead of SSE.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^ne_i^2 \\]\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y_i} - y_i)^2  \\]\n\\[MSE = \\frac{1}{n}(Xb -y)'(Xb-y)  \\] Distribute the terms above so that we can take the derivative with respect to b \\[MSE = \\frac{1}{n}( b'X'Xb - b'X'y - y'Xb + y'y) \\] \\[ MSE = \\frac{1}{n}(b'X'Xb - 2b'X'y + y'y) \\] The above takes advantage of the fact that the transpose of a scaler is a scaler. I.e. if b is k x 1 and b’ is 1 x k then b’X’y is 1 x 1 and so is y’Xb\n\\[\\frac{\\partial}{\\partial{b}} \\text{MSE} = \\frac{1}{n}(2X'Xb - 2X'y) \\]\n\\[\\frac{\\partial}{\\partial{b}} \\text{MSE} = \\frac{2}{n}(X'Xb) - \\frac{2}{n}(X'y) \\] \\[ 0 = \\frac{2}{n}(X'Xb) - \\frac{2}{n}(X'y) \\] \\[ 0 = X'Xb - X'y \\] \\[ X'Xb = X'y \\] \\[ b = (X'X)^{-1}X'y \\]\nIf we only have two predictors \\(X_1\\) and \\(X_2\\) then our error can be visualized in the following way https://allmodelsarewrong.github.io/ols.html\n\\[ MSE = E(\\hat{y},y) = \\frac{1}{n} (\\beta' x' x \\beta - 2 y'x\\beta + y'y) \\]. The first term is a quadratic, the second is linear, the third is a scalar"
  },
  {
    "objectID": "OLS.html#ridge-regression",
    "href": "OLS.html#ridge-regression",
    "title": "Regression",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nRidge is OLS with a budget \\(\\lambda\\). The coefficients won’t drop out, like in LASSO, but they’ll tend towards 0. I.e. ridge regression does not perform variable selection. This is known as an l2 norm.\n\\[\\beta_{rr}=(X'X + \\lambda I)^{-1}X'Y\\]\nRidge regression is a constrained maximization problem. Where:\n\\[\\min_b \\{\\frac{1}{n}(X'X)^{-1}(X'Y)\\} \\text{ such that } b'b \\leq c\\] The solution is above. How do you find c? or \\(\\lambda\\)?\n``In ridge regression, \\(\\lambda\\) is a tuning parameter, and therefore it cannot be found analytically. Instead, you have to implement a trial and error process with various values for \\(\\lambda\\), and determine which seems to be a good one. How? Typically with cross-validation, or other type of resampling approach.”\nBasically: 1) Select a small \\(\\lambda\\) (if lambda is big the coefs all go to 0) 2) For each fold, (so no we’re in the second for loop), fit a ridge regression and store the errors 3) Compare all of them, take the smallest one and use that \\(\\lambda\\)"
  },
  {
    "objectID": "OLS.html#least-absolute-shrinkage-and-selection-operator-lasso",
    "href": "OLS.html#least-absolute-shrinkage-and-selection-operator-lasso",
    "title": "Regression",
    "section": "Least Absolute Shrinkage and Selection Operator (LASSO)",
    "text": "Least Absolute Shrinkage and Selection Operator (LASSO)\nThis is used in variable selection. It’s also known as a l1 norm. It’s like Ridge, except it’s a constrained minimization problem subject to the norm \\(|b|\\leq c\\)\nhttps://allmodelsarewrong.github.io/lasso.html"
  },
  {
    "objectID": "OLS.html#question",
    "href": "OLS.html#question",
    "title": "Regression",
    "section": "Question",
    "text": "Question\nYou accidentally double every observation, what happens to beta?\nYou end up with \\(\\beta = 1/2 \\times (X'X)^{-1}XY\\)\nSuppose you accidentally append the data to itself, what happens?\nCoefficient remains the same.\n\nx = rnorm(1000, 10, 5)\nx2 = x + x\nx3 = x + x + x\ny = 50 + x + rnorm(1000, 0, 4)\ndoub_y = c(y,y)\ndoub_x = c(x,x)\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     50.051        0.987  \n\nlm(y~x2)  \n\n\nCall:\nlm(formula = y ~ x2)\n\nCoefficients:\n(Intercept)           x2  \n    50.0512       0.4935  \n\nlm(y~x3)\n\n\nCall:\nlm(formula = y ~ x3)\n\nCoefficients:\n(Intercept)           x3  \n     50.051        0.329  \n\nlm(doub_y ~ doub_x)\n\n\nCall:\nlm(formula = doub_y ~ doub_x)\n\nCoefficients:\n(Intercept)       doub_x  \n     50.051        0.987"
  },
  {
    "objectID": "OLS.html#best-linear-predictor",
    "href": "OLS.html#best-linear-predictor",
    "title": "Regression",
    "section": "Best Linear Predictor",
    "text": "Best Linear Predictor\nWe’re going to show that, for a population, \\(\\beta\\) is the best linear predictor for y in the mean-squared error sense (i.e, it has the lowest mean squared error).\n\nhttp://www.its.caltech.edu/~mshum/stats/natural2.pdf\nhttp://prob140.org/fa18/textbook/chapters/Chapter_25/02_Best_Linear_Predictor"
  },
  {
    "objectID": "OLS.html#dealing-with-data",
    "href": "OLS.html#dealing-with-data",
    "title": "Regression",
    "section": "Dealing with Data",
    "text": "Dealing with Data\nThree types of extreme values\n1 Outlier: extreme in the y direction\n2 Leverage point: extreme in one x direction\n3 Influence point: extreme in both directions\nIs the data corrupted? I Fix the observation (obvious data entry errors) I Remove the observation I Be transparent either way\nIs the outlier part of the data generating process? I Transform the dependent variable (log(y)) I Use a method that is robust to outliers (robust regression)\nhttps://scholar.princeton.edu/sites/default/files/bstewart/files/lecture9handout.pdf\nLogistic Regression https://bookdown.org/egarpor/SSS2-UC3M/logreg-assumps.html"
  },
  {
    "objectID": "OLS.html#bias-varience-tradeoff",
    "href": "OLS.html#bias-varience-tradeoff",
    "title": "Regression",
    "section": "Bias Varience Tradeoff",
    "text": "Bias Varience Tradeoff\nhttp://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "OLS.html#l1-and-l2-regularization-in-linear-regression",
    "href": "OLS.html#l1-and-l2-regularization-in-linear-regression",
    "title": "Regression",
    "section": "L1 and L2 Regularization in Linear Regression",
    "text": "L1 and L2 Regularization in Linear Regression\nhttps://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf A general, HUGELY IMPORTANT problem for all machine learning algorithms • We can find a hypothesis that predicts perfectly the training data but does not generalize well to new data\nSee https://uc-r.github.io/regularized_regression\nRegularization is a technique that allows\nWith a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection.\nRegularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model.\nHowever, elastic nets, and regularization models in general, still assume linear relationships between the features and the target variable."
  },
  {
    "objectID": "OLS.html#the-math",
    "href": "OLS.html#the-math",
    "title": "Regression",
    "section": "The Math",
    "text": "The Math\nThe objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P).\nI.e. in a regression we minimize the sum of squared errors (SSE) as in \\(\\text{min}{e'e} = \\text{min}{(Y-X\\beta)'(Y-X\\beta)} \\rightarrow \\beta = (X'X)^{-1}X'Y\\)\nImagine, instead we add a penalty term to the minimization problem that constrains the coefficients and progressively shrink them towards zero.\n\\[\\text{Min{SSE - P}}\\] What values can P take? There are two main options - L1 and L2:\n- L1 - or LASSO \\(P = \\lambda \\sum_{j=1}^{p}\\) |\\(B_j\\)| - L2 - or Ridge Regression \\(P = \\lambda \\sum_{j=1}^{p}B_j^2\\)\nIn both cases \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data.\n\n# Regularization\n# Ridge Regression in R\n# Load libraries, get data & set\n# seed for reproducibility \nset.seed(123)    \nlibrary(glmnet)  \nlibrary(dplyr)   \nlibrary(psych)\n  \ndata(\"mtcars\")\n# Center y, X will be standardized \n# in the modelling function\ny &lt;- mtcars %&gt;% tibble() %&gt;% select(mpg) %&gt;% \n            scale(center = TRUE, scale = FALSE) %&gt;% \n            as.matrix()\nX &lt;- mtcars %&gt;% tibble() %&gt;% select(-mpg) %&gt;% as.matrix()\n  \n# Perform 10-fold cross-validation to select lambda\nlambdas_to_try &lt;- 10^seq(-3, 5, length.out = 100)\n  \n# Setting alpha = 0 implements ridge regression\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0, \n                      lambda = lambdas_to_try,\n                      standardize = TRUE, nfolds = 10)\n  \n# Plot cross-validation results\nplot(ridge_cv)\n  \n# Best cross-validated lambda\nlambda_cv &lt;- ridge_cv$lambda.min\n  \n# Fit final model, get its sum of squared\n# residuals and multiple R-squared\nmodel_cv &lt;- glmnet(X, y, alpha = 0, lambda = lambda_cv,\n                   standardize = TRUE)\ny_hat_cv &lt;- predict(model_cv, X)\nssr_cv &lt;- t(y - y_hat_cv) %*% (y - y_hat_cv)\nrsq_ridge_cv &lt;- cor(y, y_hat_cv)^2\n  \n# selecting lambda based on the information\nX_scaled &lt;- scale(X)\naic &lt;- c()\nbic &lt;- c()\nfor (lambda in seq(lambdas_to_try)) {\n  # Run model\n  model &lt;- glmnet(X, y, alpha = 0,\n                  lambda = lambdas_to_try[lambda], \n                  standardize = TRUE)\n    \n  # Extract coefficients and residuals (remove first \n  # row for the intercept)\n  betas &lt;- as.vector((as.matrix(coef(model))[-1, ]))\n  resid &lt;- y - (X_scaled %*% betas)\n    \n  # Compute hat-matrix and degrees of freedom\n  ld &lt;- lambdas_to_try[lambda] * diag(ncol(X_scaled))\n  H &lt;- X_scaled %*% solve(t(X_scaled) %*% X_scaled + ld) %*% t(X_scaled)\n  df &lt;- tr(H)\n    \n  # Compute information criteria\n  aic[lambda] &lt;- nrow(X_scaled) * log(t(resid) %*% resid) + 2 * df\n  bic[lambda] &lt;- nrow(X_scaled) * log(t(resid) %*% resid) + 2 * df * log(nrow(X_scaled))\n}\n  \n# Plot information criteria against tried values of lambdas\nplot(log(lambdas_to_try), aic, col = \"orange\", type = \"l\",\n     ylim = c(190, 260), ylab = \"Information Criterion\")\nlines(log(lambdas_to_try), bic, col = \"skyblue3\")\nlegend(\"bottomright\", lwd = 1, col = c(\"orange\", \"skyblue3\"), \n       legend = c(\"AIC\", \"BIC\"))\n  \n# Optimal lambdas according to both criteria\nlambda_aic &lt;- lambdas_to_try[which.min(aic)]\nlambda_bic &lt;- lambdas_to_try[which.min(bic)]\n  \n# Fit final models, get their sum of \n# squared residuals and multiple R-squared\nmodel_aic &lt;- glmnet(X, y, alpha = 0, lambda = lambda_aic, \n                    standardize = TRUE)\ny_hat_aic &lt;- predict(model_aic, X)\nssr_aic &lt;- t(y - y_hat_aic) %*% (y - y_hat_aic)\nrsq_ridge_aic &lt;- cor(y, y_hat_aic)^2\n  \nmodel_bic &lt;- glmnet(X, y, alpha = 0, lambda = lambda_bic, \n                    standardize = TRUE)\ny_hat_bic &lt;- predict(model_bic, X)\nssr_bic &lt;- t(y - y_hat_bic) %*% (y - y_hat_bic)\nrsq_ridge_bic &lt;- cor(y, y_hat_bic)^2\n  \n# The higher the lambda, the more the \n# coefficients are shrinked towards zero.\nres &lt;- glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n              standardize = FALSE)\nplot(res, xvar = \"lambda\")\nlegend(\"bottomright\", lwd = 1, col = 1:6, \n       legend = colnames(X), cex = .7)"
  },
  {
    "objectID": "OLS.html#whats-your-estimand",
    "href": "OLS.html#whats-your-estimand",
    "title": "Regression",
    "section": "What’s your estimand",
    "text": "What’s your estimand\n“In every quantitative paper we read, every quantitative talk we attend, and every quantitative article we write, we should all ask one question: what is the estimand? The estimand is the object of inquiry—it is the precise quantity about which we marshal data to draw an inference.”\n“Our framework stands in contrast to the currently dominant mode of quantitative inquiry: hypotheses about regression coefficients. That mode of inquiry defines the research goal inside a particular statistical model. If your research goal is a coefficient of a particular model, then you are committed to that model: it becomes impossible to reason about other approaches to achieve the goal. By contrast, we advocate a statement of the goal outside the statistical model—like an average causal effect or a population mean—which opens the door to alternative estimation procedures that could answer the research question under more credible assumptions.”\n“We introduce a term for the goal stated outside the model—the theoretical estimand—which has two components. The first is a unit-specific quantity, which could be a realized outcome (whether person i is employed), a potential outcome (whether person i would be employed if they received job training), or a difference in potential outcomes (the effect of job training on the employment of person i). It could also be a potential outcome that would be realized under intervention of more than one variable (whether person i would be employed if they received job training and childcare), thus unlocking numerous new causal questions. The unit-specific quantity clarifies whether the research goal is causal, and if so, what counterfactual intervention is being considered. The second component of the theoretical estimand is the target population: over whom or what do we aggregate that unit-specific quantity? The unit-specific quantity and target population combine to define the theoretical estimand: the thing we would like to know if we had data for the full population in all factual or counterfactual worlds of interest. A paper may have multiple theoretical estimands.”\n“Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g., a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not. These identification assumptions can be conveyed in a Directed Acyclic Graph (DAG). Finally, one chooses an estimation strategy to learn the empirical estimand (e.g., a regression model). We use the general term “estimands” to refer to both the theoretical and the empirical estimands.”\n\n\n\nFigure 1: Data Analysis Workflow\n\n\n“Too often, research papers involve pages of rich theory followed by pages of procedures applied to data, with a vague link between the two. The theoretical and empirical estimand fill the void by precisely stating both the theoretical quantity we would like to know and the empirical quantity that our procedures are most directly designed to approximate.”\n“ A clear statement of the target population allows a researcher to clarify which approach they are taking.”\n“The IV design offers strong causal identification at a cost: the estimated causal effect is an average not over the full population, but only over the subpopulation of compliers whose treatment status is causally affected by the instrument (Imbens and Angrist 1994).”\n“A lack of common support arises whenever some subpopulation defined by a confounder (e.g., terrorists) contains no treated units or no untreated units (e.g., those on probation or not). Common support problems leave researchers three options. They can argue that the feasible subpopulation—those with covariates at which both treated and control units are observed—is theoretically interesting (a leap at the link between theory and the theoretical estimand); they can argue that the feasible subpopulation is informative about the broader population (a leap between the theoretical and empirical estimand); or, they can lean heavily on a parametric model and extrapolate what is observed in the feasible subpopulation to what they think would happen in the space beyond common support (a leap in estimation). As in experiments and IV, there is no free lunch. A statement of the target population is an opportunity for authors to put the difficulty in the pages of the article and clarify how they address it.”\n“To summarize, the theoretical estimand states the study aim in precise terms involving a unit-specific quantity aggregated over a target population. The theoretical estimand exists outside of any statistical model and liberates us to make complex research questions precise. Descriptive estimands can be stated even if some of the population would refuse all survey attempts or is structurally missing from administrative records. Causal estimands can be stated in terms of counterfactuals we could never observe. In contrast to the constraints of regression coefficients, a theoretical estimand allows us to formalize the quantity most relevant to theory.”\n17\n“ Instead of thinking about estimating the parameters of a model, we must think of the estimation algorithm as a tool to estimate the unknown components (e.g., conditional means) that appear in the empirical estimand. Doing so allows empirical evidence to inform the choice of an estimation strategy. Conceptual argument is central to the statement of the theoretical and empirical estimands, but selection of an estimation strategy can be largely data-driven.”"
  },
  {
    "objectID": "OLS.html#footnotes",
    "href": "OLS.html#footnotes",
    "title": "Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Econometric identification really means just one thing: model parameters or features being uniquely determined from the observable population that generates the data.” See more at http://fmwww.bc.edu/EC-P/wp957.pdf. Cyrus Samii defines it as “Identification refers generally to sufficiency for drawing a conclusion given the type of data that are available. [shorter link]https://uc76a733439785d112bbe0d38b98.dl.dropboxusercontent.com/cd/0/inline2/CQ61-3qehcE2gmowHjic11s015sJF6D1WJfI_poSUzkCRE9YxB8Db8l7Rj5D84svlwKIp9XQiqQ67gDu_T0hr8r8sqnKnFxqOxIdIm8M8Zt-aR2kauFd8_K9CxTtL7c9_M1kaikQZEj2XZ69YLdJE8yaQe-J5r7PROGzFXSAWWiJ5cCdo2ByxR8QZV3DjE2I4xRknw1LPeSwtIKBrJpiNQf7DU-sydqctqTQPUv6wqN4r1VPZF1Uvmip2evOt9dqZDp77xD1kxEsQV-AXjBrigqltjSVYYq2caOQELnzuIfov_TlNLPKRnW4lFoVEj5UccjZQLxlsMrJLUx0C1ZFQ4wsyZFnuPDyo0WQPtu8HQ-bpxE7fvBmTP3JeaWErj-Txmk/file”↩︎\nhttps://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say.↩︎\nhttps://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf↩︎\nhttps://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf↩︎\nThis section mostly stolen from https://diff.healthpolicydatascience.org↩︎\nSee Section 3.2 for more detail↩︎\nSee Lundberg, I., Johnson, R. and Stewart, B.M., 2021. What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), pp.532-565.https://www.rebeccajohnson.io/files/asr_estimands_pdf.pdf. This article has a discussion about how to pick estimands.↩︎\nThe problem, however, is that you don’t know the true real-world value of the estimand. You only have the estimate. Which, as I discussed above, is a function of the estimand, bias, and noise (i.e. \\(\\text{estimate} = \\text{estimand} + \\text{bias} + \\text{noise}\\)).↩︎\nhttps://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf#:~:text=Our%20starting%20point%20in%20Section%202%20is,not%20receive%20the%20treatment%20in%20either%20period.↩︎\nThis is the like Medicare spending in a state that if they didn’t expand Medicaid when they did expand Medicaid.↩︎\nNote, however, there’s nothing that stops an analyst from making a stupid decision like using TX or CA as a control group.↩︎\nhttps://diff.healthpolicydatascience.org↩︎\nThis proof is shamelessly stolen from https://diff.healthpolicydatascience.org↩︎\nThis applies to unconditional expectations too so \\(\\mathbb{E}[x+y]=\\mathbb{E}[x]+\\mathbb{E}[y]\\)↩︎\nThis imposes that (a) all units have the same treatment effect, and (b) the treatment has the same effect regardless of how long it has been since treatment started. This would impose that the effect of Medicaid expansion on insurance coverage is the same both across states and across time. Then, under a suitable generalization of the parallel trends assumption and no anticipation assumption, the population regression coefficient \\(\\beta_post\\) from is equal to \\(\\tau\\).↩︎\nhttps://github.com/Mixtape-Sessions/Advanced-DID/?tab=readme-ov-file↩︎\nCausal inference theory also clarifies the important distinction between a confounder and a collider. A confounder is a variable that causes both the exposure and the outcome. By contrast, a “collider” is a variable that is caused by at least two other variables (the causing variables “collide” in the collider). For example, if quality of life is affected by both smoking (exposure) and lung cancer (outcome), quality of life would be a collider and not a confounder for the association between smoking and lung cancer. This distinction is important, because methods designed to correct for confounding (e.g., regression analysis) can introduce bias if they are applied to colliders. For this reason, bias of this kind is termed “collider bias” (CB).↩︎"
  }
]