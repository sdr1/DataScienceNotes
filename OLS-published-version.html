<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Steven Rashin">
<meta name="dcterms.date" content="2024-04-22">

<title>Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="OLS-published-version_files/libs/clipboard/clipboard.min.js"></script>
<script src="OLS-published-version_files/libs/quarto-html/quarto.js"></script>
<script src="OLS-published-version_files/libs/quarto-html/popper.min.js"></script>
<script src="OLS-published-version_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="OLS-published-version_files/libs/quarto-html/anchor.min.js"></script>
<link href="OLS-published-version_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="OLS-published-version_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="OLS-published-version_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="OLS-published-version_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="OLS-published-version_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

<script src="OLS-published-version_files/libs/kePrint-0.0.1/kePrint.js"></script>
<link href="OLS-published-version_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="OLS-published-version_files/libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="OLS-published-version_files/libs/bsTable-3.3.7/bootstrapTable.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#tl-dr-checklist" id="toc-tl-dr-checklist" class="nav-link active" data-scroll-target="#tl-dr-checklist">TL; DR: Checklist</a>
  <ul class="collapse">
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a>
  <ul class="collapse">
  <li><a href="#less-math-heavy-or-math-heavy-with-intuitive-explanations" id="toc-less-math-heavy-or-math-heavy-with-intuitive-explanations" class="nav-link" data-scroll-target="#less-math-heavy-or-math-heavy-with-intuitive-explanations">Less Math Heavy or Math Heavy with Intuitive Explanations</a></li>
  <li><a href="#bring-on-the-pain-aka-math" id="toc-bring-on-the-pain-aka-math" class="nav-link" data-scroll-target="#bring-on-the-pain-aka-math">Bring on the Pain (aka Math)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link" data-scroll-target="#preliminaries">Preliminaries</a></li>
  <li><a href="#our-goalsterms" id="toc-our-goalsterms" class="nav-link" data-scroll-target="#our-goalsterms">Our Goals/Terms</a>
  <ul class="collapse">
  <li><a href="#goals" id="toc-goals" class="nav-link" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#sec-terms" id="toc-sec-terms" class="nav-link" data-scroll-target="#sec-terms">Terms</a>
  <ul class="collapse">
  <li><a href="#dgp-rightarrow-sample-rightarrow-dgp" id="toc-dgp-rightarrow-sample-rightarrow-dgp" class="nav-link" data-scroll-target="#dgp-rightarrow-sample-rightarrow-dgp">DGP <span class="math inline">\(\rightarrow\)</span> Sample <span class="math inline">\(\rightarrow\)</span> DGP</a></li>
  </ul></li>
  <li><a href="#ols-big-picture" id="toc-ols-big-picture" class="nav-link" data-scroll-target="#ols-big-picture">OLS Big Picture</a></li>
  </ul></li>
  <li><a href="#is-my-regression-causal" id="toc-is-my-regression-causal" class="nav-link" data-scroll-target="#is-my-regression-causal">Is My Regression Causal?</a></li>
  <li><a href="#deriving-ols" id="toc-deriving-ols" class="nav-link" data-scroll-target="#deriving-ols">Deriving OLS</a>
  <ul class="collapse">
  <li><a href="#method-of-moments" id="toc-method-of-moments" class="nav-link" data-scroll-target="#method-of-moments">Method of Moments</a></li>
  <li><a href="#linear-algebra" id="toc-linear-algebra" class="nav-link" data-scroll-target="#linear-algebra">Linear Algebra</a>
  <ul class="collapse">
  <li><a href="#preliminaries-1" id="toc-preliminaries-1" class="nav-link" data-scroll-target="#preliminaries-1">Preliminaries</a></li>
  <li><a href="#matrix-transpose-and-derivative-rules" id="toc-matrix-transpose-and-derivative-rules" class="nav-link" data-scroll-target="#matrix-transpose-and-derivative-rules">Matrix Transpose and Derivative Rules</a></li>
  <li><a href="#the-derivation" id="toc-the-derivation" class="nav-link" data-scroll-target="#the-derivation">The Derivation</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-derivation-of-ols" id="toc-maximum-likelihood-derivation-of-ols" class="nav-link" data-scroll-target="#maximum-likelihood-derivation-of-ols">Maximum Likelihood Derivation of OLS</a></li>
  <li><a href="#gradient-descent-for-ols" id="toc-gradient-descent-for-ols" class="nav-link" data-scroll-target="#gradient-descent-for-ols">Gradient Descent for OLS</a></li>
  <li><a href="#are-methods-equivalent" id="toc-are-methods-equivalent" class="nav-link" data-scroll-target="#are-methods-equivalent">Are Methods Equivalent?</a></li>
  </ul></li>
  <li><a href="#variance-of-beta---sigma2" id="toc-variance-of-beta---sigma2" class="nav-link" data-scroll-target="#variance-of-beta---sigma2">Variance of <span class="math inline">\(\beta\)</span> - <span class="math inline">\(\sigma^2\)</span></a>
  <ul class="collapse">
  <li><a href="#longer-proof" id="toc-longer-proof" class="nav-link" data-scroll-target="#longer-proof">Longer proof</a></li>
  <li><a href="#errors-in-matrix-form-in-r" id="toc-errors-in-matrix-form-in-r" class="nav-link" data-scroll-target="#errors-in-matrix-form-in-r">Errors in Matrix Form in R</a></li>
  <li><a href="#what-if-the-homoskedasticity-assumption-is-wrong" id="toc-what-if-the-homoskedasticity-assumption-is-wrong" class="nav-link" data-scroll-target="#what-if-the-homoskedasticity-assumption-is-wrong">What if the homoskedasticity assumption is wrong?</a></li>
  </ul></li>
  <li><a href="#assumptions-of-ols" id="toc-assumptions-of-ols" class="nav-link" data-scroll-target="#assumptions-of-ols">Assumptions of OLS</a></li>
  <li><a href="#uses-of-the-assumptions" id="toc-uses-of-the-assumptions" class="nav-link" data-scroll-target="#uses-of-the-assumptions">Uses of the assumptions</a>
  <ul class="collapse">
  <li><a href="#mechanical-properties-of-ols" id="toc-mechanical-properties-of-ols" class="nav-link" data-scroll-target="#mechanical-properties-of-ols">Mechanical Properties of OLS</a></li>
  <li><a href="#showing-unbiasdness" id="toc-showing-unbiasdness" class="nav-link" data-scroll-target="#showing-unbiasdness">Showing Unbiasdness</a></li>
  </ul></li>
  <li><a href="#properties-of-ols" id="toc-properties-of-ols" class="nav-link" data-scroll-target="#properties-of-ols">Properties of OLS</a>
  <ul class="collapse">
  <li><a href="#sec-FWL" id="toc-sec-FWL" class="nav-link" data-scroll-target="#sec-FWL">Regression Anatomy or Frisch-Waugh-Lowell</a></li>
  <li><a href="#what-is-it" id="toc-what-is-it" class="nav-link" data-scroll-target="#what-is-it">What is it?</a></li>
  <li><a href="#why-is-it-useful" id="toc-why-is-it-useful" class="nav-link" data-scroll-target="#why-is-it-useful">Why is it useful?</a></li>
  <li><a href="#how-do-we-use-it" id="toc-how-do-we-use-it" class="nav-link" data-scroll-target="#how-do-we-use-it">How do we use it?</a></li>
  </ul></li>
  <li><a href="#omitted-variable-bias-ovb" id="toc-omitted-variable-bias-ovb" class="nav-link" data-scroll-target="#omitted-variable-bias-ovb">Omitted Variable Bias (OVB)</a>
  <ul class="collapse">
  <li><a href="#what-is-it-1" id="toc-what-is-it-1" class="nav-link" data-scroll-target="#what-is-it-1">What is it?</a></li>
  <li><a href="#why-is-it-useful-1" id="toc-why-is-it-useful-1" class="nav-link" data-scroll-target="#why-is-it-useful-1">Why is it useful?</a></li>
  <li><a href="#how-do-we-use-it-1" id="toc-how-do-we-use-it-1" class="nav-link" data-scroll-target="#how-do-we-use-it-1">How do we use it?</a></li>
  </ul></li>
  <li><a href="#the-hat-matrix" id="toc-the-hat-matrix" class="nav-link" data-scroll-target="#the-hat-matrix">The Hat Matrix!</a></li>
  <li><a href="#sec-interpret" id="toc-sec-interpret" class="nav-link" data-scroll-target="#sec-interpret">Interpretation</a>
  <ul class="collapse">
  <li><a href="#why-this-interpretation-for-logged-dependent-variable-values" id="toc-why-this-interpretation-for-logged-dependent-variable-values" class="nav-link" data-scroll-target="#why-this-interpretation-for-logged-dependent-variable-values">Why this interpretation for logged dependent variable values?</a></li>
  <li><a href="#logged-independent-variable" id="toc-logged-independent-variable" class="nav-link" data-scroll-target="#logged-independent-variable">Logged Independent Variable</a></li>
  </ul></li>
  <li><a href="#ols-diagnostics" id="toc-ols-diagnostics" class="nav-link" data-scroll-target="#ols-diagnostics">OLS Diagnostics</a></li>
  <li><a href="#lots-of-fixed-effects" id="toc-lots-of-fixed-effects" class="nav-link" data-scroll-target="#lots-of-fixed-effects">Lots of Fixed Effects</a></li>
  <li><a href="#effective-sample" id="toc-effective-sample" class="nav-link" data-scroll-target="#effective-sample">Effective Sample</a>
  <ul class="collapse">
  <li><a href="#regression-in-machine-learning" id="toc-regression-in-machine-learning" class="nav-link" data-scroll-target="#regression-in-machine-learning">Regression in Machine Learning</a></li>
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression">Ridge Regression</a></li>
  <li><a href="#least-absolute-shrinkage-and-selection-operator-lasso" id="toc-least-absolute-shrinkage-and-selection-operator-lasso" class="nav-link" data-scroll-target="#least-absolute-shrinkage-and-selection-operator-lasso">Least Absolute Shrinkage and Selection Operator (LASSO)</a></li>
  </ul></li>
  <li><a href="#measurement-error" id="toc-measurement-error" class="nav-link" data-scroll-target="#measurement-error">Measurement Error</a></li>
  <li><a href="#causal-quantities" id="toc-causal-quantities" class="nav-link" data-scroll-target="#causal-quantities">Causal Quantities</a></li>
  <li><a href="#difference-in-differencess" id="toc-difference-in-differencess" class="nav-link" data-scroll-target="#difference-in-differencess">Difference-in-Differencess</a>
  <ul class="collapse">
  <li><a href="#canonical-difference-in-differences" id="toc-canonical-difference-in-differences" class="nav-link" data-scroll-target="#canonical-difference-in-differences">Canonical Difference-in-Differences</a></li>
  <li><a href="#average-treatment-effect-on-the-treated-att" id="toc-average-treatment-effect-on-the-treated-att" class="nav-link" data-scroll-target="#average-treatment-effect-on-the-treated-att">Average Treatment effect on the Treated (ATT)</a></li>
  <li><a href="#estimators-of-the-att" id="toc-estimators-of-the-att" class="nav-link" data-scroll-target="#estimators-of-the-att">Estimators of the ATT</a></li>
  <li><a href="#relaxing-did-assumptions-overview" id="toc-relaxing-did-assumptions-overview" class="nav-link" data-scroll-target="#relaxing-did-assumptions-overview">Relaxing DiD Assumptions Overview</a></li>
  <li><a href="#new-estimators" id="toc-new-estimators" class="nav-link" data-scroll-target="#new-estimators">New Estimators</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#question" id="toc-question" class="nav-link" data-scroll-target="#question">Question</a></li>
  <li><a href="#best-linear-predictor" id="toc-best-linear-predictor" class="nav-link" data-scroll-target="#best-linear-predictor">Best Linear Predictor</a></li>
  <li><a href="#dealing-with-data" id="toc-dealing-with-data" class="nav-link" data-scroll-target="#dealing-with-data">Dealing with Data</a></li>
  <li><a href="#bias-varience-tradeoff" id="toc-bias-varience-tradeoff" class="nav-link" data-scroll-target="#bias-varience-tradeoff">Bias Varience Tradeoff</a></li>
  <li><a href="#l1-and-l2-regularization-in-linear-regression" id="toc-l1-and-l2-regularization-in-linear-regression" class="nav-link" data-scroll-target="#l1-and-l2-regularization-in-linear-regression">L1 and L2 Regularization in Linear Regression</a></li>
  <li><a href="#the-math" id="toc-the-math" class="nav-link" data-scroll-target="#the-math">The Math</a></li>
  <li><a href="#whats-your-estimand" id="toc-whats-your-estimand" class="nav-link" data-scroll-target="#whats-your-estimand">What’s your estimand</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="OLS-published-version.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Regression</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Steven Rashin </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 22, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="tl-dr-checklist" class="level1">
<h1>TL; DR: Checklist</h1>
<ol type="1">
<li><p>Get summary stats for the variables. Check for outliers or weird patterns</p></li>
<li><p>Plot the independent variables against the dependent variable. Same goal as above.</p></li>
<li><p>Figure out the model and standard errors - you most likely want robust or cluster robust standard errors</p></li>
<li><p>Run the regression. If you have over 5 fixed effects (FEs), you can use a couple of approaches - if you don’t care about the FEs themselves, demean the dependent and independent variable of interest by group (i.e.&nbsp;<span class="math inline">\(y_{\text{demeaned}}=\alpha + \beta_1 x_\text{1 demeaned} + \beta_2 x_\text{2 demeaned}\)</span>) , or use plm or some other package</p></li>
<li><p>Check regression diagnostics</p>
<ul>
<li>Residuals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary.</li>
<li>Normal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required.</li>
<li>Scale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic.</li>
<li>Residuals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model’s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots. You can also do this using the <a href="#the-hat-matrix">The Hat Matrix!</a></li>
<li>Can also use <a href="https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html" class="uri">https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html</a></li>
</ul></li>
<li><p>Interpret the coefficients (see <a href="#sec-interpret">Section&nbsp;12</a>)</p>
<ul>
<li>No logged DV, no logged IV
<ul>
<li>A one unit increase in x, increases y by coefficient (<span class="math inline">\(\beta\)</span>) units</li>
</ul></li>
<li>No logged DV, logged IV
<ul>
<li>Divide the coefficient by 100, this tells us that a 1% increase in the independent variable increases (decreases) the dependent variable by coefficient/100 units . For an 10% increase multiply the coefficient by log(1.1) (for x increase log(1.x)). Suppose our coefficient is 5. The interpretation is that a one percent increase in x increases the dependent variable by 0.05. For every 10% increase in the independent variable, the dependent variable increases by 5 * log(1.1) = 0.47</li>
</ul></li>
<li>Logged DV, No logged IV
<ul>
<li>Exponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the y for every one-unit increase in the independent variable. exp(coef) – 1) * 100. Suppose your coefficient is 0.5, (exp(0.5)-1)*100 = 64.9, a one unit increase in your independent variable increases your dependent variably by 64.9%. If our coefficient were 5, the result would be 14,741.32%. This is unlikely!</li>
</ul></li>
<li>Logged DV, logged IV
<ul>
<li>Interpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. Suppose the coefficient is, again, 5. For every 1% increase in the independent variable, the dependent variable increases by 5%. Suppose, instead, we wanted an x percent increase - use the formula <span class="math inline">\((1.x^5 – 1) * 100\)</span>. For example, a 10% increase in our independent variable increases our dependent variable by <span class="math inline">\((1.1^5 - 1) * 100 = 61.05\%\)</span></li>
</ul></li>
</ul></li>
<li><p>Put regression into table form</p></li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/Checklist in code_f4f09985a299d52528898027ed93566b">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggfortify)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate some data:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">15</span>, <span class="dv">5</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">30</span>, <span class="dv">10</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#group &lt;- ifelse(x1&gt;50,1,0)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sim_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> x1,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> x2,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> y</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># summaries of variables</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>sim_data <span class="sc">%&gt;%</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  skimr<span class="sc">::</span><span class="fu">skim</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">Piped data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">1000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 16%">
<col style="width: 11%">
<col style="width: 16%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">x1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">15.13</td>
<td style="text-align: right;">5.06</td>
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">11.59</td>
<td style="text-align: right;">15.12</td>
<td style="text-align: right;">18.69</td>
<td style="text-align: right;">29.92</td>
<td style="text-align: left;">▁▅▇▅▁</td>
</tr>
<tr class="even">
<td style="text-align: left;">x2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">30.02</td>
<td style="text-align: right;">10.21</td>
<td style="text-align: right;">-5.90</td>
<td style="text-align: right;">23.15</td>
<td style="text-align: right;">30.02</td>
<td style="text-align: right;">36.96</td>
<td style="text-align: right;">60.52</td>
<td style="text-align: left;">▁▃▇▅▁</td>
</tr>
<tr class="odd">
<td style="text-align: left;">y</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">50.52</td>
<td style="text-align: right;">15.23</td>
<td style="text-align: right;">-3.65</td>
<td style="text-align: right;">40.11</td>
<td style="text-align: right;">51.70</td>
<td style="text-align: right;">60.55</td>
<td style="text-align: right;">95.42</td>
<td style="text-align: left;">▁▃▇▆▁</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot variables against DV to check for weirdness</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sim_data <span class="sc">%&gt;%</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="at">key =</span> <span class="st">"other_variable"</span>, <span class="at">value =</span> <span class="st">"value"</span>,<span class="sc">-</span>y) <span class="sc">%&gt;%</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(., <span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>other_variable)<span class="sc">+</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>() <span class="sc">+</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">"Check that each x is linearly related to the y"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/Checklist%20in%20code-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model, can also do cluster robust</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>std <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>robust <span class="ot">&lt;-</span> estimatr<span class="sc">::</span><span class="fu">lm_robust</span>(y <span class="sc">~</span> x1<span class="sc">+</span>x2)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Output the diagnostic plots.  Look for weirdness (won't find it here because we're in a nice simualted sandbox)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>residvfitted <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">autoplot</span>(std, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">1</span>)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">"Residuals vs Fitted - checks linear relationship assumption </span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="st">        of linear regression. A linear relationship will demonstrate a</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="st">        horizontal red line here. Deviations from a horizontal line</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="st">        suggest nonlinearity and that a different approach may be</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="st">        necessary."</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>qq <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">autoplot</span>(std, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">2</span>)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">"Normal Q-Q - checks whether or not the residuals (the difference</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="st">        between the observed and predicted values) from the model are</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="st">        normally distributed. The best fit models points fall along the</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="st">        dashed line on the plot. Deviation from this line suggests that</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="st">        a different analytical approach may be required."</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>scalevlocation <span class="ot">&lt;-</span>   ggplot2<span class="sc">::</span><span class="fu">autoplot</span>(std, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">3</span>)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">" Scale-Location - checks the homoscedasticity of the model. A</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="st">        horizontal red line with points equally spread out indicates a</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="st">        well-fit model. A non-horizontal line or points that cluster</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="st">        together suggests that your data are not homoscedastic."</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>residvleeverage <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">autoplot</span>(std, <span class="at">which =</span> <span class="fu">c</span>(<span class="dv">4</span>)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">"Residuals vs Leverage - helps to identify outlier or extreme</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="st">        values that may disproportionately affect the model's results.</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="st">        Their inclusion or exclusion from the analysis may affect the</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="st">        results of the analysis. Note that the top three most extreme</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="st">        values are identified with numbers next to the points in all</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="st">        four plots. You can also do this using the [The Hat Matrix!]"</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>residvfitted <span class="sc">+</span> qq </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/Checklist%20in%20code-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>scalevlocation <span class="sc">+</span> residvleeverage</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/Checklist%20in%20code-3.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lots of diagnostic options, see https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Summary for the output</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>modelsummary<span class="sc">::</span><span class="fu">modelsummary</span>(std)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">&nbsp;(1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: center;">5.334</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(1.391)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">x1</td>
<td style="text-align: center;">1.004</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.064)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">x2</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left; box-shadow: 0px 1.5px;"></td>
<td style="text-align: center; box-shadow: 0px 1.5px;">(0.031)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Num.Obs.</td>
<td style="text-align: center;">1000</td>
</tr>
<tr class="even">
<td style="text-align: left;">R2</td>
<td style="text-align: center;">0.556</td>
</tr>
<tr class="odd">
<td style="text-align: left;">R2 Adj.</td>
<td style="text-align: center;">0.555</td>
</tr>
<tr class="even">
<td style="text-align: left;">AIC</td>
<td style="text-align: center;">7478.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BIC</td>
<td style="text-align: center;">7498.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Log.Lik.</td>
<td style="text-align: center;">−3735.405</td>
</tr>
<tr class="odd">
<td style="text-align: left;">F</td>
<td style="text-align: center;">624.570</td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSE</td>
<td style="text-align: center;">10.14</td>
</tr>
</tbody>
</table>


</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>modelsummary<span class="sc">::</span><span class="fu">modelsummary</span>(robust)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">&nbsp;(1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: center;">5.334</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(1.421)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">x1</td>
<td style="text-align: center;">1.004</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.062)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">x2</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left; box-shadow: 0px 1.5px;"></td>
<td style="text-align: center; box-shadow: 0px 1.5px;">(0.032)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Num.Obs.</td>
<td style="text-align: center;">1000</td>
</tr>
<tr class="even">
<td style="text-align: left;">R2</td>
<td style="text-align: center;">0.556</td>
</tr>
<tr class="odd">
<td style="text-align: left;">R2 Adj.</td>
<td style="text-align: center;">0.555</td>
</tr>
<tr class="even">
<td style="text-align: left;">AIC</td>
<td style="text-align: center;">7478.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BIC</td>
<td style="text-align: center;">7498.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSE</td>
<td style="text-align: center;">10.14</td>
</tr>
</tbody>
</table>


</div>
</div>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>These are the sources I used to create this!</p>
<section id="less-math-heavy-or-math-heavy-with-intuitive-explanations" class="level3">
<h3 class="anchored" data-anchor-id="less-math-heavy-or-math-heavy-with-intuitive-explanations">Less Math Heavy or Math Heavy with Intuitive Explanations</h3>
<ul>
<li>A visual introduction to probability <a href="https://seeing-theory.brown.edu/" class="uri">https://seeing-theory.brown.edu/</a></li>
<li>Conditional probability visual guide <a href="https://setosa.io/conditional/" class="uri">https://setosa.io/conditional/</a></li>
<li>Biltzstein and Hwang’s probability textbook <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view" class="uri">https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view</a></li>
<li>Ethan Bueno de Mesquita and Anthony Fowler’s Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis</li>
<li>All Models are Wrong (a good visual guide to OLS) <a href="https://allmodelsarewrong.github.io/" class="uri">https://allmodelsarewrong.github.io/</a></li>
<li>Section 5.7 of <a href="https://jhudatascience.org/tidyversecourse/model.html#model-diagnostics" class="uri">https://jhudatascience.org/tidyversecourse/model.html#model-diagnostics</a></li>
<li>The bias-variance tradeoff <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="uri">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></li>
</ul>
</section>
<section id="bring-on-the-pain-aka-math" class="level3">
<h3 class="anchored" data-anchor-id="bring-on-the-pain-aka-math">Bring on the Pain (aka Math)</h3>
<ul>
<li>A guide to OLS via matrices <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" class="uri">https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf</a></li>
<li>Brandon Stewart’s class on applied social science stats - this is the best econometrics class I’ve seen from a slide perspective <a href="https://bstewart.scholar.princeton.edu/soc500" class="uri">https://bstewart.scholar.princeton.edu/soc500</a></li>
<li>Mauricio Romero’s lecture on Ordinary Least Squares <a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf" class="uri">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a></li>
<li><a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf" class="uri">http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf</a></li>
<li>Angrist and Pischke’s Mostly Harmless Econometrics</li>
<li>Notes on MHE with a good derivation of the law of iterated expectations <a href="https://www.leonardgoff.com/resources/mostlyharmlesslecturenotes.pdf" class="uri">https://www.leonardgoff.com/resources/mostlyharmlesslecturenotes.pdf</a></li>
</ul>
</section>
</section>
</section>
<section id="preliminaries" class="level1">
<h1>Preliminaries</h1>
<p>There are a lot of ways to implement Ordinary Least Squares (Before we do, note that ordinary means that all observations are treated equally. If we don’t want to do that, we can use weighted least squares or WLS.)</p>
<p>This guide covers OLS via:</p>
<ul>
<li>Linear Algebra</li>
<li>Method of Moments</li>
<li>Maximum Likelihood</li>
<li>Gradient Descent (See Gradient Descent Guide for more details)</li>
</ul>
<p>Remember: OLS is an estimator - it’s a machine that we plug data into and we get out estimates. As long as there is variation in our x, we get estimates (regardless of whether they make sense!).</p>
<p>We use it because we care about the relationship between an outcome, an independent variable of interest, while controlling for various factors. That is, the correlation cov(x,y) / <span class="math inline">\(\sigma_x \sigma_y\)</span> doesn’t capture the relationship we care about because there may be confounding (i.e.&nbsp;a common cause).</p>
</section>
<section id="our-goalsterms" class="level1">
<h1>Our Goals/Terms</h1>
<section id="goals" class="level2">
<h2 class="anchored" data-anchor-id="goals">Goals</h2>
<p>There are a few different ways to describe the goals of social science research:</p>
<ol type="1">
<li><p>Our goal is to learn about the data generating process that generated the sample (Brandon Stewart). Another way DGP are also described as real world process(es) that creates/generates the data we’re interested in. In other words, the DGP describes the rules that created variation in the population itself and in our data. DGPs are usually unknown, unless you’re simulating.</p></li>
<li><p>The goal of quantitative social science is not limited to uncovering causally identified facts; the goal is to harness (many) pieces of evidence to obtain an inference to the best explanation—both within and across studies. (Stewart and Spirling)</p></li>
<li><p>The goal of statistical inference is to learn about the unobserved population distribution, which can be characterized by parameters. We want to estimate these population parameters. (What is a parameter? It’s a particular aspect of a population distribution)</p></li>
</ol>
<p>Note that sometimes the unobserved population distribution and the DGP are sometimes considered the same. Matt Blackwell and Brandon Stewart, for example, sometimes call the population distribution the data generating process (DGP).</p>
</section>
<section id="sec-terms" class="level2">
<h2 class="anchored" data-anchor-id="sec-terms">Terms</h2>
<p>Estimands are the parameters that we aim to estimate (in other words “the estimand is the object of inquiry—it is the precise quantity about which we marshal data to draw an inference”). These are often written in Greek letters. Think of these as the truth. They are the result of the true data generating process.</p>
<p>Estimators are functions of sample data (i.e.&nbsp;statistics/rules) which we use to learn about the estimands. They are often written using modified Greek letters (e.g., <span class="math inline">\(\hat{\beta}\)</span>). These are procedures (e.g.&nbsp;means, variances, OLS). In concrete terms, Ordinary Least Squares is an estimator of some estimand, which was created by some data generating process (DGP).</p>
<p>Estimates are particular values of estimators realized in a given sample. They are denoted by English letters (e.g., X, <span class="math inline">\(x_i\)</span>) and are data from our sample. For example, the mean wealth of a person who completes college may be an estimate we care about.</p>
<p>Note that, theoretically, estimate = estimand + bias + noise (see e.g.&nbsp;Fowler and Bueno de Mesquita’s book). Where bias is a systematic error and noise is an idiosyncratic one that is particular to any observation. So our population estimate is a function of the true estimand, systematic bias, and irreducible noise. Our goal is for the bias to be as close to 0 as possible. This, however, is hard!</p>
<section id="dgp-rightarrow-sample-rightarrow-dgp" class="level3">
<h3 class="anchored" data-anchor-id="dgp-rightarrow-sample-rightarrow-dgp">DGP <span class="math inline">\(\rightarrow\)</span> Sample <span class="math inline">\(\rightarrow\)</span> DGP</h3>
<p>Putting this together:</p>
<ol start="0" type="1">
<li><p>(Unobserved) There is a data-generating process that creates the data/parameters we care about. E.g., some people get cancer because of a mutation, candidates for office spend money strategically to win elections, lobbyists argue that policy should change, etc… These processes create data that we’re interested in such as cancer rates/election spending/lobbying results</p></li>
<li><p>We decide to care about a population parameter - the estimand - the true effect of an intervention (e.g.&nbsp;the true effect of a cancer drug on mortality or the true effect of spending on campaign outcomes). ``Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g.&nbsp;a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not… The distinction between the theoretical and empirical estimands is subtle but important: the former may involve unobservable quantities such as counterfactuals while the latter involves only observable data.”</p></li>
<li><p>We then gather a sample of the population (or, maybe the whole population itself but then we say this population is a realization of a super-population), and measure the effect of this intervention on that sample. Say gather 1000 cancer patients or all congressional elections from 2000 - 2022.</p></li>
<li><p>Once we have sample, we select an estimator (e.g.&nbsp;mean, regression, etc…), which we use to learn about the estimand. Particular values of the estimator are estimates (e.g., population mean).</p></li>
<li><p>If we’ve done this well, we’re able to use our sample to talk about the population.</p></li>
</ol>
<p>So we have: data <span class="math inline">\(\rightarrow\)</span> calculation <span class="math inline">\(\rightarrow\)</span> estimate <span class="math inline">\(\rightarrow\)</span> (maybe) truth</p>
<p>Sources:</p>
<ul>
<li><p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture3handout.pdf" class="uri">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture3handout.pdf</a></p></li>
<li><p><a href="https://twitter.com/nickchk/status/1272993322395557888" class="uri">https://twitter.com/nickchk/status/1272993322395557888</a></p></li>
<li><p><a href="https://jamanetwork.com/journals/jama/fullarticle/2783611" class="uri">https://jamanetwork.com/journals/jama/fullarticle/2783611</a></p></li>
<li><p><a href="https://doi.org/10.1177%2F00031224211004187" class="uri">https://doi.org/10.1177%2F00031224211004187</a></p></li>
<li><p>Fowler and Bueno de Mesquita Book</p></li>
<li><p>What Good is a Regression? paper by Spirling and Stweart</p></li>
</ul>
</section>
</section>
<section id="ols-big-picture" class="level2">
<h2 class="anchored" data-anchor-id="ols-big-picture">OLS Big Picture</h2>
<ol type="1">
<li><p>Regression provides the best linear predictor for the dependent variable in the same way that the conditional expectation function (CEF) is the best unrestricted predictor of the dependent variable</p></li>
<li><p>If we prefer to think of approximating <span class="math inline">\(E(y_i|x_i)\)</span> as opposed to predicting <span class="math inline">\(y_i\)</span>, even if the CEF is nonlinear, regression provides the best linear approximation to it (Angrist and Pischke)</p></li>
<li><p>If the CEF is linear (i.e.&nbsp;if the process that produces the population distribution is linear), then it makes the most sense to use linear regression to estimate it. Restated, the population regression function is the best we can do in the class of all linear functions to approximate <span class="math inline">\(E(y_i|x_i)\)</span> (Angrist and Pischke)</p></li>
<li><p>Linear regression may be interesting even if the underlying CEF is not linear. <span class="math inline">\(E(y_i|x_i)\)</span>, is the minimum mean squared error predictor of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i\)</span> in the class of all functions of <span class="math inline">\(x_i\)</span> (Angrist and Pischke)</p></li>
<li><p><span class="math inline">\(\beta_{OLS}\)</span> is an estimator of a parameter we do not observe</p></li>
<li><p>The standard error is the standard deviation of the estimator</p></li>
<li><p>A confidence interval is an interval where we know with some probability the true estimate lives</p></li>
<li><p>A p-value is the largest probability of obtaining results at least as extreme as those actually observed, under the assumption that the null hypothesis is correct.</p></li>
<li><p>Regression anatomy helps us understand OLS as a “matching estimator” (try to compare observations that are alike in the Xs). (Note that this comes from MHE’s regression anatomy theorem. Suppose you have a multivariate regression <span class="math inline">\(Y_i = \beta_0 +\beta_1X_{1i}+\beta_2X_{2i}+\epsilon_i\)</span> and two auxillary regressions of <span class="math inline">\(X_{1i}\)</span> on <span class="math inline">\(X_{2i}\)</span>, and vice versa. The regression anatomy theorem states that <span class="math inline">\(\beta_1\)</span> captures the effect of <span class="math inline">\(\tilde{x}_{1i}\)</span> (the residuals from the regression of <span class="math inline">\(X_{1i}\)</span> on <span class="math inline">\(X_{2i}\)</span>) - the part of <span class="math inline">\(X_{1i}\)</span> not explained by <span class="math inline">\(X_2\)</span>.). Similarly <span class="math inline">\(\beta_2\)</span> captures the effect of <span class="math inline">\(\tilde{x}_{2i}\)</span> (the residuals from the regression of <span class="math inline">\(X_{2i}\)</span> on <span class="math inline">\(X_{ii}\)</span>) - the part of <span class="math inline">\(X_{2i}\)</span> not explained by <span class="math inline">\(X_1\)</span>.)</p></li>
</ol>
<p>Source</p>
<ul>
<li><a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf" class="uri">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a> - <a href="http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf" class="uri">http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf</a></li>
</ul>
</section>
</section>
<section id="is-my-regression-causal" class="level1">
<h1>Is My Regression Causal?</h1>
<p>Short answer: unless you have some exogenous variation or randomization, the answer is no.</p>
<p>Sekhon: “Without an experiment, a natural experiment, a discontinuity, or some other strong design, no amount of econometric or statistical modeling can make the move from correlation to causation persuasive. (Sekhon, 2009, p.&nbsp;503)</p>
<p>Angrist and Pischke: Regression is causal when the corresponding conditional expectation function (CEF) is causal. If, for example <span class="math inline">\(Y_i\)</span> is fall grades and <span class="math inline">\(D_i\)</span> is a treatment dummy indicating students receiving randomized GPA incentives, then <span class="math inline">\(E[Y_i|D_i,Wi]\)</span> has a causal interpretation, revealing differences in average potential GPAs indexed by <span class="math inline">\(D_i\)</span>, conditional on control variables, <span class="math inline">\(W_i\)</span>. The regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(D_i\)</span> and <span class="math inline">\(W_i\)</span> inherits this CEFs causal interpretation</p>
<p>At the end of the day, OLS (and other matching/weighting estimators) “mop up” imbalances that makes CIA plausible</p>
<p>Thought experiment necessary to test CIA:</p>
<ul>
<li><p>How could it be that two units that are identical with respect to all meaningful background factors nonetheless receive different treatment?</p></li>
<li><p>Your answer to this question is your source of identification</p></li>
</ul>
<p>Sources</p>
<ul>
<li><p><a href="https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.11.060606.135444" class="uri">https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.11.060606.135444</a></p></li>
<li><p><a href="http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf" class="uri">http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf</a></p></li>
<li><p><a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf" class="uri">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a></p></li>
</ul>
</section>
<section id="deriving-ols" class="level1">
<h1>Deriving OLS</h1>
<section id="method-of-moments" class="level2">
<h2 class="anchored" data-anchor-id="method-of-moments">Method of Moments</h2>
<p>This is the method used in Scott Cunningham’s Causal Inference Mixtape and Angrist and Pischke’s Mostly Harmless Econometrics.</p>
<p>Think of the population regression as a moment of the population distribution. This proceeds in two steps:</p>
<ol type="1">
<li><p>First we derive an estimator for the population regression coefficient</p></li>
<li><p>Then we replace it with the sample analog.</p></li>
</ol>
<p>Let’s assume that the data generating process in the sky follows the following form:</p>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1x_i + u_i\]</span> That is, every y is a function of an intercept <span class="math inline">\(\beta_0\)</span>, a coefficient <span class="math inline">\(\beta_1\)</span> on <span class="math inline">\(x_i\)</span> and a random error <span class="math inline">\(u_i\)</span>. To make it concrete, assume that your earnings at age 40 are only systematically determined by your education and some random error <span class="math inline">\(u_i\)</span>. So nothing else matters systematically. That is, majoring in electrical engineering has the exact same returns as majoring in underwater basket weaving if they both take four years to complete. Except for a random error (e.g., sometimes electrical engineers decide to live as artists). This is a very strong assumption because we have to believe that everyone’s ability is the same.</p>
<p>How do we go from the above to a regression?</p>
<p>We need two assumptions, laid out below:</p>
<ol type="1">
<li><p>We need to assume <span class="math inline">\(E(u_i)=0\)</span>. That is, the expected value of any error term is 0. This doesn’t mean that every error term is 0 for each i. Quite the contrary! Some errors are positive, some are negative, but the mean of the distribution is 0.</p></li>
<li><p>We also need to assume <span class="math inline">\(E(u_i|x_i)=E(u_i)\)</span> This assumption - mean independence - that states that the disturbances average out to 0 for any value of X. Put differently, no observations of the independent variables convey any information about the expected value of the disturbance.</p>
<ul>
<li><p>This implies that: <span class="math inline">\(E(u_i | x_i)=E(u_i)=0\)</span> and <span class="math inline">\(E(u_ix_i)=E(E(u_i|x_i))=0\)</span>. From these we can derive the population regression coefficients.</p></li>
<li><p>At a population level:</p></li>
</ul></li>
</ol>
<p><span class="math display">\[E[Y|X] = \beta_0 + \beta_1x_i\]</span> <span class="math inline">\(E[Y|X]\)</span> is a population level regression function or conditional expectation function (i.e.&nbsp;mean). We solve for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></p>
<p>Solving for <span class="math inline">\(\beta_0\)</span></p>
<p><span class="math display">\[E[u_i|x_i] = E[y_i-\beta_0-\beta_1x_i] = 0\]</span> <span class="math display">\[E[y_i]-\beta_0-E[\beta_1x_i] = 0\]</span> Above, note that <span class="math inline">\(\beta_0\)</span> is a constant so <span class="math inline">\(E[\beta_0]=\beta_0\)</span></p>
<p><span class="math display">\[E[y_i]-\beta_1 E[x_i] = \beta_0\]</span> Now do <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[E[u_ix_i] = 0 \]</span> Replace <span class="math inline">\(u_i\)</span> with <span class="math inline">\(y-\beta_0-\beta_1x_i\)</span>. A note about this proof - be careful with expectations and with minus signs</p>
<p><span class="math display">\[E[(y_i-\beta_0-\beta_1x_i)x_i] = 0  \]</span> We note <span class="math inline">\(x_i\)</span> is a scaler, so we can move it to the front.</p>
<p><span class="math display">\[E [x_i(y_i-\beta_0-\beta_1 x_i)]= 0 \]</span> Now replace <span class="math inline">\(\beta_0\)</span> with what we derived above <span class="math inline">\(E[y_i]-\beta_1 E[x_i] = \beta_0\)</span></p>
<p>Remember that since the term is <span class="math inline">\(-\beta_1\)</span> we need to distribute the minus!</p>
<p><span class="math display">\[E[x_i(y_i - E[y_i]+\beta_1 E[x_i] - \beta_1 x_i)] = 0 \]</span></p>
<p><span class="math display">\[E[x_i(y_i - E[y_i] - \beta_1 x_i  + \beta_1 E[x_i]) ] = 0 \]</span> Factor out <span class="math inline">\(\beta_1\)</span>. Watch the minus sign on the <span class="math inline">\(E[x_i]\)</span>!</p>
<p><span class="math display">\[E[x_i(y_i - E[y_i] - \beta_1 (x_i  - E[x_i])) ] = 0 \]</span> <span class="math display">\[E[x_i(y_i - E[y_i])] = \beta_1 E [x_i  (x_i  - E[x_i]) ] \]</span> On the right hand side, we note that <span class="math inline">\(E[(x_i - E[x_i])^2] = E[x_i^2 - E[x]^2]\)</span>. Given that this took me a few hours to figure out, here’s the proof. Note that we start with <span class="math inline">\(E[(x_i - E[x_i])^2] = E[x_i^2 - 2 x_i E[x_i]-E[x_i]^2]\)</span>. Now we distribute the expectations to all the terms. <span class="math inline">\(E[x_i^2] - E[2 x_i E[x_i]]+E[E[x_i]^2]]\)</span>. Now clean up the expectations. <span class="math inline">\(E[x_i^2] - 2 E[ x_i^2 ]+E[x_i]^2 = E[x_i^2]-E[x_i]^2 = E[(x_i-E[x_i])(x_i-E[x_i])]\)</span></p>
<p><span class="math display">\[E[x_i(y_i - E[y_i])] = \beta_1 E[(x_i-E[x_i])(x_i-E[x_i])]  \]</span> Note that we can do the same thing to the left hand side too. So <span class="math inline">\(E[x_i(y_i - E[y_i])] = E[(x_i-E[x_i])(y_i-E[y_i])]\)</span></p>
<p>Now, putting it all together:</p>
<p><span class="math display">\[\frac{E[(x_i-E[x_i])(y_i-E[y_i])]}{E[(x_i-E[x_i])(x_i-E[x_i])]} = \beta_1 \]</span> <span class="math display">\[\frac{\text{population covariance}}{\text{population variance  }} = \beta_1 \]</span> But! we don’t have the population x or y or E[x] or E[y]. So we have to use the sample analogs.</p>
<p><span class="math display">\[\frac{1}{N}\sum_{i=1}^N [Y_i - \hat{\beta}_0-\hat{\beta}_1 X_i] = 0 \]</span> and</p>
<p><span class="math display">\[\frac{1}{N}\sum_{i=1}^N [X_i(Y_i - \hat{\beta}_0-\hat{\beta}_1 X_i)] = 0 \]</span></p>
<p>We can rewrite the solution for <span class="math inline">\(\beta_0\)</span> as <span class="math display">\[ \beta_0 = \bar{Y}-\hat{\beta_1}\bar{X} \]</span></p>
<p>If we plug that into the second equation, we get:</p>
<p><span class="math display">\[\frac{1}{N}\sum_{i=1}^N X_i[Y_i - \bar{Y}-\hat{\beta_1}\bar{X} -\hat{\beta}_1 X_i] = 0 \]</span></p>
<p>Now separate the <span class="math inline">\(\beta_1\)</span>’s to the right hand side</p>
<p><span class="math display">\[\frac{1}{N}\sum_{i=1}^N X_i(Y_i - \bar{Y}) = \hat{\beta_1} \frac{1}{N}\sum_{i=1}^N X_i (X_i - \bar{X}) \]</span> <span class="math display">\[ \hat{\beta_1} = \frac{\frac{1}{N}\sum_{i=1}^N X_i(X_i - \bar{Y}) }{\frac{1}{N}\sum_{i=1}^N X_i (X_i - \bar{X})} \]</span> Which is equivalent to:</p>
<p><span class="math display">\[\hat{\beta_1} = \frac{\frac{1}{N}\sum_{i=1}^N (X_i - \bar{X})(Y_i-\bar{Y}) }{\frac{1}{N}\sum_{i=1}^N (X_i - \bar{X}) (X_i - \bar{X})} = \frac{\text{Covariance(X,Y)}}{\text{Variance(X)}}  \]</span></p>
</section>
<section id="linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra">Linear Algebra</h2>
<p>In this section we derive the OLS model using linear algebra. Before getting into the derivatives, note the properties of transposes and matrix derivatives necessary to do this in the section below.</p>
<section id="preliminaries-1" class="level3">
<h3 class="anchored" data-anchor-id="preliminaries-1">Preliminaries</h3>
<ul>
<li>X is an <span class="math inline">\(x \times k + 1\)</span> matrix with k variables and n observations. The plus 1 means we have an intercept as denoted by a vector of ones in the X matrix. This means <span class="math inline">\(X'\)</span> or <span class="math inline">\(X^T\)</span> is <span class="math inline">\((k + 1) \times n\)</span></li>
<li>e/u are <span class="math inline">\(n \times 1\)</span> vectors</li>
<li>y is an <span class="math inline">\(n \times 1\)</span> vector</li>
<li><span class="math inline">\(\beta\)</span> is an <span class="math inline">\(n \times 1\)</span> vector</li>
<li>A and B are matrices, a and b are vectors</li>
<li>n is the number of observations, k is the number of predictors</li>
<li><span class="math inline">\(y_i\)</span> denotes the i’th value of <span class="math inline">\(y\)</span></li>
</ul>
</section>
<section id="matrix-transpose-and-derivative-rules" class="level3">
<h3 class="anchored" data-anchor-id="matrix-transpose-and-derivative-rules">Matrix Transpose and Derivative Rules</h3>
<p>Transpose rules:</p>
<ul>
<li>(AB)’ = B’A’<br>
</li>
<li>(a’Bc)’ = c’B’a<br>
</li>
<li>a’b = b’a - (A + B)C = AC + BC<br>
</li>
<li>(a + b)’C = a’C + b’C<br>
</li>
<li><span class="math inline">\(AB \neq BA\)</span></li>
</ul>
<p>Derivative rules for taking the derivative with respect to X</p>
<ul>
<li><span class="math inline">\(X'B \rightarrow B\)</span></li>
<li><span class="math inline">\(x'b \rightarrow b\)</span></li>
<li><span class="math inline">\(x'x \rightarrow 2x\)</span></li>
<li><span class="math inline">\(x'Bx \rightarrow 2Bx\)</span></li>
</ul>
<p>See <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf" class="uri">http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf</a></p>
</section>
<section id="the-derivation" class="level3">
<h3 class="anchored" data-anchor-id="the-derivation">The Derivation</h3>
<p>Our goal here is to derive the coefficient <span class="math inline">\(\beta\)</span>.</p>
<p>So we start with argmin <span class="math inline">\(e'e\)</span></p>
<p><span class="math display">\[e'e = (y-X\hat{\beta})'(y-X\hat{\beta})\]</span></p>
<p><span class="math display">\[= y'y - y' X \hat{\beta} - \hat{\beta}'X'y + \hat{\beta}'X'X\hat{\beta} \]</span></p>
<p>Note above that <span class="math inline">\(y'\)</span> is <span class="math inline">\(1 \times n\)</span>. So <span class="math inline">\(y'X\hat{\beta}\)</span> is <span class="math inline">\((1 \times n) (k \times n) (n \times 1)\)</span> also <span class="math inline">\(\hat{\beta}'X'y\)</span> is <span class="math inline">\((1 \times k) (k \times n) (n \times 1) \rightarrow 1 \times 1\)</span>. We choose <span class="math inline">\(\hat{\beta}'X'y\)</span> to make the final step of the proof work. (We could also rearrange because X’y = y’X)</p>
<p><span class="math display">\[ =  y'y - 2 \hat{\beta}'X'y +    \hat{\beta}'X'X\hat{\beta}         \]</span> We minimize this quantity by taking the partial derivative with respect to <span class="math inline">\(\hat{\beta}\)</span> and setting it equal to 0 and solving for <span class="math inline">\(\hat{\beta}\)</span></p>
<p><span class="math display">\[\frac{\partial e'e}{\partial \beta} = -2X'y + 2X'X\hat{\beta}               \]</span></p>
<p><span class="math display">\[0 = -2X'y + 2X'X\hat{\beta}               \]</span></p>
<p><span class="math display">\[2X'y = 2X'X\hat{\beta}               \]</span> <span class="math display">\[  X'y = X'X\hat{\beta}                \]</span> <span class="math display">\[ (X'X)^{-1} X'y = \hat{\beta}   \]</span></p>
<p>The best guide for what this looks like in practice is <a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf" class="uri">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a> slides 99-105 (page numbers are n plus 1)</p>
</section>
</section>
<section id="maximum-likelihood-derivation-of-ols" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-derivation-of-ols">Maximum Likelihood Derivation of OLS</h2>
<p>Assume that <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>. That means we know that <span class="math inline">\(y_i \sim N(\hat{b}'x_i,\sigma^2)\)</span></p>
<p>We start with the joint distribution of <span class="math inline">\(y_1,y_2,y_3 \dots y_n\)</span></p>
<p><span class="math display">\[P(y|X,b,\sigma^2) = \prod_{i=1}^nf(y_i;X,b,\sigma^2) \]</span> <span class="math display">\[P(y|X,b,\sigma^2) = \prod_{i=1}^n \frac{1}{\sigma^2 \sqrt{2 \pi} } \text{exp}(-\frac{1}{2\sigma^2}(y_i-b'x_i)^2) \]</span> Distribute the <span class="math inline">\(\prod\)</span> to the exponent, but remember that it replicates the constant <span class="math inline">\(\frac{1}{2\pi\sigma^2}\)</span> <span class="math inline">\(c^n\)</span> times where c is the constant and n are the number of replications of the product notation.</p>
<p><span class="math display">\[(2 \pi \sigma^2)^{\frac{-n}{2}} \text{exp}(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-b'x_i)^2)  \]</span></p>
<p>Now we take the logarithm of each side. This gives us the log likelihood which we’re going to… maximize!</p>
<p><span class="math display">\[ l = \ln[P(y|X,b,\sigma^2)]\]</span> <span class="math display">\[\ln(l) = \ln((2 \pi \sigma^2)^{\frac{-n}{2}} \text{exp}(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-b'x_i)^2) ) \]</span></p>
<p><span class="math display">\[l = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-b'x_i)^2  \]</span> Here’s a neat little step - replace the row-notation above with matrix notation.</p>
<p><span class="math display">\[\ln l = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2} (\textbf{y}-\textbf{Xb})'(\textbf{y}-\textbf{Xb})  \]</span></p>
<p>Now we have an expression for the log likelihood - so we want to … maximize it (i.e.&nbsp;for maximum likelihood!). What we’re going to do in a few steps is take the partial derivative with respect to b (i.e.&nbsp;<span class="math inline">\(\frac{\partial l}{\partial b}\)</span>) and set the whole thing to 0. This means that, at this point, we don’t care about terms that don’t have a b in them. I.e. the first term <span class="math inline">\(-\frac{n}{2}\ln(2\pi\sigma^2)\)</span> does not have a b in it at all, so we can just call that <span class="math inline">\(c\)</span>.</p>
<p><span class="math display">\[\ln l = c - \frac{1}{2\sigma^2} (\textbf{y}-\textbf{Xb})'(\textbf{y}-\textbf{Xb})    \]</span> <span class="math display">\[\ln l = c - \frac{1}{2\sigma^2} (\textbf{y'y} + \textbf{b'X'Xb} - \textbf{2b'X'y})    \]</span> Now do the partial derivative and set them equal to 0.</p>
<p><span class="math display">\[ \frac{\partial l}{\partial b} =  2\textbf{X'Xb} - \textbf{2X'y} = 0\]</span></p>
<p><span class="math display">\[ \frac{\partial l}{\partial b} \rightarrow 2\textbf{X'Xb} = \textbf{2X'y} \]</span> <span class="math display">\[\hat{\textbf{b}}_{ML} =  (\textbf{X'X})^{-1}\textbf{X'y}\]</span> To get the variance - take the partial with respect to <span class="math inline">\(\sigma\)</span></p>
<p>Source:</p>
<ul>
<li><a href="https://allmodelsarewrong.github.io/olsml.html" class="uri">https://allmodelsarewrong.github.io/olsml.html</a></li>
</ul>
</section>
<section id="gradient-descent-for-ols" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-for-ols">Gradient Descent for OLS</h2>
<p>See Gradient Descent help file! Here we’re not deriving OLS but we’re estimating the coefficients analytically.</p>
<p>Source:</p>
<ul>
<li><a href="https://allmodelsarewrong.github.io/gradient.html" class="uri">https://allmodelsarewrong.github.io/gradient.html</a></li>
<li><a href="https://www.ocf.berkeley.edu/~janastas/stochastic-gradient-descent-in-r.html" class="uri">https://www.ocf.berkeley.edu/~janastas/stochastic-gradient-descent-in-r.html</a></li>
</ul>
</section>
<section id="are-methods-equivalent" class="level2">
<h2 class="anchored" data-anchor-id="are-methods-equivalent">Are Methods Equivalent?</h2>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-1_5523eceece921b77f6804d22ca0a28db">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(haven)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: zoo</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'zoo'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    as.Date, as.Date.numeric</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sandwich)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sjPlot)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#DGP for x</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">rnorm</span>(<span class="dv">10000</span>,<span class="at">mean =</span> <span class="dv">13</span>, <span class="at">sd =</span> <span class="dv">5</span>))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">4000</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">40000</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> a <span class="sc">+</span> b <span class="sc">*</span> x <span class="sc">+</span> u</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>DGP <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> x, </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> y</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(DGP, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y)) <span class="sc">+</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Truth"</span>,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Simulated X"</span>,</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Simulated Y"</span>,</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> <span class="fu">stat_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> y <span class="sc">~</span> x, <span class="at">geom =</span> <span class="st">"smooth"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Method of moments </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(x, y) <span class="sc">/</span> <span class="fu">var</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3991.188</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Algebra </span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>x_mat <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, x)) <span class="co"># remember to add column of ones!</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>( (<span class="fu">t</span>(x_mat) <span class="sc">%*%</span> x_mat) ) <span class="sc">%*%</span> (<span class="fu">t</span>(x_mat) <span class="sc">%*%</span> y) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]
  49988.649
x  3991.188</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MLE</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>loglikMLE <span class="ot">&lt;-</span> <span class="cf">function</span>(par, y){</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">as.vector</span>(x)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(<span class="fu">dnorm</span>(y, <span class="at">mean =</span> par[<span class="dv">1</span>] <span class="sc">+</span> par[<span class="dv">2</span>]<span class="sc">*</span>x, <span class="at">sd =</span> par[<span class="dv">3</span>], <span class="at">log =</span> T))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>MLE <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="at">intercept =</span> <span class="dv">9000</span>, <span class="at">x =</span> <span class="dv">400</span>, <span class="at">sigma =</span> <span class="dv">1</span>), <span class="at">fn =</span> loglikMLE, <span class="co">#parameters to be estimated, function to optimise</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">y =</span> <span class="fu">as.vector</span>(y), <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">reltol =</span> <span class="fl">1e-16</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>               <span class="co">#we specify -1 in the controls so that we MAXIMIZE rather than minimize, which is the default</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>               )</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(MLE<span class="sc">$</span>par,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>intercept         x     sigma 
  49908.3    3997.4   40364.5 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Via Gradient Descent </span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>cost <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, theta) {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>( (X <span class="sc">%*%</span> theta <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span> ) <span class="sc">/</span> (<span class="dv">2</span><span class="sc">*</span><span class="fu">length</span>(y))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">#We must also set two additional parameters: learning rate and iteration limit</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>num_iters <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># keep history</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>cost_history <span class="ot">&lt;-</span> <span class="fu">double</span>(num_iters)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>theta_history <span class="ot">&lt;-</span> <span class="fu">list</span>(num_iters)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize coefficients</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">nrow=</span><span class="dv">2</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># add a column of 1's for the intercept coefficient</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">matrix</span>(x))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient descent</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iters) {</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> (X <span class="sc">%*%</span> theta <span class="sc">-</span> y)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>  delta <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> error <span class="sc">/</span> <span class="fu">length</span>(y)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> theta <span class="sc">-</span> alpha <span class="sc">*</span> delta</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>  cost_history[i] <span class="ot">&lt;-</span> <span class="fu">cost</span>(X, y, theta)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>  theta_history[[i]] <span class="ot">&lt;-</span> theta</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>gradient_descent_ols_estimates <span class="ot">&lt;-</span> theta[,<span class="dv">1</span>] <span class="sc">%&gt;%</span> <span class="fu">tibble</span>()</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="co"># how close is this to truth</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>est <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, DGP)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>truth_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(a, b)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>est<span class="sc">$</span>coefficients <span class="sc">%&gt;%</span> </span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(truth_vec) <span class="sc">%&gt;%</span> </span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(<span class="fu">c</span>(<span class="st">"a"</span>,<span class="st">"b"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(gradient_descent_ols_estimates) <span class="sc">%&gt;%</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(MLE<span class="sc">$</span>par[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]) <span class="sc">%&gt;%</span></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="st">"OLS Estimate"</span> <span class="ot">=</span> <span class="dv">1</span>, <span class="st">"Truth"</span> <span class="ot">=</span> <span class="dv">2</span>, <span class="st">"Parameter"</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">"Gradient Descent"</span> <span class="ot">=</span> <span class="dv">4</span>, <span class="st">"MLE"</span> <span class="ot">=</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">relocate</span>(Parameter, <span class="at">.before =</span> <span class="st">`</span><span class="at">OLS Estimate</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">relocate</span>(Truth, <span class="at">.before =</span> <span class="st">`</span><span class="at">OLS Estimate</span><span class="st">`</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
• `` -&gt; `...2`</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
New names:
New names:
• `` -&gt; `...3`</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 5
  Parameter Truth `OLS Estimate` `Gradient Descent`    MLE
  &lt;chr&gt;     &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;
1 a         50000         49989.             49989. 49908.
2 b          4000          3991.              3991.  3997.</code></pre>
</div>
</div>
</section>
</section>
<section id="variance-of-beta---sigma2" class="level1">
<h1>Variance of <span class="math inline">\(\beta\)</span> - <span class="math inline">\(\sigma^2\)</span></h1>
<p>We’ve shown how to derive the OLS model above using method of moments, linear algebra and maximum likelihood.</p>
<p>First a necessary (and, totally pedantic) note:</p>
<ul>
<li><p>Errors/disturbances are the vertical distances between observations and the unknown Conditional Expectation Function. Therefore, they are unknown.</p></li>
<li><p>Residuals are the vertical distances between observations and the estimated regression function. Therefore, they are known.</p></li>
</ul>
<p>If <span class="math inline">\(\hat{\beta}= (X'X)^{-1}X'y\)</span> and <span class="math inline">\(y = X\beta + u\)</span></p>
<p><span class="math display">\[\hat{\beta} = (X'X)^{-1}X'(X\beta + u) = (X'X)^{-1}X'X\beta + (X'X)^{-1}X'u = I\beta + (X'X)^{-1}X'u\]</span> Now take the variance of each side</p>
<p><span class="math display">\[V(\hat{\beta}|X) = V(\beta + (X'X)^{-1}X'u|X) \]</span> <span class="math display">\[V(\hat{\beta}|X) = V(\beta|X) + V((X'X)^{-1}X'u|X) \]</span> In the above, note that <span class="math inline">\(V(\beta|X)=0\)</span>, since <span class="math inline">\(\beta\)</span> is constant</p>
<p>To get to the next step, note that when A is a constant matrix and X is random, <span class="math inline">\(V(AX) = A V(X) A'\)</span> and <span class="math inline">\(u = y - X\beta\)</span>. We’re not going to substitute in the <span class="math inline">\(u\)</span> but point this out as to why it remains trapped in the variance operator</p>
<p><span class="math display">\[V(\hat{\beta}|X) = (X'X)^{-1}X' V(u|X) X(X'X)^{-1} \]</span></p>
<p>Note that <span class="math inline">\(V(u|X) = \sigma^2\)</span> by the Gauss-Markov assumptions</p>
<p>So we’re left with:</p>
<p><span class="math display">\[V(\hat{\beta}|X) = (X'X)^{-1}X' \sigma^2 X(X'X)^{-1} = \sigma^2(X'X)^{-1} \]</span> But we don’t have <span class="math inline">\(\sigma^2\)</span> - that’s a population concept. So we estimate it with <span class="math inline">\(\hat{\sigma}^2=\frac{u'u}{n-k}\)</span>.</p>
<p>What does this look like?</p>
<p>It means, essentially, that the diagonals of the variance-covariance matrix, are the standard errors of <span class="math inline">\(\hat{\beta}\)</span></p>
<p>See <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" class="uri">https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf</a></p>
<section id="longer-proof" class="level3">
<h3 class="anchored" data-anchor-id="longer-proof">Longer proof</h3>
<p>We can also start from Var(<span class="math inline">\(\hat{\beta}) = E[(\hat{\beta}-E[\beta])]^2\)</span></p>
<p>First, replace <span class="math inline">\(\hat{\beta}\)</span> with <span class="math inline">\((X'X)^{-1}X'Y\)</span> <span class="math display">\[= E[(X'X)^{-1}X'Y - E[\beta]]^2 \]</span> Now replace <span class="math inline">\(Y\)</span> with <span class="math inline">\(X\beta + e\)</span> <span class="math display">\[= E[(X'X)^{-1}X'(X\beta + e) - E[\beta]]^2  \]</span> We know that <span class="math inline">\(E[\beta] = \beta\)</span>, so make that sub <span class="math display">\[= E[(X'X)^{-1}X'(X\beta + e) - \beta]^2  \]</span> Do the multiplication <span class="math display">\[= E[(X'X)^{-1}X'X\beta + (X'X)^{-1}X'e - \beta]^2  \]</span> Recognize <span class="math inline">\((X'X)^{-1}X'X = I\)</span> <span class="math display">\[= E[I\beta + (X'X)^{-1}X'e - \beta]^2  \]</span> We’ve added and subtracted a <span class="math inline">\(\beta\)</span> so they cancel out <span class="math display">\[= E[(X'X)^{-1}X'e]^2  \]</span> Expand the square. Note that X is fixed so E(X)=X <span class="math display">\[= ((X'X)^{-1}X'e )^ 2  \]</span> Note that to square in matrix algebra we have to take the prime of the original term (i.e.&nbsp;XX is non-conformable unless X is square, but we can do XX’ and that would be an nkkn matrix or n*n)</p>
<p><span class="math display">\[= (X'X)^{-1}X'e e'X(X'X)^{-1}  \]</span></p>
<p>Notice that we have <span class="math inline">\((X'X)^{-1}X'e e'X\)</span></p>
<p><span class="math display">\[= \sigma^2 I (X'X)^{-1} \]</span></p>
<p>From the last step take the square root of the diagonals to get the standard errors</p>
</section>
<section id="errors-in-matrix-form-in-r" class="level2">
<h2 class="anchored" data-anchor-id="errors-in-matrix-form-in-r">Errors in Matrix Form in R</h2>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-2_abf73b7e7e6dcad7364e885b247e5bd0">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">30</span>, <span class="dv">10</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="dv">4</span> <span class="sc">+</span> <span class="dv">5</span> <span class="sc">*</span> x1 <span class="sc">+</span> <span class="dv">7</span> <span class="sc">*</span> x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">10</span>) </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x1 <span class="sc">+</span> x2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y1 ~ x1 + x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-31.0138  -6.3202   0.5665   6.4822  28.6204 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.62275    1.10495   2.374   0.0178 *  
x1           5.03229    0.05966  84.352   &lt;2e-16 ***
x2           7.02873    0.02956 237.753   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.581 on 997 degrees of freedom
Multiple R-squared:  0.9845,    Adjusted R-squared:  0.9844 
F-statistic: 3.158e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="dv">1</span>, x1, x2) </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(X)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(y1)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># vcov in R</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>e <span class="ot">=</span> y <span class="sc">-</span> X <span class="sc">%*%</span> beta</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># this is as above</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>std_error <span class="ot">&lt;-</span> ( <span class="fu">t</span>(e) <span class="sc">%*%</span> e  <span class="sc">/</span> (<span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="sc">-</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>]) )[<span class="dv">1</span>,<span class="dv">1</span>] <span class="sc">*</span> (<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X))</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Robust standard errors</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co">#https://library.virginia.edu/data/articles/understanding-robust-standard-errors</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co"># n / (n-k) * u_hat^2</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>HC1 <span class="ot">&lt;-</span>  (<span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="sc">/</span>  (<span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="sc">-</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>]) ) <span class="sc">*</span> (e <span class="sc">%*%</span> <span class="fu">t</span>(e))</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>vce_hc1 <span class="ot">&lt;-</span> (<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> ((HC1 <span class="sc">*</span> <span class="fu">diag</span>(<span class="fu">dim</span>(X)[<span class="dv">1</span>])) <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)) </span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="co"># show all results are equivalent</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">our_beta =</span> beta,</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">their_beta =</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x1 <span class="sc">+</span> x2)<span class="sc">$</span>coefficients,</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">our_error =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(std_error)),</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">their_error =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">vcov</span>(<span class="fu">lm</span>(y1 <span class="sc">~</span> x1 <span class="sc">+</span> x2)))),</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">our_robust_ses =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(vce_hc1)),</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">their_robust_ses =</span> <span class="fu">coeftest</span>(m, <span class="at">vcov. =</span> <span class="fu">vcovHC</span>(m, <span class="at">type =</span> <span class="st">'HC1'</span>))[,<span class="dv">2</span>]</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 6
  our_beta[,1] their_beta our_error their_error our_robust_ses their_robust_ses
         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;
1         2.62       2.62    1.10        1.10           1.08             1.08  
2         5.03       5.03    0.0597      0.0597         0.0607           0.0607
3         7.03       7.03    0.0296      0.0296         0.0283           0.0283</code></pre>
</div>
</div>
</section>
<section id="what-if-the-homoskedasticity-assumption-is-wrong" class="level2">
<h2 class="anchored" data-anchor-id="what-if-the-homoskedasticity-assumption-is-wrong">What if the homoskedasticity assumption is wrong?</h2>
<p>We can use Huber-White standard errors. These state that instead of replacing the <span class="math inline">\(ee'\)</span> with <span class="math inline">\(\sigma^2I\)</span>, we let the errors vary <span class="math inline">\((X'X)^{-1}X'ee'(X'X)^{-1}X'\)</span></p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-3_0a24b5c07036cb2b5feb38538f272fcf">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Robust standard errors</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co">#https://library.virginia.edu/data/articles/understanding-robust-standard-errors</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># n / (n-k) * u_hat^2</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>HC1 <span class="ot">&lt;-</span>  (<span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="sc">/</span>  (<span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="sc">-</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>]) ) <span class="sc">*</span> (e <span class="sc">%*%</span> <span class="fu">t</span>(e))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>vce_hc1 <span class="ot">&lt;-</span> (<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> ((HC1 <span class="sc">*</span> <span class="fu">diag</span>(<span class="fu">dim</span>(X)[<span class="dv">1</span>])) <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)) </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate using sqrt of diagonal</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">diag</span>(vce_hc1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="assumptions-of-ols" class="level1">
<h1>Assumptions of OLS</h1>
<p>Gauss-Markov Assumptions in Matrix Form.</p>
<ol type="1">
<li><p>Linearity: <span class="math inline">\(y = X\beta + u\)</span> i.e.&nbsp;<span class="math inline">\(Y_i = \beta_0 + \beta_1X_i + \beta_2Z_i + ... + u_i\)</span>. This means that the population model is linear in its parameters and correctly specified. In other words, we assume this to be the structural model, i.e., the model describing the true process generating Y. By linear in the parameters, we allow the variables (i.e.&nbsp;the <span class="math inline">\(X_i\)</span> or <span class="math inline">\(Z_i\)</span>) to be non-linear. So we could have <span class="math inline">\(Y_i = \beta_0 + \beta_1(X_i^2) + \beta_2 \log (Z_i) + ... + u_i\)</span> and this would be ok.</p></li>
<li><p>Random/iid sample: <span class="math inline">\((y_i, x_i')\)</span> are an iid sample from the population. The observed data represent a random sample from the population described by the model. This is violated in time-series, and when the sample doesn’t represent the population.</p></li>
<li><p>No perfect collinearity: <span class="math inline">\(X\)</span> is an <span class="math inline">\(n \times (k + 1)\)</span> matrix with rank k + 1. There is variation in the explanatory variable. This is the identification condition that allows us to invert a matrix and get <span class="math inline">\(\beta\)</span>. In the bivariate case, this means that there is variation in x.</p></li>
<li><p>Zero conditional mean: <span class="math inline">\(E[u|X] = 0\)</span>. Expected value of the error term is zero conditional on all values of the explanatory variable. The assumption implies that <span class="math inline">\(E(y) = X\beta\)</span>. This is important since it essentially says that we get the mean function right.</p></li>
</ol>
<p>Recall that <span class="math inline">\(u\)</span> represents all unobserved factors that influence <span class="math inline">\(Y\)</span>. If such unobserved factors are also correlated with X, Cov<span class="math inline">\((X, u) \neq 0\)</span>. For example, assume we care about explaining wages as only a function of education. We must assume that ability is the same for everyone regardless of education (i.e.&nbsp;E[ability|education = low] = E[ability|education = high]). That is, everyone has the same ability and the only thing that varies is education!</p>
<ol start="5" type="1">
<li><p>Homoskedasticity: <span class="math inline">\(var(u|X) = \sigma_u^2\textbf{I}_n\)</span> The error term has the same variance conditional on all values of the explanatory variable.</p></li>
<li><p>Normality: <span class="math inline">\(u|X\sim N(0,\sigma_u^2\textbf{I}_n)\)</span>. The error term is independent of the explanatory variables and normally distributed.</p></li>
</ol>
</section>
<section id="uses-of-the-assumptions" class="level1">
<h1>Uses of the assumptions</h1>
<p>If we don’t have variation in x, we can’t calculate the coefficients. (If we don’t have variation in y, <span class="math inline">\(\beta\)</span> = 0)</p>
<p>If 1-4 hold estimates are unbiased (i.e.&nbsp;sample beta = population beta), and consistent (OLS estimator would converge to the true population parameter as the sample size get larger, and tends to infinity.) Note that ``an estimator is unbiased if the expected value of the sampling distribution of the estimators is equal the true population parameter value. An estimator is consistent if, as the sample size increases, tends to infinity, the estimates converge to the true population parameter.”</p>
<p>If 1-5 are known as the Gauss-Markov assumptions and, if they hold, the estimates are BLUE (best linear unbiased estimator), they also allow for large-sample inference. Best means lowest variance. Linear means among linear estimators. Unbiased means that sample beta is the population beta. If you violate homoskedasticity, then OLS isn’t the best among the linear-unbiased-estimators.</p>
<p>1-6 allow for small-sample inference</p>
<p>See:</p>
<ul>
<li><p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture5_handout2020.pdf" class="uri">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture5_handout2020.pdf</a></p></li>
<li><p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture6_handout2020.pdf" class="uri">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture6_handout2020.pdf</a></p></li>
<li><p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture7_handout2020.pdf" class="uri">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture7_handout2020.pdf</a></p></li>
</ul>
<section id="mechanical-properties-of-ols" class="level2">
<h2 class="anchored" data-anchor-id="mechanical-properties-of-ols">Mechanical Properties of OLS</h2>
<ol type="1">
<li>The observed values of <strong>X</strong> are uncorrelated with the residuals. (Note we’re just assuming that X is uncorrelated with the unobserved disturbances)</li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-4_1ce371f673d4607946755ecba76e5119">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> sysuse<span class="sc">::</span>auto)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(fit1<span class="sc">$</span>residuals, sysuse<span class="sc">::</span>auto<span class="sc">$</span>mpg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 8.338145e-17</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(fit1<span class="sc">$</span>residuals, sysuse<span class="sc">::</span>auto<span class="sc">$</span>weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 9.020833e-17</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>The sum of the residuals is 0</li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-5_a70984238017c4d7c6610e7b7ab33446">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(fit1<span class="sc">$</span>residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -3.637979e-12</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>The sample mean of the residuals is 0</li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-6_65b2f2d4646947832b11abaedcdc770e">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(fit1<span class="sc">$</span>residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -9.832375e-14</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>The regression hyperplane passes through the means of the observed values <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{y}\)</span>. We know this because we know that <span class="math inline">\(\bar{e} = 0\)</span> and <span class="math inline">\(e = y-X\beta\)</span>. If we divide by n - the number of observations, we’re left with <span class="math inline">\(0=\bar{e}=\bar{y}-\bar{X}\hat{\beta}\)</span> therefore <span class="math inline">\(\bar{y}=\bar{X}\hat{\beta}\)</span></li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-7_78b5985cda3ffbdf8cf5b1aa9ed0e3aa">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We have a regression price = intercept + weight + mpg + error</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># mean of covariate mpg</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>mpg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 21.2973</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># mean of covariate weight</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3019.459</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># mean of dependent variable price</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>price)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6165.257</code></pre>
</div>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># show equal</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>price),<span class="dv">3</span>) <span class="sc">==</span> <span class="fu">round</span>(<span class="fu">predict</span>(fit1, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">weight =</span> <span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>weight), <span class="at">mpg =</span> <span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>mpg))), <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   1 
TRUE </code></pre>
</div>
</div>
<ol start="5" type="1">
<li>The predicted values of y are uncorrelated with the residuals</li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-8_8fc9a40f12eaf892f890841cbf518015">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(fit1<span class="sc">$</span>residuals, fit1<span class="sc">$</span>fitted.values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -5.475433e-17</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>The mean of the predicted Y’s for the sample will equal the mean of the observed y’s</li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-9_ce7d2db4e227facc58bd25f434df0028">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(fit1<span class="sc">$</span>fitted.values) <span class="sc">==</span> <span class="fu">mean</span>(sysuse<span class="sc">::</span>auto<span class="sc">$</span>price)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
</section>
<section id="showing-unbiasdness" class="level2">
<h2 class="anchored" data-anchor-id="showing-unbiasdness">Showing Unbiasdness</h2>
<p>What does this mean? That <span class="math inline">\(E[\hat{\beta}]=\beta\)</span>. In words, our estimated <span class="math inline">\(\hat{\beta}\)</span> is equal to the population <span class="math inline">\(\beta\)</span></p>
<p>To begin, we start with our definition of <span class="math inline">\(\beta\)</span> from above. We note that this requires the assumption of linearity in parameters AND no multicollinearity.</p>
<p><span class="math display">\[\hat{\beta} = (X'X)^{-1} X'y \]</span></p>
<p>Now we replace y with the definition of y (that is <span class="math inline">\(y = X\beta\)</span> + u). Note the lack of a hat, this is important! <span class="math display">\[\hat{\beta} = (X'X)^{-1} X'(X \beta + u) \]</span> Distribute the <span class="math inline">\((X'X)^{-1} X'\)</span></p>
<p><span class="math display">\[\hat{\beta} = (X'X)^{-1} X'X\beta + (X'X)^{-1} X'u \]</span> Note that <span class="math inline">\((X'X)^{-1} X'X = I\)</span></p>
<p><span class="math display">\[ \hat{\beta} = I\beta + (X'X)^{-1} X'u \]</span> We’re not done yet BUT we can see that unless <span class="math inline">\(E[u|X]=0\)</span>, <span class="math inline">\(\hat{\beta} \neq \beta\)</span>.</p>
<p>Now, take conditional expectation of each side</p>
<p><span class="math display">\[ E[\hat{\beta}|X] =  E[\beta|X] + E[(X'X)^{-1} X'u|X] \]</span> <span class="math display">\[ E[\hat{\beta}|X] =  \beta + (X'X)^{-1} X'E[u|X] \]</span> As long as we have the zero conditional mean assumption <span class="math inline">\(E[u|X]=0\)</span> then <span class="math inline">\(E[\hat{\beta}|X]=\beta\)</span></p>
</section>
</section>
<section id="properties-of-ols" class="level1">
<h1>Properties of OLS</h1>
<section id="sec-FWL" class="level2">
<h2 class="anchored" data-anchor-id="sec-FWL">Regression Anatomy or Frisch-Waugh-Lowell</h2>
<p>Regression anatomy theorem helps us interpret a single slope coefficient in a multiple regression model</p>
<p>Also, help us understand “OLS” as a “matching estimator” (try to compare observations that are alike in the Xs)</p>
<p>See <a href="https://econ.lse.ac.uk/staff/spischke/ec533/Griliches_measurement%20error.pdf" class="uri">https://econ.lse.ac.uk/staff/spischke/ec533/Griliches_measurement%20error.pdf</a> and <a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107" class="uri">https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107</a></p>
</section>
<section id="what-is-it" class="level2">
<h2 class="anchored" data-anchor-id="what-is-it">What is it?</h2>
<p>This theorem tells us how to interpret a regression coefficient. Suppose we have a model <span class="math display">\[Y_{i} = \alpha + \beta_1 X_{1i} + \beta_2X_{2i} + e_i\]</span></p>
<p>The theorem says that <span class="math display">\[\beta_{1i}=\frac{COV(Y_i,\tilde{X}_{1i})}{VAR(\tilde{X}_{1i})}\]</span> where <span class="math inline">\(\tilde{X}_{1i}\)</span> are the residuals from a regression of <span class="math inline">\(X_{1i}\)</span> on <span class="math inline">\(X_{2i}\)</span>:</p>
<p><span class="math display">\[X_{i1}=\pi_0+\pi_1X_{2i}+\tilde{X}_{1i}\]</span> Suppose we care about <span class="math inline">\(X_{1i}\)</span> it tells us that controlling for <span class="math inline">\(X_{ki}\)</span> for all <span class="math inline">\(k \neq 1\)</span> that we’re looking at just the variation in <span class="math inline">\(y\)</span> explained by <span class="math inline">\(X_{1i}\)</span> and not by the variation in <span class="math inline">\(X_{ki}\)</span>.</p>
<p>The auxillary regression of <span class="math inline">\(X_{1i}\)</span> on <span class="math inline">\(X_{2i}\)</span> contains the term <span class="math inline">\(\tilde{X}_{1i}\)</span> which partials out (i.e.&nbsp;removes) the influence of <span class="math inline">\(X_{2i}\)</span> on <span class="math inline">\(X_{1i}\)</span></p>
<p>We can also use it to explain the omitted variable bias formula (Short equals long plus the effect of omitted in long times the regression of omitted on included). Suppose we have two regressions <span class="math inline">\(Y_{i} = \beta_0 + \beta_1 X_{1i} + e_i\)</span> and <span class="math inline">\(Y_{i} = \beta_0^* + \beta_1^* X_{1i} + \beta_2^*X_{2i} + v_i\)</span>, then <span class="math inline">\(\beta_1 = \frac{Cov(X_{1i},Y_i)}{Var(X_i)}=\beta_{1i}^*+\gamma \delta_{X_2X_1}\)</span> where <span class="math inline">\(\delta_{X_2X_1}\)</span> is the regression of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_1\)</span>. Note that this is the opposite! order of regression anatomy. The auxillary regression in regression anatomy is <span class="math inline">\(X_1\)</span> on <span class="math inline">\(X_2\)</span></p>
</section>
<section id="why-is-it-useful" class="level2">
<h2 class="anchored" data-anchor-id="why-is-it-useful">Why is it useful?</h2>
<p>We can break a multivariate regression with K regressors into K simpler bivariate models.</p>
<ol type="1">
<li><p>It allows you to construct a bi-dimensional scatterplot of a dependent variable an independent variable of interest using the coefficients from a multiple regression. I.e. you run the regressions above and then plot the coefficient of interest from the multiple regression with the original <span class="math inline">\(y_i\)</span> on the y-axis and the residuals (<span class="math inline">\(\tilde{X}_{1i}\)</span>) on the x-axis.</p></li>
<li><p>It shows us why multicollinearity is a problem in regression - it means that most of the variation is between the regressors and not between the variable of interest (or the residual variable <span class="math inline">\(\tilde{X_{1i}}\)</span>) and the dependent variable. This means that the coefficient is unlikely to be significant because most of the variation between the multi-co-linear coefficients is between the coefficients and not between the coefficients and y.</p></li>
<li><p>We can use it in a multivariate OLS model to decompose the variance of each individual variable into three components (although we only really care about b and c):</p></li>
</ol>
<!-- -->
<ol type="a">
<li>Variance not associated with y</li>
<li>Variance associated with y and shared with other regressors</li>
<li>Variance associated with y and not shared with other regressors</li>
</ol>
<p>When you construct an OLS model, the inclusion of a new regressor is valuable when the additional explaining power contained in it is not already fully captured by the other K regressors. Accordingly, the new variable must mainly provide the kind of variance denoted with (c). (<a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107" class="uri">https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107</a>)</p>
</section>
<section id="how-do-we-use-it" class="level2">
<h2 class="anchored" data-anchor-id="how-do-we-use-it">How do we use it?</h2>
<ol type="1">
<li>Scatterplots and analysis of the contribution of each independent variable</li>
</ol>
<div class="cell" data-hash="OLS-published-version_cache/html/FWL coding_3ea53c824c3ba7bf697a79bccf7dddaa">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sysuse)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># see https://github.com/scunning1975/mixtape/blob/master/R/reganat.R</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>auto <span class="ot">&lt;-</span> sysuse<span class="sc">::</span>auto <span class="sc">%&gt;%</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">length =</span> length <span class="sc">-</span> <span class="fu">mean</span>(length))</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co"># show regression anatomy</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>long_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> length <span class="sc">+</span> weight <span class="sc">+</span> headroom <span class="sc">+</span> mpg,<span class="at">data =</span> auto)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co"># below to show reg anatomy and FWL result in same estimate</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>long_without_coef_interest <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> weight <span class="sc">+</span> headroom <span class="sc">+</span> mpg,<span class="at">data =</span> auto)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="co"># we care about effect of length on price</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>x_residuals_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> length <span class="sc">~</span> weight <span class="sc">+</span> headroom <span class="sc">+</span> mpg,<span class="at">data =</span> auto)</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>x_residuals <span class="ot">&lt;-</span> x_residuals_regression<span class="sc">$</span>residuals</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a><span class="co"># show regression anatomy - all are the same! So</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>long_regression<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   length 
-94.49651 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is regression anatomy</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(auto<span class="sc">$</span>price, x_residuals)<span class="sc">/</span><span class="fu">var</span>(x_residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -94.49651</code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is also regression anatomy</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> x_residuals, <span class="at">data =</span> auto)<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x_residuals 
  -94.49651 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>long_expressed_as_short <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> x_residuals, <span class="at">data =</span> auto)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># can also do Frisch–Waugh–Lovell.  It's regression anatomy but with the residuals from a regression of y on x2 and x3 (i.e without x1)</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(long_without_coef_interest<span class="sc">$</span>residuals, x_residuals)<span class="sc">/</span><span class="fu">var</span>(x_residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -94.49651</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now for the plotting!</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co"># show bias in short regression first</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>bivariate_incorrect <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> length, <span class="at">data =</span> auto)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="co"># how do we plot?  For geom_smooth you need a y and an x that's linear.</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># the x's are just the x residuals for everything - so those are easy</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># the y's are different!  the main difference between the two is that short uses the bivariate coefficient (times the value of the data itself), and the long uses the 'correct' coefficient from the long regression.  you could also use the coefficient from the bivariate regression of y on the residuals from the auxillary regression (the x1 on x2 through xn).  To keep the data consistent with the plotted points, we use the intercept from the bivariate regression of the y regressed on the residuals from the auxillary regression.</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="co"># want a whole bunch of x,y pairs to plot</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>short <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">price =</span> long_expressed_as_short<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> bivariate_incorrect<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> auto<span class="sc">$</span>length , </span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_residuals =</span> x_residuals, </span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>long <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">price =</span> long_expressed_as_short<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> long_expressed_as_short<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> auto<span class="sc">$</span>length , </span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_residuals =</span> x_residuals, </span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note could also have price as price = long_regression$coefficients[1] + long_expressed_as_short$coefficients[2] * auto$length </span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>auto <span class="sc">%&gt;%</span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>x_residuals, <span class="at">y =</span> price)) <span class="sc">+</span></span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">data =</span> short, <span class="at">color =</span> <span class="st">"blue"</span>,<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F) <span class="sc">+</span></span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">data =</span> long, <span class="at">color =</span> <span class="st">"red"</span>,<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F) <span class="sc">+</span></span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Regression Anatomy"</span>, </span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">caption =</span> <span class="st">"</span></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a><span class="st">    We've fixed these lines to have the same intercept </span><span class="sc">\n</span><span class="st"> and just let the slope vary to show the difference between the long and the short </span><span class="sc">\n</span></span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a><span class="st">    The x, y pairs are the original y's and the x residuals </span><span class="sc">\n</span></span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a><span class="st">    Blue is the effect of y on x1, not controlling for x2 </span><span class="sc">\n</span></span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a><span class="st">    Red is the effect of y on x1 controlling for x2"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'
`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/FWL%20coding-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot correctly for future use</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>auto <span class="sc">%&gt;%</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>x_residuals, <span class="at">y =</span> price)) <span class="sc">+</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">data =</span> long, <span class="at">color =</span> <span class="st">"red"</span>,<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F) <span class="sc">+</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Regression Anatomy"</span>, </span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">caption =</span> <span class="st">"Your long reference footnote goes in here"</span>) <span class="sc">+</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position=</span><span class="st">"none"</span>,</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>, <span class="co">#title</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>                                <span class="co">#family = font,           #set font family</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>                                <span class="at">size =</span> <span class="dv">18</span>,                <span class="co">#set font size</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>                                <span class="at">face =</span> <span class="st">'bold'</span>,            <span class="co">#bold typeface</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>                                <span class="at">vjust =</span> <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/FWL%20coding-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<ol start="2" type="1">
<li>Contribution of the variance of each variable to the model</li>
</ol>
<p>For the above note that a partial correlation is computed between two residuals. A semipartial is computed between one residual and another raw or unresidualized variable.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/variance contribution_483dc40c262b6f86434c205debecc89b">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="co">#reganat price length mpg weight, dis(length) semip</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># change back to non-demeaned length so that we can check our results with https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107#:~:text=The%20regression%20anatomy%20theorem%20is,issue%20in%20time%2Dseries%20econometrics.</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>auto <span class="ot">&lt;-</span> sysuse<span class="sc">::</span>auto</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="co"># need for semi partial correlation</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>long_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> length <span class="sc">+</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> auto)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co"># need for partial correlation</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>long_without_coef_interest <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> auto)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co"># we care about effect of length on price</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>x_residuals_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> length <span class="sc">~</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> auto)</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a><span class="co"># regression anatomy works</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a><span class="co"># bivariate &lt;- lm(formula = price ~ x_residuals_regression$residuals, data = auto)</span></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a><span class="do">#### PARTIAL AND SEMI-PARTIAL COEFFICIENTS ####</span></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a><span class="co"># partial of price with length</span></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>partial <span class="ot">&lt;-</span> <span class="fu">cor</span>(x_residuals_regression<span class="sc">$</span>residuals, long_without_coef_interest<span class="sc">$</span>residuals)</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a><span class="co"># semipartial correlations of price with length</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>semi_partial <span class="ot">&lt;-</span> <span class="fu">cor</span>(x_residuals_regression<span class="sc">$</span>residuals, auto<span class="sc">$</span>price)</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a><span class="co"># the below is the contribution to the r^2 or the variance explained by x1</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>semi_partial<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.06398732</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure first x is the coef of interest</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># see page 14 https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>importance_of_coef <span class="ot">&lt;-</span> <span class="cf">function</span>(df,y,xvars,...){</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># select variables</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>  df <span class="sc">%&lt;&gt;%</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="fu">all_of</span>(y), <span class="fu">all_of</span>(xvars))</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>  y_numeric <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(y) <span class="sc">%&gt;%</span> <span class="fu">pull</span>()</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># run long regression</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>  long_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(y, <span class="fu">paste</span>(xvars, <span class="at">collapse =</span> <span class="st">" + "</span>), <span class="at">sep =</span> <span class="st">" ~ "</span>)</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Saturated model </span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>  long_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(long_formula, <span class="at">data =</span> df)</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Saturated model without coef of interest</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>  long_formula_without_coef_interest <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(y, <span class="fu">paste</span>(xvars[<span class="sc">-</span><span class="dv">1</span>], <span class="at">collapse =</span> <span class="st">" + "</span>), <span class="at">sep =</span> <span class="st">" ~ "</span>)</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># long residuals without coefficient of interest</span></span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>  long_residuals_wo_ci <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> long_formula_without_coef_interest, <span class="at">data =</span> df)</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># X residuals</span></span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>  x_residuals_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(</span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>      <span class="fu">paste</span>(xvars[<span class="dv">1</span>], <span class="fu">paste</span>(xvars[<span class="sc">-</span><span class="dv">1</span>], <span class="at">collapse =</span> <span class="st">" + "</span>), <span class="at">sep =</span> <span class="st">" ~ "</span>)</span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>  x_residuals <span class="ot">&lt;-</span> <span class="fu">lm</span>(x_residuals_formula, <span class="at">data =</span> df)</span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>  partial <span class="ot">&lt;-</span> <span class="fu">cor</span>(x_residuals<span class="sc">$</span>residuals, long_residuals_wo_ci<span class="sc">$</span>residuals)</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>  semi_partial <span class="ot">&lt;-</span> <span class="fu">cor</span>(x_residuals<span class="sc">$</span>residuals, y_numeric)</span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a>  semi_partial_squared <span class="ot">&lt;-</span> semi_partial<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>   <span class="co">#Under normal conditions, the sum of the squared semipartials can be subtracted from the overall R2 for the complete OLS regression to get the value of common variance shared by the independent variables with y.</span></span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the semi-partial formula here doens't work</span></span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>  <span class="co"># numerator &lt;- summary(long_regression)$r.squared - summary(long_residuals_wo_ci)$r.squared</span></span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># denominator &lt;- (1 - summary(long_regression)$r.squared)*(length(summary(long_regression)$residuals)-summary(long_regression)$df[1]-1)</span></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># contribution &lt;- numerator/denominator</span></span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a>  <span class="co">#   contribution</span></span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a>  <span class="co">#INTERPRETATION - semipartial squared is the variance explained by an x individually</span></span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># if you sum up the semi-partials of all the x's, you get all of their individual contributions to the model</span></span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a>  <span class="co"># if you then subtract that number from the overall r^2 you get the amount of variance common to the x's </span></span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">partial =</span> partial, <span class="at">semi_partial =</span> semi_partial, <span class="at">semi_partial_squared =</span> semi_partial_squared))</span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a>lenth_cont <span class="ot">&lt;-</span> <span class="fu">importance_of_coef</span>(auto, <span class="at">y =</span> <span class="st">"price"</span>, <span class="at">xvars =</span> <span class="fu">c</span>(<span class="st">"length"</span>,<span class="st">"weight"</span>,<span class="st">"mpg"</span>))<span class="sc">$</span>semi_partial_squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.
ℹ Please use `all_of()` or `any_of()` instead.
  # Was:
  data %&gt;% select(y)

  # Now:
  data %&gt;% select(all_of(y))

See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>weight_cont <span class="ot">&lt;-</span> <span class="fu">importance_of_coef</span>(auto, <span class="at">y =</span> <span class="st">"price"</span>, <span class="at">xvars =</span> <span class="fu">c</span>(<span class="st">"weight"</span>,<span class="st">"length"</span>,<span class="st">"mpg"</span>))<span class="sc">$</span>semi_partial_squared</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>mpg_cont <span class="ot">&lt;-</span> <span class="fu">importance_of_coef</span>(auto, <span class="at">y =</span> <span class="st">"price"</span>, <span class="at">xvars =</span> <span class="fu">c</span>(<span class="st">"mpg"</span>,<span class="st">"weight"</span>,<span class="st">"length"</span>))<span class="sc">$</span>semi_partial_squared</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance of x's individually</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>lenth_cont <span class="sc">+</span> weight_cont <span class="sc">+</span> mpg_cont</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2021244</code></pre>
</div>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance common to the x's</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(price <span class="sc">~</span> length <span class="sc">+</span> weight <span class="sc">+</span> mpg, auto))<span class="sc">$</span>r.squared <span class="sc">-</span> (lenth_cont <span class="sc">+</span> weight_cont <span class="sc">+</span> mpg_cont)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.155252</code></pre>
</div>
</div>
</section>
</section>
<section id="omitted-variable-bias-ovb" class="level1">
<h1>Omitted Variable Bias (OVB)</h1>
<section id="what-is-it-1" class="level2">
<h2 class="anchored" data-anchor-id="what-is-it-1">What is it?</h2>
<p>Suppose you have a short regression with K regressors and a long regression with K + n regressors where n is less than the number of observations in your sample:</p>
<p>Short equals long plus the effect(s) of omitted times the regression(s) of omitted on included, all computed in a model maintaining the set of controls included in both short and long</p>
</section>
<section id="why-is-it-useful-1" class="level2">
<h2 class="anchored" data-anchor-id="why-is-it-useful-1">Why is it useful?</h2>
<p>It shows us the effect of omitting variables from our regression.</p>
</section>
<section id="how-do-we-use-it-1" class="level2">
<h2 class="anchored" data-anchor-id="how-do-we-use-it-1">How do we use it?</h2>
<p>It shows the impact of omitting variables in a regression. It’s mostly theoretical since you’d use variables if you had them!</p>
<div class="cell" data-hash="OLS-published-version_cache/html/OVB_19ba73a7fb3a869b9bd8311b45839b71">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n08MRU_R2.pdf</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>auto <span class="ot">&lt;-</span> sysuse<span class="sc">::</span>auto</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>short <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> mpg, <span class="at">data =</span> auto)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>long <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> mpg <span class="sc">+</span> length, <span class="at">data =</span> auto)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>ommitted_on_included <span class="ot">&lt;-</span> <span class="fu">lm</span>(length <span class="sc">~</span> mpg, <span class="at">data =</span> auto)</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Show this works</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>short<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      mpg 
-238.8943 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>long<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">+</span> ommitted_on_included<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> long<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      mpg 
-238.8943 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># round(as.numeric(short$coefficients[2]),5) == round(as.numeric(long$coefficients[2]) + as.numeric(ommitted_on_included$coefficients[2]) * as.numeric(long$coefficients[3]),5)</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Now do OVB for a regression with more than 2 variables</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>short <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> auto)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>long <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> price <span class="sc">~</span> mpg <span class="sc">+</span> weight <span class="sc">+</span> length <span class="sc">+</span> trunk, <span class="at">data =</span> auto)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Need to do two of these regressions! then take effect of MPG on each</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>ommitted_on_included_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(length <span class="sc">~</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> auto)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>ommitted_on_included_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(trunk <span class="sc">~</span> mpg <span class="sc">+</span> weight, <span class="at">data =</span> auto)</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficient on MPG in short </span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>short<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      mpg 
-49.51222 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>long<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">+</span> ommitted_on_included_1<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> long<span class="sc">$</span>coefficients[<span class="dv">4</span>] <span class="sc">+</span> ommitted_on_included_2<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> long<span class="sc">$</span>coefficients[<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      mpg 
-49.51222 </code></pre>
</div>
</div>
</section>
</section>
<section id="the-hat-matrix" class="level1">
<h1>The Hat Matrix!</h1>
<p>This stuff from <a href="https://allmodelsarewrong.github.io/ols.html" class="uri">https://allmodelsarewrong.github.io/ols.html</a></p>
<p>We know that <span class="math inline">\(\beta = (X'X)^{-1}X'Y\)</span>. We can use this to show how we predict <span class="math inline">\(\hat{y}\)</span>. We know that <span class="math inline">\(\hat{y}=X\beta + e\)</span>. Substitute in <span class="math inline">\(\beta = (X'X)^{-1}X'Y\)</span> into the previous equation such that <span class="math display">\[\hat{y}=X(X'X)^{-1}X'Y\]</span></p>
<p>If we call <span class="math inline">\(H=X(X'X)^{-1}X'\)</span>, then we can say <span class="math inline">\(\hat{y}=Hy\)</span></p>
<p>It makes hats, what else does it do? It allows us to detect leverage points. A zero value, <span class="math inline">\(H_{ii} = 0\)</span>, indicates a point which has no influence on prediction.</p>
<p>Points are high leverage if their eigen value is greater than <span class="math inline">\(2\sum_n^i h_{ii}/n\)</span></p>
<p>Show the leverage of any point</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-10_1acc66ced601d1ca61ec9a0835fd3105">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">=</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> wt, mtcars) <span class="co"># OLS including all points</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">model.matrix</span>(fit) <span class="co"># X model matrix</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    hat_matrix <span class="ot">=</span> X<span class="sc">%*%</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">%*%</span><span class="fu">t</span>(X)) <span class="co"># Hat matrix</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">diag</span>(hat_matrix)[<span class="dv">1</span>] <span class="co"># First diagonal point in Hat matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Mazda RX4 
0.04326896 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>    fitwithout1 <span class="ot">=</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> wt, mtcars[<span class="sc">-</span><span class="dv">1</span>,]) <span class="co"># OLS excluding first data point.</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    new <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">wt=</span>mtcars[<span class="dv">1</span>,<span class="st">'wt'</span>]) <span class="co"># Predicting y hat in this OLS w/o first point.</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    y_hat_without <span class="ot">=</span> <span class="fu">predict</span>(fitwithout1, <span class="at">newdata=</span>new) <span class="co"># ... here it is.</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">residuals</span>(fit)[<span class="dv">1</span>] <span class="co"># The residual when OLS includes data point.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mazda RX4 
-2.282611 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>    lev <span class="ot">=</span> <span class="dv">1</span> <span class="sc">-</span> (<span class="fu">residuals</span>(fit)[<span class="dv">1</span>]<span class="sc">/</span>(mtcars[<span class="dv">1</span>,<span class="st">'mpg'</span>] <span class="sc">-</span>  y_hat_without)) <span class="co"># Leverage</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">all.equal</span>(<span class="fu">diag</span>(hat_matrix)[<span class="dv">1</span>],lev) <span class="co">#TRUE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
</section>
<section id="sec-interpret" class="level1">
<h1>Interpretation</h1>
<p><strong>No logged DV, no logged IV</strong> A one unit increase in x, increases y by coefficient (<span class="math inline">\(\beta\)</span>) units</p>
<ul>
<li>Suppose our <span class="math inline">\(\beta\)</span> is 5. The interpretation is “a one unit increase in x increases the dependent variable by 5 units.” Or, a one standard deviation increase in x increases the dependent variable by <span class="math inline">\(\beta \times \text{sd}(x)\)</span> units.<br>
</li>
<li>Suppose our <span class="math inline">\(\beta\)</span> is <span class="math inline">\(-5\)</span>. The interpretation is “a one unit increase in x decreases the dependent variable by 5 units.” Or, a one standard deviation increase in x decreases the dependent variable by <span class="math inline">\(\beta \times \text{sd}(x)\)</span> units.</li>
</ul>
<p><strong>No logged DV, logged IV</strong> Divide the coefficient by 100, this tells us that a 1% increase in the independent variable increases (decreases) the dependent variable by coefficient/100 units. Example: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1% increase in the independent variable, our dependent variable increases by about 0.002. For an 10% increase multiply the coefficient by log(1.1) (for x increase log(1.x)). Example: For every 10% increase in the independent variable, our dependent variable increases by about <span class="math inline">\(0.198 \times \log(1.10) = 0.02\)</span>.</p>
<ul>
<li><p>Suppose our <span class="math inline">\(\beta\)</span> is 5. The interpretation is that a one percent increase in x increases the dependent variable by 0.05. For every 10% increase in the independent variable, the dependent variable increases by <span class="math inline">\(5 \times \log(1.1) = 0.48\)</span></p></li>
<li><p>Suppose our <span class="math inline">\(\beta\)</span> is <span class="math inline">\(-5\)</span>. A one percent increase in x decreases the dependent variable by 0.05 units. For every 10% increase in x, the dependent variable decreases by <span class="math inline">\(5 \times \log(1.1) = 0.48\)</span> units</p></li>
</ul>
<p><strong>Logged DV, No logged IV</strong> Exponentiate the coefficient. This gives the multiplicative factor for every one-unit increase in the independent variable. Example: the coefficient is 0.198. exp(0.198) = 1.218962. For every one-unit increase in the independent variable, our dependent variable increases by a factor of about 1.22, or 22%. Recall that multiplying a number by 1.22 is the same as increasing the number by 22%. A shortcut is to exponentiate the coefficient, then subtract one from this number, and multiply by 100 (i.e.&nbsp;<span class="math inline">\((e^\beta-1)\times100\)</span>). This gives the percent increase (or decrease) in the y for every one-unit increase in the independent variable. “In summary, when the outcome variable is log transformed, it is natural to interpret the exponentiated regression coefficients. These values correspond to changes in the ratio of the expected geometric means of the original outcome variable.”</p>
<ul>
<li><p>Suppose our <span class="math inline">\(\beta\)</span> is 5. A one unit increase in x increases the dependent variable by a factor of <span class="math inline">\(\exp(5)=148.4\)</span> or <span class="math inline">\(14,740\%\)</span> (remember to subtract the one from the beta).</p></li>
<li><p>Suppose our <span class="math inline">\(\beta\)</span> is <span class="math inline">\(-5\)</span>. A one unit increase in x decreases the dependent variable by a factor of <span class="math inline">\(\exp(-5)=0.007\)</span> or 99.32% <span class="math inline">\(((\exp(-5)-1) \times 100)\)</span></p></li>
</ul>
<p><strong>Logged DV, logged IV</strong> Interpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. Suppose the coefficient is 0.198. For every 1% increase in the independent variable, the dependent variable increases by 0.2%. Suppose, instead, we wanted an x percent increase - use the formula <span class="math inline">\((1.x^{0.198} – 1) * 100\)</span>. For example, a 10% increase in our independent variable increases our dependent variable by <span class="math inline">\((1.1^{0.198} - 1) \times 100 = 1.91\%\)</span></p>
<ul>
<li><p>Suppose our <span class="math inline">\(\beta\)</span> is <span class="math inline">\(5\)</span>. A 1% increase in the independent variable increases the dependent variably by 5%.</p></li>
<li><p>Suppose our <span class="math inline">\(\beta\)</span> is <span class="math inline">\(-5\)</span>. A 1% increase in the independent variable decreases the dependent variably by 5%.</p></li>
</ul>
<section id="why-this-interpretation-for-logged-dependent-variable-values" class="level3">
<h3 class="anchored" data-anchor-id="why-this-interpretation-for-logged-dependent-variable-values">Why this interpretation for logged dependent variable values?</h3>
<p>“Our independent variable has a multiplicative relationship with our dependent variable instead of the usual additive relationship. Hence the need to express the effect of a one-unit change in x on y as a percent.”</p>
<p><span class="math display">\[\log(y)=\beta_0+\beta_1x\]</span> <span class="math display">\[\exp(\log(y))=\exp(\beta_0+\beta_1x)\]</span> <span class="math display">\[y=\exp(\beta_0+\beta_1x)\]</span> <span class="math display">\[y=\exp(\beta_0)\times(\beta_1x)\]</span></p>
</section>
<section id="logged-independent-variable" class="level3">
<h3 class="anchored" data-anchor-id="logged-independent-variable">Logged Independent Variable</h3>
<p><span class="math display">\[y=\beta_0 + \beta_1\log(x_1) + \beta_2(x_2)\]</span> <span class="math display">\[e^y=e^{\beta_0 + \beta_1\log(x_1) + \beta_2(x_2)}\]</span> <span class="math display">\[e^y=e^{\beta_0 + \beta_1\log(x_1) + \beta_2(x_2)}\]</span> <span class="math display">\[e^y=e^{\beta_0}\times e^{\beta_1\log(x_1)} \times e^ {\beta_2(x_2)}\]</span> <span class="math display">\[e^y=e^{\beta_0}\times e^{\beta_1\log(x_1)} \times e^ {\beta_2(x_2)}\]</span> <span class="math display">\[e^y=e^{\beta_0}\times x_1^{\beta_1} \times e^ {\beta_2(x_2)} \]</span></p>
<p>Source</p>
<ul>
<li><p><a href="https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/" class="uri">https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/</a></p></li>
<li><pre><code>&lt;https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/&gt;</code></pre></li>
<li><p><a href="https://www.statalist.org/forums/forum/general-stata-discussion/general/1362222-interpretation-of-coefficients-for-log-transformed-dependent-variable-panel-data" class="uri">https://www.statalist.org/forums/forum/general-stata-discussion/general/1362222-interpretation-of-coefficients-for-log-transformed-dependent-variable-panel-data</a></p></li>
</ul>
</section>
</section>
<section id="ols-diagnostics" class="level1">
<h1>OLS Diagnostics</h1>
<p>So, you have estimates and you understand the estimates. Great. Now let’s see where the models are wrong. Run the command plot() after OLS and you’ll get the following plots that help with regression diagnostics.</p>
<ol type="1">
<li><p>Residuals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary.</p></li>
<li><p>Normal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required.</p></li>
<li><p>Scale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic.</p></li>
<li><p>Residuals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model’s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots.</p></li>
</ol>
<p>Sources: - <a href="https://sscc.wisc.edu/sscc/pubs/RegDiag-R/linearity.html" class="uri">https://sscc.wisc.edu/sscc/pubs/RegDiag-R/linearity.html</a></p>
<ul>
<li><pre><code>&lt;https://jhudatascience.org/tidyversecourse/model.html#linear-modeling&gt;</code></pre></li>
</ul>
</section>
<section id="lots-of-fixed-effects" class="level1">
<h1>Lots of Fixed Effects</h1>
<p>Sometimes we have a model with a lot (i.e.&nbsp;more than 50) fixed effects. Base R handles these models poorly as it computes the fixed effects individually. There’s a way around this - demeaning all the independent variables or using the PLM package.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-11_48283e4a1f08d56055a1ea6e1999f0c1">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> </span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="fu">structure</span>(<span class="fu">list</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="fl">2.3</span>, <span class="dv">4</span>, <span class="fl">5.7</span>, <span class="dv">3</span>, <span class="fl">4.5</span>, <span class="dv">6</span>), <span class="at">x2 =</span> <span class="fu">rnorm</span>(<span class="dv">9</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="fl">4.4</span>, </span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="fl">6.8</span>, <span class="fl">9.2</span>, <span class="fl">0.54</span>, <span class="fl">1.9</span>, <span class="fl">3.26</span>, <span class="sc">-</span><span class="fl">6.92</span>, <span class="sc">-</span><span class="fl">5.495</span>, <span class="sc">-</span><span class="fl">4.07</span>), <span class="at">g =</span> <span class="fu">structure</span>(<span class="fu">c</span>(1L, </span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), <span class="at">.Label =</span> <span class="fu">c</span>(<span class="st">"a"</span>, <span class="st">"b"</span>, <span class="st">"c"</span>), <span class="at">class =</span> <span class="st">"factor"</span>), </span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">t =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="at">x3 =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)), <span class="at">class =</span> <span class="st">"data.frame"</span>, <span class="at">row.names =</span> <span class="fu">c</span>(<span class="cn">NA</span>, </span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>9L)) <span class="sc">%&gt;%</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>( g ) <span class="sc">%&gt;%</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>( </span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean.x=</span><span class="fu">mean</span>(x), </span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">x.demeaned =</span> x <span class="sc">-</span> mean.x,</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean.y=</span><span class="fu">mean</span>(y), </span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">y.demeaned =</span> y <span class="sc">-</span> mean.y,</span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean.x2 =</span> <span class="fu">mean</span>(x2),</span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">x2.demeaned =</span> x2 <span class="sc">-</span> mean.x2,</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean.x3 =</span> <span class="fu">mean</span>(x3),</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x3.demeaned =</span> x3 <span class="sc">-</span> mean.x3</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a><span class="fu">palette</span>( <span class="fu">c</span>( <span class="st">"steelblue"</span>, <span class="st">"darkred"</span>, <span class="st">"darkgreen"</span>  ) )</span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( data<span class="sc">$</span>x, data<span class="sc">$</span>y, <span class="at">bty=</span><span class="st">"n"</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">cex=</span><span class="dv">3</span>,</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">8</span>), </span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>g), </span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">"Economic growth (%)"</span>, </span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">"International aid ($M)"</span>)</span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a>id.label <span class="ot">&lt;-</span> <span class="fu">paste0</span>( <span class="fu">toupper</span>( data<span class="sc">$</span>g ), <span class="st">"[t=="</span>, data<span class="sc">$</span>t, <span class="st">"]"</span> )</span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>( <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> data, <span class="at">subset=</span>(g<span class="sc">==</span><span class="st">"a"</span>) ), <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">lty=</span><span class="dv">3</span>,</span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a>        <span class="at">col=</span><span class="fu">adjustcolor</span>(<span class="st">"gray40"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) )</span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>( <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> data,<span class="at">subset=</span>(g<span class="sc">==</span><span class="st">"b"</span>) ), <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">lty=</span><span class="dv">3</span>,</span>
<span id="cb83-38"><a href="#cb83-38" aria-hidden="true" tabindex="-1"></a>        <span class="at">col=</span><span class="fu">adjustcolor</span>(<span class="st">"gray40"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) )</span>
<span id="cb83-39"><a href="#cb83-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-40"><a href="#cb83-40" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>( <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> data,<span class="at">subset=</span>(g<span class="sc">==</span><span class="st">"c"</span>) ), <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">lty=</span><span class="dv">3</span>,</span>
<span id="cb83-41"><a href="#cb83-41" aria-hidden="true" tabindex="-1"></a>        <span class="at">col=</span><span class="fu">adjustcolor</span>(<span class="st">"gray40"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) )</span>
<span id="cb83-42"><a href="#cb83-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-43"><a href="#cb83-43" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(data<span class="sc">$</span>x, data<span class="sc">$</span>y, <span class="fu">parse</span>( <span class="at">text=</span>id.label ), </span>
<span id="cb83-44"><a href="#cb83-44" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="st">"gray60"</span>, <span class="at">pos=</span><span class="dv">2</span>, <span class="at">cex=</span><span class="dv">2</span>, <span class="at">offset=</span><span class="dv">1</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">palette</span>( <span class="fu">c</span>( <span class="st">"steelblue"</span>, <span class="st">"darkred"</span>, <span class="st">"darkgreen"</span>  ) )</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">palette</span>( <span class="fu">adjustcolor</span>( <span class="fu">palette</span>(), <span class="at">alpha.f =</span> <span class="fl">0.4</span> ) )</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>( <span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>) )</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( data<span class="sc">$</span>x, data<span class="sc">$</span>y, <span class="at">bty=</span><span class="st">"n"</span>,</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>     <span class="co"># ylim = c(-5,5), xlim = c(-3,3), bty="n", </span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex=</span><span class="dv">4</span>, <span class="at">col =</span> <span class="fu">factor</span>(data<span class="sc">$</span>g), </span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Economic growth (%)"</span>, </span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"International aid ($M)"</span>,</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"POOLED MODEL"</span>)</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( data<span class="sc">$</span>x.demeaned, data<span class="sc">$</span>y.demeaned, </span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>), <span class="at">bty=</span><span class="st">"n"</span>, </span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex=</span><span class="dv">4</span>, <span class="at">col =</span> <span class="fu">factor</span>(data<span class="sc">$</span>g), </span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Economic growth (%)"</span>, </span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"International aid ($M)"</span>,</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"DEMEANED DATA"</span>)</span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="dv">4</span>, <span class="at">col =</span> <span class="st">"steelblue"</span>, <span class="at">lwd =</span> <span class="dv">5</span> )</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="fl">4.5</span>, <span class="at">col =</span> <span class="st">"darkred"</span>, <span class="at">lwd =</span> <span class="dv">5</span> )</span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="dv">5</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">5</span> )</span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.4</span>, <span class="sc">-</span><span class="dv">4</span>, <span class="st">"A"</span>, <span class="at">col =</span> <span class="st">"steelblue"</span>)</span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.4</span>, <span class="sc">-</span><span class="fl">4.5</span>, <span class="st">"B"</span>, <span class="at">col =</span> <span class="st">"darkred"</span>)</span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.4</span>, <span class="sc">-</span><span class="dv">5</span>, <span class="st">"C"</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-11-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>Pooled_OLS <span class="ot">=</span> <span class="fu">lm</span> ( y <span class="sc">~</span> x <span class="sc">+</span> x2, <span class="at">data =</span> data )</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>Demean_OLS <span class="ot">=</span> <span class="fu">lm</span> ( y.demeaned <span class="sc">~</span> x.demeaned <span class="sc">+</span> x2.demeaned, <span class="at">data =</span> data )</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>fixed_reg <span class="ot">=</span> <span class="fu">lm</span> ( y <span class="sc">~</span> x <span class="sc">+</span> x2 <span class="sc">+</span> <span class="fu">as.factor</span>(g), <span class="at">data =</span> data)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>plm_ols <span class="ot">=</span> plm<span class="sc">::</span><span class="fu">plm</span>(y <span class="sc">~</span> x <span class="sc">+</span> x2,<span class="at">data =</span> data,<span class="at">index =</span> <span class="fu">c</span>(<span class="st">"g"</span>))</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">bind_rows</span>(</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(<span class="st">"Pooled"</span>,<span class="fu">tidy</span>(Pooled_OLS)),</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(<span class="st">"De-meaned"</span>,<span class="fu">tidy</span>(Demean_OLS)),</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(<span class="st">"Fixed Effects"</span>,<span class="fu">tidy</span>(fixed_reg)),</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">bind_cols</span>(<span class="st">"PLM"</span>,<span class="fu">tidy</span>(plm_ols))</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">Model =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(term)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
New names:
New names:
New names:
• `` -&gt; `...1`</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 13 × 6
   Model         term           estimate std.error statistic    p.value
   &lt;chr&gt;         &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
 1 Pooled        (Intercept)   -1.30e+ 0    5.83   -2.23e- 1 0.831     
 2 De-meaned     (Intercept)    9.77e-18    0.0998  9.79e-17 1         
 3 Fixed Effects (Intercept)    2.76e+ 0    0.403   6.84e+ 0 0.00240   
 4 Fixed Effects as.factor(g)b -4.87e+ 0    0.308  -1.58e+ 1 0.0000935 
 5 Fixed Effects as.factor(g)c -1.29e+ 1    0.339  -3.80e+ 1 0.00000287
 6 Pooled        x              5.16e- 1    1.32    3.91e- 1 0.710     
 7 Fixed Effects x              1.01e+ 0    0.0860  1.18e+ 1 0.000296  
 8 PLM           x              1.01e+ 0    0.0860  1.18e+ 1 0.000296  
 9 De-meaned     x.demeaned     1.01e+ 0    0.0702  1.44e+ 1 0.00000689
10 Pooled        x2             3.02e+ 0    2.47    1.22e+ 0 0.268     
11 Fixed Effects x2            -8.02e- 2    0.199  -4.03e- 1 0.707     
12 PLM           x2            -8.02e- 2    0.199  -4.03e- 1 0.707     
13 De-meaned     x2.demeaned   -8.02e- 2    0.162  -4.94e- 1 0.639     </code></pre>
</div>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># stargazer::stargazer( Pooled_OLS, Demean_OLS, fixed_reg,plm_ols,</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="co">#            #type = "html", </span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="co">#            dep.var.labels = c("Economic growth", </span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="co">#                               "Economic growth", </span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="co">#                               "Economic growth (de-meaned)",</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="co">#                               "Economic growth (PLM)"),</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="co">#            column.labels = c("Cross-Sectional OLS", </span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a><span class="co">#                              "Pooled OLS", "De-meaned OLS"),</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a><span class="co">#            covariate.labels = c("Constant", </span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a><span class="co">#                                 "International aid - Year 1", </span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a><span class="co">#                                 "International aid", </span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a><span class="co">#                                 "International aid - De-meaned OLS"),</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a><span class="co">#            omit.stat = "all", </span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a><span class="co">#            digits = 2, intercept.bottom = FALSE )</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So, what’s going on above? We have a plot where a simple regression of y on x will result in a negative coefficient - i.e.&nbsp;aid reduces growth. But, when we account for the grouping in the data with fixed effects, the relationship is positive.</p>
<p>The regression results show that we can get the same coefficients using fixed effects regressions, the plm package, or demeaning the data. The standard errors, however, are not the same!</p>
<p>Source:</p>
<ul>
<li><a href="https://ds4ps.org/pe4ps-textbook/docs/p-040-fixed-effects.html#de-meaned-ols-model" class="uri">https://ds4ps.org/pe4ps-textbook/docs/p-040-fixed-effects.html#de-meaned-ols-model</a></li>
</ul>
</section>
<section id="effective-sample" class="level1">
<h1>Effective Sample</h1>
<p>Essentially, `` Causal effects estimated via multiple regression differentially weight each unit’s contribution. The “effective sample” that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce.”</p>
<p>Our formal analysis builds on results of Angrist and Krueger (1999, 1311–12) and Angrist and Pischke (2009, chap.&nbsp;3), who show that multiple regression estimates are equivalent to weighted averages of unit-specific contributions, with the resulting multiple regression weights driven by the conditional variance of the causal factor of interest.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-12_bf48ba5b838fe665c99e78c43f712b88">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run auxillary regression, take residuals from that, square them and then you have the weightns</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>effective_sample <span class="ot">&lt;-</span> <span class="cf">function</span>(lm_aux, data, group){</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># lm_formula = "log(cumulative_sequence_length_over_10 + 1) ~ Coalition + log(totrevenue + 1)"</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># lm_formula &lt;- as.formula(lm_formula)</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>  lm_aux <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(lm_aux)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>  fit.d <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_aux, <span class="at">data=</span>data)</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>  d.tilde <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">residuals</span>(fit.d))</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> d.tilde<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> <span class="fu">tapply</span>(w, data <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">all_of</span>(group)), mean)</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(w1, <span class="fu">names</span>(w1)) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="fu">desc</span>(w1)) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">pct =</span> <span class="fu">round</span>(w1<span class="sc">/</span><span class="fu">sum</span>(w1)<span class="sc">*</span><span class="dv">100</span>,<span class="dv">2</span>))</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>  }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>See <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12185" class="uri">https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12185</a></p>
<section id="regression-in-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="regression-in-machine-learning">Regression in Machine Learning</h2>
<p>Here we care about prediction instead of explaining.</p>
<p>Sum of Squared Error <span class="math inline">\(= \sum_{i=1}^n(\hat{Y_i}-Y_i)^2\)</span></p>
<p>Mean Squared Error <span class="math inline">\(=\frac{1}{n}\sum_{i=1}^n(\hat{Y_i}-Y_i)^2\)</span></p>
<p>Here <span class="math inline">\(SSE = n \times MSE\)</span></p>
<p>We can derive OLS using MSE instead or SSE if we want. That is, we can minimize MSE instead of SSE.</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^ne_i^2 \]</span></p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^n (\hat{y_i} - y_i)^2  \]</span></p>
<p><span class="math display">\[MSE = \frac{1}{n}(Xb -y)'(Xb-y)  \]</span> Distribute the terms above so that we can take the derivative with respect to b <span class="math display">\[MSE = \frac{1}{n}( b'X'Xb - b'X'y - y'Xb + y'y) \]</span> <span class="math display">\[ MSE = \frac{1}{n}(b'X'Xb - 2b'X'y + y'y) \]</span> The above takes advantage of the fact that the transpose of a scaler is a scaler. I.e. if b is k x 1 and b’ is 1 x k then b’X’y is 1 x 1 and so is y’Xb</p>
<p><span class="math display">\[\frac{\partial}{\partial{b}} \text{MSE} = \frac{1}{n}(2X'Xb - 2X'y) \]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial{b}} \text{MSE} = \frac{2}{n}(X'Xb) - \frac{2}{n}(X'y) \]</span> <span class="math display">\[ 0 = \frac{2}{n}(X'Xb) - \frac{2}{n}(X'y) \]</span> <span class="math display">\[ 0 = X'Xb - X'y \]</span> <span class="math display">\[ X'Xb = X'y \]</span> <span class="math display">\[ b = (X'X)^{-1}X'y \]</span></p>
<p>If we only have two predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> then our error can be visualized in the following way <a href="https://allmodelsarewrong.github.io/ols.html" class="uri">https://allmodelsarewrong.github.io/ols.html</a></p>
<p><span class="math display">\[ MSE = E(\hat{y},y) = \frac{1}{n} (\beta' x' x \beta - 2 y'x\beta + y'y) \]</span>. The first term is a quadratic, the second is linear, the third is a scalar</p>
</section>
<section id="ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression">Ridge Regression</h2>
<p>Ridge is OLS with a budget <span class="math inline">\(\lambda\)</span>. The coefficients won’t drop out, like in LASSO, but they’ll tend towards 0. I.e. ridge regression does not perform variable selection. This is known as an l2 norm.</p>
<p><span class="math display">\[\beta_{rr}=(X'X + \lambda I)^{-1}X'Y\]</span></p>
<p>Ridge regression is a constrained maximization problem. Where:</p>
<p><span class="math display">\[\min_b \{\frac{1}{n}(X'X)^{-1}(X'Y)\} \text{ such that } b'b \leq c\]</span> The solution is above. How do you find c? or <span class="math inline">\(\lambda\)</span>?</p>
<p>``In ridge regression, <span class="math inline">\(\lambda\)</span> is a tuning parameter, and therefore it cannot be found analytically. Instead, you have to implement a trial and error process with various values for <span class="math inline">\(\lambda\)</span>, and determine which seems to be a good one. How? Typically with cross-validation, or other type of resampling approach.”</p>
<p>Basically: 1) Select a small <span class="math inline">\(\lambda\)</span> (if lambda is big the coefs all go to 0) 2) For each fold, (so no we’re in the second for loop), fit a ridge regression and store the errors 3) Compare all of them, take the smallest one and use that <span class="math inline">\(\lambda\)</span></p>
</section>
<section id="least-absolute-shrinkage-and-selection-operator-lasso" class="level2">
<h2 class="anchored" data-anchor-id="least-absolute-shrinkage-and-selection-operator-lasso">Least Absolute Shrinkage and Selection Operator (LASSO)</h2>
<p>This is used in variable selection. It’s also known as a l1 norm. It’s like Ridge, except it’s a constrained minimization problem subject to the norm <span class="math inline">\(|b|\leq c\)</span></p>
<p><a href="https://allmodelsarewrong.github.io/lasso.html" class="uri">https://allmodelsarewrong.github.io/lasso.html</a></p>
</section>
</section>
<section id="measurement-error" class="level1">
<h1>Measurement Error</h1>
<p>What does measurement error in X and Y do to the coefficeints?</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-13_809493b21071855a472edcf36a349adf">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"arm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: MASS</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'MASS'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:dplyr':

    select</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Matrix</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'Matrix'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:tidyr':

    expand, pack, unpack</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: lme4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
arm (Version 1.14-4, built: 2024-4-1)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Working directory is /Users/stevenrashin/Documents/GitHub/DataScienceNotes</code></pre>
</div>
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>sigma_ystar <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>sigma_x <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>fake_data <span class="ot">&lt;-</span></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">runif</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">10</span>),</span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="at">mean =</span> a <span class="sc">+</span> b<span class="sc">*</span>x, <span class="at">sd =</span> sigma),</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">y_star =</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">mean =</span> y, <span class="at">sd =</span> sigma_ystar),</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">x_star =</span>  <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>, x, sigma_x),</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a><span class="co"># now add error in y</span></span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a>no_error <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> fake_data,y<span class="sc">~</span>x)</span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a>error_y <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> fake_data,y_star<span class="sc">~</span>x)</span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a>error_x <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> fake_data,y<span class="sc">~</span>x_star)</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>error_xy <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> fake_data,y_star<span class="sc">~</span>x_star)</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a>no_error_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> fake_data, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y =</span> y )) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F) <span class="sc">+</span> <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">20</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">6</span>))</span>
<span id="cb100-25"><a href="#cb100-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-26"><a href="#cb100-26" aria-hidden="true" tabindex="-1"></a>error_y_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> fake_data, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y =</span> y_star )) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F)<span class="sc">+</span> <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">20</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">6</span>))</span>
<span id="cb100-27"><a href="#cb100-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-28"><a href="#cb100-28" aria-hidden="true" tabindex="-1"></a>error_x_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> fake_data, <span class="fu">aes</span>(<span class="at">x=</span>x_star, <span class="at">y =</span> y )) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F)<span class="sc">+</span> <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">20</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">6</span>))</span>
<span id="cb100-29"><a href="#cb100-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-30"><a href="#cb100-30" aria-hidden="true" tabindex="-1"></a>error_xy_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> fake_data, <span class="fu">aes</span>(<span class="at">x=</span>x_star, <span class="at">y =</span> y_star )) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> F)<span class="sc">+</span> <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">20</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">6</span>))</span>
<span id="cb100-31"><a href="#cb100-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-32"><a href="#cb100-32" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(no_error_plot, error_y_plot, error_x_plot,error_xy_plot, </span>
<span id="cb100-33"><a href="#cb100-33" aria-hidden="true" tabindex="-1"></a>          <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"No Error"</span>, <span class="st">"Error Y"</span>, <span class="st">"Error X"</span>,<span class="st">"Error X &amp; Y"</span>),</span>
<span id="cb100-34"><a href="#cb100-34" aria-hidden="true" tabindex="-1"></a>          <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'
`geom_smooth()` using formula = 'y ~ x'
`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="causal-quantities" class="level1">
<h1>Causal Quantities</h1>
<p>Stolen from <a href="https://mixtape.scunning.com/04-potential_outcomes#randomization-inference" class="uri">https://mixtape.scunning.com/04-potential_outcomes#randomization-inference</a></p>
<p>Note: these are all population means.</p>
<p>Average Treatment Effect (ATE)</p>
<p><span class="math display">\[\text{ATE}=\mathbb{E}[Y_{i}(1)]-\mathbb{E}[Y_{i}(0)]\]</span> <span class="math display">\[\text{ATE}=\mathbb{E}[Y_{i}^1]-\mathbb{E}[Y_{i}^0]\]</span></p>
<p>Notice, as with our definition of individual-level treatment effects, that the average treatment effect requires both potential outcomes for each unit. Since we only know one of these by the switching equation, the average treatment effect, or the ATE, is inherently unknowable. Thus, the ATE, like the individual treatment effect, is not a quantity that can be calculated. But it can be estimated</p>
<p>The second parameter of interest is the average treatment effect for the treatment group. That’s a mouthful, but let me explain. There exist two groups of people in this discussion we’ve been having: a treatment group and a control group. The average treatment effect for the treatment group, or ATT for short, is simply that population mean treatment effect for the group of units that had been assigned the treatment in the first place according to the switching equation. Insofar as differs across the population, the ATT will likely differ from the ATE. In observational data involving human beings, it almost always will be different from the ATE, and that’s because individuals will be endogenously sorting into some treatment based on the gains they expect from it. Like the ATE, the ATT is unknowable, because like the ATE, it also requires two observations per treatment unit</p>
<p>Average Treatment effect on the Treated (ATT)</p>
<p><span class="math display">\[\text{ATT}=\mathbb{E}[Y_{i}(1)|D=1]-\mathbb{E}[Y_{i}(0)|D=1]\]</span> <span class="math display">\[\text{ATT}=\mathbb{E}[Y_{i}^1|D=1]-\mathbb{E}[Y_{i}^0|D=1]\]</span></p>
<p>The final parameter of interest is called the average treatment effect for the control group, or untreated group. It’s shorthand is ATU, which stands for average treatment effect for the untreated. And like ATT, the ATU is simply the population mean treatment effect for those units who sorted into the control group.11 Given heterogeneous treatment effects, it’s probably the case that the <span class="math inline">\(\text{ATT}\neq\text{ATU}\)</span>, especially in an observational setting. The formula for the ATU is as follows:</p>
<p><span class="math display">\[\text{ATU}=\mathbb{E}[Y_{i}(1)|D=0]-\mathbb{E}[Y_{i}(0)|D=0]\]</span> <span class="math display">\[\text{ATU}=\mathbb{E}[Y_{i}^1|D=0]-\mathbb{E}[Y_{i}^0|D=0]\]</span></p>
<p>CATE</p>
<p><span class="math display">\[\text{CATE}=\mathbb{E}[Y_{i}(1)|D_i=1,X_i=x]-\mathbb{E}[Y_{i}(0)|D_i=0,X_i=x]\]</span></p>
<p>LATE</p>
<p>The LATE theorem states that under a set of basic identifying conditions, an instrumental variable identifies the average causal effect for the subpopulation of units whose treatment status is in fact moved by the instrument. Summary statistics describing this subpopulation can be computed using the kappa-weighting results of Abadie (2003). The result from Aronow and Samii (2016) described above is a LATE-type result, showing that under the relevant identifying assumptions, linear regression estimates are consistent for the average causal effect local to a subpopulation whose traits can be characterized by reweighting the nominal sample by the multiple regression weights. [link to paper]<a href="https://gregoryeady.com/ResearchMethodsCourse/assets/readings/Samii,%20Cyrus%20-%202016%20-%20Causal%20Empiricism%20in%20Quantitative%20Research.pdf" class="uri">https://gregoryeady.com/ResearchMethodsCourse/assets/readings/Samii,%20Cyrus%20-%202016%20-%20Causal%20Empiricism%20in%20Quantitative%20Research.pdf</a></p>
</section>
<section id="difference-in-differencess" class="level1">
<h1>Difference-in-Differencess</h1>
<p>Suppose that we want to isolate (i.e.&nbsp;identify) the effect of a policy on some outcome we care about.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Our question here is causal because we care about the effects of the policy itself and not whether the outcome we’re studying changed for other reasons. Some examples of causal questions are: did expanding Medicaid make people healthier? Did a tax reform stimulate investment?</p>
<p>There are a number of ways we could estimate this effect:</p>
<ul>
<li><p>We could subtract the mean value of some outcome between the treated and non-treated groups after the treatment (i.e.&nbsp;<span class="math inline">\(Y_{1,1}-Y_{0,1}\)</span>). This approach, however, does not account for differences in average outcomes between the treated and not-treated groups. That is, it doesn’t do anything about selection bias.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p></li>
<li><p>We could also look at differences in the treated units and see how much its outcome changed as a result of the treatment (i.e.&nbsp;<span class="math inline">\(Y_{1,1}-Y_{1,0}\)</span>). This approach also fails because the change might not have been due to the treatment but something else independent of the treatment</p></li>
<li><p>Note that we could classify the first two bullets here as a Pre-Post analysis</p></li>
<li><p>We could do a regression or reweighing or double machine learning models. This approach fails because you can’t rule out selection on unobservables. We would need to have data on everything that affects treatment timing and the outcome of interest (unconfoundedness assumption).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p></li>
</ul>
<p>So we need a better technique, like difference-in-differences or DiD for short. DiD is an estimator that, under certain assumptions (parallel trends and no-anticipation), identify a causal effect of an intervention on an outcome. DiD methods exploit variation in time (before vs.&nbsp;after) and across groups (treated vs.&nbsp;untreated) to recover causal effects of interest. The advantage? It allow for selection on unobservables and for time-trends.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>To start our DiD journey, we need to transform our question into a <em>target estimand</em> - a statistical representation of the part of the policy that we care about.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> In the Medicaid example above, our target estimand might be the difference in average health care spending in a state that expanded Medicaid after they expanded minus the average spending in that state if the state had not expanded Medicaid. Note that the target estimand is expressed in terms of potential outcomes.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> In our example scenario, the state that expanded has two <em>potential outcomes</em>: health care spending under the new law and health care spending without the new law. Only one of these is observable (spending with the new law); the other is unobservable because it didn’t happen (spending without the new law).</p>
<p>To estimate the effect of the law on health care spending we need data on health care spending. Our DiD algorithm that takes data as an input and produces a value of the estimand is called the estimator. The estimator’s output, given data input, is called the estimate. This value represents our best guess at the estimand, given the data we have. The goal of DiD is using an estimator (e.g.&nbsp;simple means or ordinary least squares) to produce an estimate that is the sample equivalent of the theoretical estimand (the average treatment effect on the treated or ATT).<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>To recap:</p>
<ol type="1">
<li>We have a causal question that can’t be answered convincingly (in this case causally) by a simple difference between population averages because of selection bias</li>
<li>We select an estimand (e.g.&nbsp;an ATT) that helps us answer the target question</li>
<li>We choose an estimator (difference-in-differences)</li>
<li>We gather data</li>
<li>We compute an estimate (get a numerical value of) of our estimand. Note that because the estimand relies on unobservable quantities, our estimate is our best guess of the estimand</li>
</ol>
<section id="canonical-difference-in-differences" class="level3">
<h3 class="anchored" data-anchor-id="canonical-difference-in-differences">Canonical Difference-in-Differences</h3>
<p>Note: Most of this is blatantly stolen from Roth, Jonathan, Sant’Anna, Pedro H. C., Bilinski, Alyssa, Poe, John. 2023. What’s trending in difference-in-differences? A synthesis of the recent econometrics literature. <em>Journal of Econometrics</em> (235)(8): 2218-2244<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>In the canonical difference-in-differences model, where two time periods are available, there is a treated population of units that receives a treatment of interest beginning in the second period, and a comparison population that does not receive the treatment in either period. (As as aside: the canonical design is often referred to as <span class="math inline">\(2 \times 2\)</span> which means 2 groups (treated/untreated) by 2 periods (pre-treatment/post-treatment).)</p>
<p>The key identifying assumption (i.e.&nbsp;the assumption that allows us to identify/isolate the causal effect we care about) is that the average outcome among the treated and comparison populations would have followed (1) <strong>parallel trends</strong> in the absence of treatment. We also assume that the treatment has (2) no causal effect before its implementation (<strong>no anticipation</strong>). Together, these assumptions allow us to identify the average treatment effect on the treated (ATT) (see the proof below for why we need these assumptions to derive an estimator that we can use). If we observe a large number of independent clusters from the treated and comparison populations, the ATT can be consistently estimated using a two-way fixed effects (TWFE) regression specification, and clustered standard errors provide asymptotically valid inference.</p>
<p>The figure below shows how the parallel trends assumption works and why it’s important to justify it.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-14_2c0687df3a6c7966a212520928bed1dc">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Idnefitying Assumption Visualized</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="average-treatment-effect-on-the-treated-att" class="level3">
<h3 class="anchored" data-anchor-id="average-treatment-effect-on-the-treated-att">Average Treatment effect on the Treated (ATT)</h3>
<p>The causal estimand of primary interest in the canonical DiD setup is the average treatment effect on the treated (ATT) in period t = 2. This is defined as:</p>
<p><span class="math display">\[\tau_2 = E[Y_{i,2}(1) − Y_{i,2}(0) | D_i = 1]\]</span></p>
<p>here <span class="math inline">\(Y_{\color{blue}{i},\color{red}{2}}(\color{green}{1})\)</span> is the outcome for unit <span class="math inline">\(\color{blue}{i}\)</span> in period <span class="math inline">\(\color{red}{2}\)</span> when treated <span class="math inline">\(\color{green}{1}\)</span>. It simply measures the average causal effect on treated units in the period that they are treated (t = 2). Note that this is an estimand and <strong>not</strong> an estimate because it requires a counterfactual population (<span class="math inline">\(\mathbb{E}[Y_{i,2}(0)|D=1]\)</span>) - which is the effect of an untreated population in period 2 given the population was treated.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>One difficulty in the causal inference literature is that potential outcomes are not expressed uniformly. So familiarity with different ways of expressing the same quantities is important.</p>
<p>Expressed a different way:</p>
<p><span class="math display">\[\text{ATT} \equiv \mathbb{E}[Y^1(2)-Y^0(2)|A=1]\]</span> where <span class="math inline">\(Y^\color{green}{a}(\color{red}{t})\)</span> is the potential outcome given treatment <span class="math inline">\(\color{green}{a}\)</span> at time <span class="math inline">\(\color{red}{t}\)</span>. Above, <span class="math inline">\(t=2\)</span> represents the post-treatment period, <span class="math inline">\(a=1\)</span> represents treatment and <span class="math inline">\(a=0\)</span> represents no treatment. (See, even the notation of periods is different! Above t = 0 was pre-treatment and t = 1 was post-treatment.) So what does the ATT look like in non-math terms? Suppose CT enacted a new law expanding health insurance and policymakers want to see if the law increases spending on healthcare related services. Translated literally, the equation above is Population Means[Spending in CT with the new law − Spending in CT without the new law |Given new law implemented]. You’ll immediately note that the second quantity - spending without the new law - is unobservable because the new law was enacted.</p>
<p>So how do we estimate the ATT when the some of the potential outcomes are unobservable? In diff-in-diff, we use data from the control group to impute untreated outcomes in the treated group. In the Connecticut example above, that would mean using a similar state that didn’t enact a similar law. For Medicaid this becomes tricky because comparable states like Massachusetts and Rhode Island expanded coverage. So you’d have to use a state like South Carolina and make a case that it’s similar to Connecticut.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> These control groups are the “secret sauce” of diff-in-diff because they help us learn something about the unobservable counterfactual outcomes of the treated group. In the next section I’ll show you the proof of how we can use the sample to calculate the sample analog of the estimand/ATT.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<section id="deriving-the-att" class="level4">
<h4 class="anchored" data-anchor-id="deriving-the-att">Deriving the ATT</h4>
<p>This proof shows that the ATT can be expressed as the difference in the differences between the treated and untreated observations for the canonical <span class="math inline">\(2 \times 2\)</span> case. This is useful because it shows (a) how we use the parallel trends and consistency assumptions and (b) how we go from theoretical quantities to sample analogs. Note that the steps in the proof are numbered and explanations of each step are below as needed.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<ol type="1">
<li><span class="math display">\[\text{ATT} \equiv {\mathbb{E}[Y^1(2)-Y^0(2)|A=1]}\text{ (Definition of the ATT)}\]</span></li>
</ol>
<ul>
<li>Note that none of the terms above are estimable because they all depend on potential outcomes</li>
</ul>
<ol start="2" type="1">
<li><span class="math display">\[= \underbrace{\mathbb{E}[Y^1(2)|A=1]}_{\color{green}{\text{sample analog exists}}} - \underbrace{\mathbb{E}[Y^0(2)|A=1]}_{\color{red}{\text{sample analog doesn't exist}}}\text{ (Linearity of expectations)}\]</span></li>
</ol>
<p>In the line above, expectations are linear, so we can split them. I.e. <span class="math inline">\(\mathbb{E}[x+y|A=1]=\mathbb{E}[x|A=1]+\mathbb{E}[y|A=1]\)</span>.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<ol start="3" type="1">
<li><span class="math display">\[= \underbrace{\mathbb{E}[Y^1(2)|A=1]}_{\color{green}{\text{sample analog exists}}} - \underbrace{(\mathbb{E}[Y^0(2)-Y^0(1)|A=0]}_{\color{green}{\text{sample analog exists}}} + \underbrace{\mathbb{E}[Y^0(1)|A=1]}_{\color{green}{\text{sample analog exists}}})\text{ (Counterfactual/parallel trends assumption)}\]</span></li>
</ol>
<p>In the line above, we use the parallel trends assumption. I.e. “the change in outcomes from pre- to post-intervention in the control group is a good proxy for the counterfactual change in untreated potential outcomes in the treated group.” In other words, the potential outcomes of the control and treatment groups are the same. So the treated - by assumption - aren’t behaving differently than the control would be when they get the treatment. In math notation <span class="math inline">\(\mathbb{E}[\color{green}{Y^0(2)}-\color{red}{Y^0(1)}|A=1]=\mathbb{E}[\color{green}{Y^0(2)-Y^0(1)|A=0}]\)</span>. To get from <span class="math inline">\(\color{red}{\mathbb{E}[Y^0(2)|A=1]}\)</span> in step 2 to <span class="math inline">\(\color{green}{\mathbb{E}[Y^0(2)-Y^0(1)|A=0] + \mathbb{E}[Y^0(1)|A=1]}\)</span> in step 3, first separate each term from the definition by linearity (as we did in step 1). Then rearrange.</p>
<ul>
<li><span class="math inline">\(\mathbb{E}[Y^0(2)-Y^0(1)|A=1]=\mathbb{E}[Y^0(2)-Y^0(1)|A=0]\)</span></li>
<li><span class="math inline">\(\mathbb{E}[Y^0(2)|A=1]-\mathbb{E}[Y^0(1)|A=1]=\mathbb{E}[Y^0(2)|A=0]-\mathbb{E}[Y^0(1)|A=0]\)</span></li>
<li><span class="math inline">\(\mathbb{E}[Y^0(2)|A=1]= \mathbb{E}[Y^0(2)|A=0]- \mathbb{E}[Y^0(1)|A=0] + \mathbb{E}[Y^0(1)|A=1]\)</span></li>
</ul>
<ol start="4" type="1">
<li><p><span class="math display">\[= {(\mathbb{E}[Y^1(2)-Y^0(1)|A=1]) - (\mathbb{E}[Y^0(2)-Y^0(1)|A=0] )} \text{ (Rearranging terms)}\]</span></p></li>
<li><p><span class="math display">\[= (\mathbb{E}[Y(2)-Y(1)|A=1]) - (\mathbb{E}[Y(2)-Y(1)|A=0] ) \text{ (Consistency Assumption)}\]</span></p></li>
</ol>
<p>The Consistency Assumption is written mathematically as <span class="math inline">\(Y(t)=(1−A)\times Y^0(t) + A \times Y^1(t)\)</span>, Every unit has two potential outcomes, but we only observe one — the one corresponding to their actual treatment status. The consistency assumption links the potential outcomes with the real world outcomes. That is, it links <span class="math inline">\(Y^a(t)\)</span> at time <span class="math inline">\(t\)</span> with treatment <span class="math inline">\(a\)</span> to the observed outcomes <span class="math inline">\(Y(t)\)</span>. If a unit is treated <span class="math inline">\((A=1)\)</span>, then the observed outcome is the potential outcome with treatment <span class="math inline">\(Y(t)=Y^1(t)\)</span> and the potential outcome with no treatment <span class="math inline">\(Y^0(t)\)</span> is unobserved. If a unit is not treated <span class="math inline">\((A=0)\)</span>, then <span class="math inline">\(Y(t)=Y^0(t)\)</span> and <span class="math inline">\(Y^1(t)\)</span> is unobserved</p>
<p>Let’s write this a slightly different way to show what the parallel trends assumption is doing.</p>
<p>In the proof below the <span class="math inline">\(\color{green}{\text{green}}\)</span> terms are estimable and the <span class="math inline">\(\color{red}{\text{red}}\)</span> ones are not. This idea is shamelessly stolen from <a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf" class="uri">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a>. Since the notation differs again, here <span class="math inline">\(Y_t(g)\)</span> is the potential outcome at period <span class="math inline">\(t\)</span> given units were exposed to treatment in period <span class="math inline">\(g\)</span>. If <span class="math inline">\(g=\infty\)</span> this means that the units were never exposed to treatment.</p>
<ol type="1">
<li><span class="math display">\[\text{ATT} \equiv \underbrace{\mathbb{E}[Y_{i,t=2}(2)|G_i=1]}_{\color{green}{\text{estimable from the data}}} - \underbrace{\mathbb{E}[Y_{i,t=2}(\infty)|G_i=0]}_{\color{red}{\text{counterfactual / not estimable from the data}}}\]</span></li>
</ol>
<p><span class="math display">\[= \color{green}{\mathbb{E}[Y_{i,t=2}|G_i=1]} - \color{red}{\mathbb{E}[Y_{i,t=2}(\infty)|G_i=0]} \]</span> The <span class="math inline">\(\color{green}{\text{green object}}\)</span> is estimable from data. The <span class="math inline">\(\color{red}{\text{red object}}\)</span> still depends on potential outcomes, and our goal is to find ways to ‘impute’ it. This is where PT and no-anticipation come into play!</p>
</section>
<section id="summing-up-att" class="level4">
<h4 class="anchored" data-anchor-id="summing-up-att">Summing Up ATT</h4>
<p>The causal effect we’re after is:</p>
<p><span class="math display">\[\text{ATT} = \tau_2 = \mathbb{E}[\underbrace{Y_{i,2}(1)}_{\text{Observable}}-\underbrace{Y_{i,2}(0)}_{\text{Unobservable}}\underbrace{|D=1}_{\text{ Given treated}}]\]</span></p>
<p>This is identified as:</p>
<p><span class="math display">\[\tau_2 = \underbrace{\mathbb{E}[Y_{i,2}{\color{blue}{\textbf{-}}}Y_{i,1}|D=1]}_{\text{Change for} D_i = 1} {\color{green}{\textbf{-}}} \underbrace{\mathbb{E}[Y_{i,2}{\color{blue}{\textbf{-}}}Y_{i,1}|D=0]}_{\text{Change for} D_i = 0}\]</span> This is the <span class="math inline">\(\color{green}{\text{difference}}\text{-in-}\color{blue}{\text{differences}}\)</span> of population means!</p>
<p>Good news: The static specification yields a sensible estimand when there is no heterogeneity in treatment effects across either time or units.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> That is if <span class="math inline">\(\underbrace{Y_{it}}_{\text{unit i and time t}}(\underbrace{g}_{\text{treatment}})−Y_{it}(\underbrace{\infty}_{\text{Never treated}}) \equiv \tau\)</span> then <span class="math inline">\(\beta = \tau\)</span> from <span class="math inline">\(Y_{it} =\alpha_i +\phi_t +D_{it}\beta+\epsilon_{it}\)</span>. We can estimate the causal effect of a policy on an outcome! In different words, in the simple two-period model, the estimand (population coefficient) of the two-way fixed effects specification corresponds with the ATT under the parallel trends and no anticipation assumptions.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<p>Bad news: the assumptions are rarely met. We delve into this in after discussing estimators.</p>
</section>
</section>
<section id="estimators-of-the-att" class="level3">
<h3 class="anchored" data-anchor-id="estimators-of-the-att">Estimators of the ATT</h3>
<p>So, how do we apply this to the data? For a straightforward estimate of the ATT, we could simply plug in the sample averages for the four expectations on the right-hand side:</p>
<ul>
<li><p>The post-intervention average (<span class="math inline">\(\bar{Y}_{1,2}\)</span>) of the treated group for <span class="math inline">\(\mathbb{E}[Y(2)|A=1]\)</span></p></li>
<li><p>The pre-intervention average (<span class="math inline">\(\bar{Y}_{1,1}\)</span>) of the treated group for <span class="math inline">\(\mathbb{E}[Y(1)|A=1]\)</span></p></li>
<li><p>The post-intervention average (<span class="math inline">\(\bar{Y}_{0,2}\)</span>) of the control group for <span class="math inline">\(\mathbb{E}[Y(2)|A=0]\)</span></p></li>
<li><p>The pre-intervention average (<span class="math inline">\(\bar{Y}_{0,1}\)</span>) of the control group for <span class="math inline">\(\mathbb{E}[Y(1)|A=0]\)</span></p></li>
</ul>
<p>This can be written as: <span class="math display">\[\hat{\tau}_{DiD}=(\bar{Y}_{1,2}-\bar{Y}_{1,1})-(\bar{Y}_{0,2}-\bar{Y}_{0,1})\]</span> Where <span class="math inline">\(\bar{Y}_{dt}\)</span> is sample mean for group <span class="math inline">\(d\)</span> in period <span class="math inline">\(t\)</span></p>
<p>Conveniently,<span class="math inline">\(\hat{\tau}\)</span> is algebraically equal to OLS coefficient <span class="math inline">\(\hat{\beta}\)</span> from</p>
<p><span class="math display">\[Y_{i,t} =\alpha_i +\phi_t +D_{i,t}\beta_{post}+\epsilon_{i,t}\]</span> where <span class="math inline">\(D_{i,t} =D_i \times 1[t=2]\)</span></p>
<p>This is also equivalent to a first-differences model:</p>
<p><span class="math display">\[\Delta Y_i =\alpha+\Delta D_i \beta+u_{it}\]</span></p>
<p>Note that above the <span class="math inline">\(\Delta D_i\)</span> is one for switchers (i.e.&nbsp;untreated to treated) and zero for stayers (i.e.&nbsp;undtreated to untreated). We use the <span class="math inline">\(\Delta\)</span> notation to note that this is a “first-differenced” model.</p>
<p>The code below shows that in a <span class="math inline">\(2 \times 2\)</span> these are all equivalent.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/did_9bb41929dbca1cd03f75067874cb9873">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gt)</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(skimr)</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="co"># simulated simple DID</span></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a><span class="co"># treated is the group dummy!  (if you have treated</span></span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="co"># as a dummy that only activates for the treated group</span></span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a><span class="co"># at the treated time, then your regression doesnt </span></span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a><span class="co"># run)</span></span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>treated1_pre <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a>treated1_post <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">20</span>, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a>control_pre <span class="ot">&lt;-</span>  <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a>control_post <span class="ot">&lt;-</span>  <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(treated1_pre, treated1_post, control_pre, control_post) </span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a>simulated_did_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">id =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, <span class="dv">1001</span><span class="sc">:</span><span class="dv">2000</span>, <span class="dv">1001</span><span class="sc">:</span><span class="dv">2000</span>), </span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(treated1_pre, treated1_post, control_pre, control_post),</span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">g =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">1000</span>)),</span>
<span id="cb103-22"><a href="#cb103-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">time =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">1000</span>)),</span>
<span id="cb103-23"><a href="#cb103-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">treated =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">1000</span>)),</span>
<span id="cb103-24"><a href="#cb103-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">did =</span> time <span class="sc">*</span> treated</span>
<span id="cb103-25"><a href="#cb103-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-26"><a href="#cb103-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-27"><a href="#cb103-27" aria-hidden="true" tabindex="-1"></a><span class="co"># First differnce model, show </span></span>
<span id="cb103-28"><a href="#cb103-28" aria-hidden="true" tabindex="-1"></a>first_differenced <span class="ot">&lt;-</span> simulated_did_data <span class="sc">%&gt;%</span> </span>
<span id="cb103-29"><a href="#cb103-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(id) <span class="sc">%&gt;%</span> </span>
<span id="cb103-30"><a href="#cb103-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb103-31"><a href="#cb103-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">diff =</span> y<span class="sc">-</span><span class="fu">lag</span>(y),</span>
<span id="cb103-32"><a href="#cb103-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">did =</span> treated ) <span class="sc">%&gt;%</span></span>
<span id="cb103-33"><a href="#cb103-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb103-34"><a href="#cb103-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(diff))</span>
<span id="cb103-35"><a href="#cb103-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-36"><a href="#cb103-36" aria-hidden="true" tabindex="-1"></a>first_differenced <span class="sc">%&gt;%</span> </span>
<span id="cb103-37"><a href="#cb103-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb103-38"><a href="#cb103-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">skim</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<table style="width: auto;" class="table table-condensed">
<caption>
Data summary
</caption>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
4000
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>

</table>
<p><strong>Variable type: numeric</strong></p>

<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
id
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1000.5
</td>
<td style="text-align:right;">
577.42
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
500.75
</td>
<td style="text-align:right;">
1000.50
</td>
<td style="text-align:right;">
1500.25
</td>
<td style="text-align:right;">
2000.00
</td>
<td style="text-align:left;">
▇▇▇▇▇
</td>
</tr>
<tr>
<td style="text-align:left;">
y
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
11.63
</td>
<td style="text-align:right;">
-29.14
</td>
<td style="text-align:right;">
2.01
</td>
<td style="text-align:right;">
9.89
</td>
<td style="text-align:right;">
17.60
</td>
<td style="text-align:right;">
52.36
</td>
<td style="text-align:left;">
▁▅▇▃▁
</td>
</tr>
<tr>
<td style="text-align:left;">
g
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.50
</td>
<td style="text-align:right;">
2.00
</td>
<td style="text-align:right;">
2.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
time
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
treated
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
did
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
diff
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:left;">
▁▁▇▁▁
</td>
</tr>
</tbody>

</table>
</div>
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">"one period"</span> <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">data =</span> simulated_did_data, <span class="at">formula =</span> y <span class="sc">~</span> time <span class="sc">+</span> treated <span class="sc">+</span> did),</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"first differenced"</span> <span class="ot">=</span> <span class="fu">lm</span>(<span class="at">data =</span> first_differenced,<span class="at">formula =</span> diff <span class="sc">~</span> did)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> simulated_did_data, <span class="at">formula =</span> y <span class="sc">~</span> time <span class="sc">+</span> treated <span class="sc">+</span> did)</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>fd <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> first_differenced,<span class="at">formula =</span> diff <span class="sc">~</span> did)</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Show that forumla above works </span></span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>model1<span class="sc">$</span>coefficients[<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     did 
10.20638 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>fd<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>did 
  0 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is the same!</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>(<span class="fu">mean</span>(treated1_post) <span class="sc">-</span> <span class="fu">mean</span>(treated1_pre)) <span class="sc">-</span> (<span class="fu">mean</span>(control_post)<span class="sc">-</span> <span class="fu">mean</span>(control_pre))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10.20638</code></pre>
</div>
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># FWL decomposition - for negative weights discussed later</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>aux_regression <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> simulated_did_data, <span class="at">formula =</span> did <span class="sc">~</span> time <span class="sc">+</span> treated)</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(model1<span class="sc">$</span>model<span class="sc">$</span>y, aux_regression<span class="sc">$</span>residuals)<span class="sc">/</span><span class="fu">var</span>(aux_regression<span class="sc">$</span>residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10.20638</code></pre>
</div>
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>aux_regression<span class="sc">$</span>fitted.values <span class="sc">%&gt;%</span> tibble <span class="sc">%&gt;%</span> <span class="fu">range</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.25  0.75</code></pre>
</div>
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">modelsummary</span>(models)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">one period</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">first differenced</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: center;">5.091</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.314)</td>
<td style="text-align: center;">(0.000)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">time</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.443)</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">treated</td>
<td style="text-align: center;">4.696</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.443)</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">did</td>
<td style="text-align: center;">10.206</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left; box-shadow: 0px 1.5px;"></td>
<td style="text-align: center; box-shadow: 0px 1.5px;">(0.627)</td>
<td style="text-align: center; box-shadow: 0px 1.5px;">(0.000)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Num.Obs.</td>
<td style="text-align: center;">4000</td>
<td style="text-align: center;">4000</td>
</tr>
<tr class="even">
<td style="text-align: left;">R2</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">R2 Adj.</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">AIC</td>
<td style="text-align: center;">29710.4</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">BIC</td>
<td style="text-align: center;">29741.9</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Log.Lik.</td>
<td style="text-align: center;">−14850.198</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">F</td>
<td style="text-align: center;">502.448</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSE</td>
<td style="text-align: center;">9.91</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>


</div>
</div>
<p>So, this is all well and good, right? If we have two groups and meet the assumptions of parallel trends and no anticipation then yes. If, however, we have more than two groups and more than two periods, treatment effects can change. This presents a big problem!</p>
</section>
<section id="relaxing-did-assumptions-overview" class="level3">
<h3 class="anchored" data-anchor-id="relaxing-did-assumptions-overview">Relaxing DiD Assumptions Overview</h3>
<p>The Roth et al.&nbsp;(2023) paper provides a good overview of the advances in the DiD literature. This section is largely stolen from them. I’ll cite other literature, so assume this section is Roth et al (2023) unless noted otherwise. Like the first example is from a presentation by Sant’Anna</p>
<p>Many DiD empirical applications, however, deviate from the standard DiD setup:</p>
<ul>
<li>Availability of covariates X</li>
<li>More than two time periods</li>
<li>Variation in treatment timing</li>
<li>Non-binary treatments</li>
<li>Parallel trends may not hold exactly</li>
<li>Only a few treated and untreated clusters are available</li>
</ul>
<p>We can group the recent innovations in DiD lit by which elements of the canonical model they relax:</p>
<ul>
<li>Multiple periods and staggered treatment timing</li>
<li>Relaxing or allowing PT to be violated</li>
<li>Inference with a small number of clusters</li>
</ul>
<p>A common theme is that these new estimators isolate “clean” comparisons between treated and not-yet-treated groups, and then aggregate them using user-specified weights to estimate a target parameter of economic interest.</p>
<p>Recall that in the simple two-period model, the estimand (population coefficient) of the two-way fixed effects specification corresponds with the ATT under the parallel trends and no anticipation assumptions. A substantial focus of the recent literature has been whether the estimand of commonly-used generalizations of this TWFE model to the multi-period, staggered timing case have a similar, intuitive causal interpretation. In short, the literature has shown that the estimand of TWFE specifications in the staggered setting often does not correspond with an intuitive causal parameter even under the natural extensions of the parallel trends and no-anticipation assumptions described above.</p>
<section id="estimators-that-alleviate-twfe-issues" class="level4">
<h4 class="anchored" data-anchor-id="estimators-that-alleviate-twfe-issues">Estimators that alleviate TWFE Issues</h4>
<p>Use R package did for the Callaway and Sant’Anna (2021) solution</p>
<p>This section shamelessly stolen from Pedro Sant’Anna’s presentation at Population Dynamics and Health Program Workshop at the University of Michigan: <a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf" class="uri">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a></p>
<ul>
<li><p>Callaway and Sant’Anna (2021) provides high-level conditions for one to consider more general first-step estimators that allows for covariates and some flexible “data-adaptive” (machine learning) procedures.<br>
</p></li>
<li><p>Sun and Abraham (2021): Proposed estimator coincides with CS when there are no covariates and use the never-treated/last-treated cohort as a comparison group. However, this paper has many other results about the pitfalls of TWFE that are not in CS.<br>
</p></li>
<li><p>Gardner (2021), Borusyak et al.&nbsp;(2021) and Wooldridge (2021b): Propose “imputation”/regression based methods to recover cohort-time ATT’s . These three papers do not nest nor is nested by CS, but identification assumptions are sometimes stronger. Benefit: more precise estimates when these assumptions are correct.<br>
</p></li>
<li><p>Wooldridge (2021a): Propose estimators that are suitable for nonlinear models. It relies on alternative types of parallel trends assumptions, e.g.&nbsp;’ratio-in-ratios” if exponential model. If use canonical link functions, standard errors can be easily estimated<br>
</p></li>
<li><p>de Chaisemartin and D’Haultfœuille (2020, 2021): Estimator coincides with CS when there are no covariates, uses not-yet-treated units as comparison group, and treatment is staggered. However, these two papers allow for treatment turning on-off, which is not allowed in CS. de Chaisemartin and D’Haultfœuille (2020), though, rules out dynamic treatment effects. When covariates are available, these papers do not nest nor are nested by CS. However, they seem to implicitly impose homogeneity assumptions wrt to X (e.g., ATT does not vary according to age).<br>
</p></li>
<li><p>Roth and Sant’Anna (2021): When treatment timing is as-good-as-random, we can do much better than DiD in terms of efficiency. However, it requires more than PT.</p></li>
<li><p>Callaway and Sant’Anna also propose an an alogous estimator using not-yet-treated rather than never-treated units.</p></li>
<li><p>Sun and Abraham (2021) propose a similar estimator but with different comparisons groups (e.g.&nbsp;using last-to-be treated rather than not-yet-treated)</p></li>
<li><p>Borusyak et. al.(2021), Wooldridge (2021), Gardner (2021) propose “imputation” estimators that estimate the counterfactual <span class="math inline">\(Y_{it}(0)\)</span> using a TWFE model that is fit using only pre-treatment data</p>
<ul>
<li>Main difference from C&amp;S is that this uses more pre-treatment periods,not just period g−1</li>
<li>This can sometimes bemore efficient (if outcome not too serially correlated), but also relies on a stronger PT assumption that may be more susceptible to bias</li>
</ul></li>
<li><p>Roth and Sant’Anna (2021) show that you can get even more precise estimates if you’re willing to assume treatment timing is “as good as random”</p></li>
</ul>
<p>Advice from Roth In most cases, using the “new” DiD methods will not lead to a big change in your results (empirically, TE heterogeneity is not that large in most cases) - The exceptions are cases where there are periods where almost all units are treated– this is when “forbidden comparisons” get the most weight</p>
</section>
<section id="work-in-progress-below-here" class="level4">
<h4 class="anchored" data-anchor-id="work-in-progress-below-here">Work in progress below here</h4>
</section>
<section id="multiple-periods-and-variation-in-treatment-timing" class="level4">
<h4 class="anchored" data-anchor-id="multiple-periods-and-variation-in-treatment-timing">Multiple Periods and variation in treatment timing</h4>
<p>Stolen from <a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf" class="uri">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a></p>
<p>With multiple time periods and variation in treatment timing, TWFE does not respect our assumptions: OLS is “variational hungry” and makes many comparisons of means Some of these comparisons are bad: use already-treated units as a comparison group to “later-treated” groups This can lead to “negative weighting” problems. Solution to the TWFE problem is simple Separate the identification, aggregation and estimation/inference parts of the problem Use ATT(g,t) as a building block so we can transparently see how things are constructed Many different aggregation schemes are possible: they deliver different parameters! Can allow for covariates via regressions adjustments, IPW and DR.</p>
<p>From a technical point of view (read for example Goodman-Bacon 2019), the traditional TWFE model obtains a parameter for TE that is the average of all possible 2x2 designs that could be constructed from the above matrix. However, not all of them are good ones!</p>
<ul>
<li><p>The intuition for these negative results is that the TWFE OLS specification combines two sources of comparisons:</p>
<ol type="1">
<li><p><strong>Clean comparisons:</strong> DiD’s between treated and not-yet-treated units</p></li>
<li><p><strong>Forbidden comparisons:</strong> DiD’s between two sets of already-treated units (who began treatment at different times)</p></li>
</ol></li>
<li><p>These forbidden comparisons can lead to negative weights: the “control group” is already treated, so we run into problems if their treatment effects change over time</p></li>
<li><p>Consider the two period model, except suppose now that our two groups are <strong>always-treated</strong> units (treated in both periods) and <strong>switchers</strong> (treated only in period 2)</p></li>
<li><p>With two periods, the coefficient <span class="math inline">\(\beta\)</span> from <span class="math inline">\(Y_{it} = \alpha_i + \phi_t + D_{it} \beta + \epsilon_{it}\)</span> is the same as from the first-differenced regression <span class="math inline">\(\Delta Y_i = \alpha + \Delta D_i \beta + u_i\)</span></p></li>
<li><p>Observe that <span class="math inline">\(\Delta D_i\)</span> is one for switchers and zero for stayers. That is, the stayers are the control group! Thus, <span class="math display">\[\hat{\beta}=\underbrace{(\bar{Y}_{\text{Switchers,2}}-\bar{Y}_{\text{Switchers,1}})}_{\text{Change for switchers}}-\underbrace{(\color{red}{\bar{Y}_{\text{Always Treated,2}}}-\bar{Y}_{\text{Always Treated,1}})}_{\text{Change for Always Treated}}\]</span></p></li>
<li><p>Problem: if the treatment effect for the always-treated grows over time, that will enter <span class="math inline">\(\hat{\beta}\)</span> negatively!</p></li>
<li><p>Can also show similar intuition using Frish-Waugh-Lovell (see <a href="#sec-FWL">Section&nbsp;9.1</a>)</p></li>
<li><p>The literature has placed a lot of emphasis on the fact that some treatment effects may get negative weights.</p></li>
<li><p>But even if the weights are non-negative,they might not give us the most intuitive parameter</p></li>
<li><p>For example, suppose each unit <span class="math inline">\(i\)</span> has treatment effect <span class="math inline">\(\tau_i\)</span> in every period if they are treated (no dynamics). Then <span class="math inline">\(\beta\)</span> gives a weighted average of the <span class="math inline">\(\tau_i\)</span> where the weights are largest for units treated closest to the middle of the panel</p></li>
<li><p>It is not obvious that these weights are relevant for policy, even if they are all non-negative!</p></li>
<li><p>The possibility of negative weights is concerning because, for instance, all of the treatment effects (<span class="math inline">\(\tau_s\)</span> show the treatment effect in the <span class="math inline">\(s^{th}\)</span> period after treatment) could be positive and yet the coefficient <span class="math inline">\(\beta_{post}\)</span> may be negative! To see why, take the coefficient from the regression <span class="math inline">\(\color{red}{\beta_{post}}\)</span> <span class="math inline">\(Y_{i,t} =\alpha_i +\phi_t +D_{i,t}\color{red}{\beta_{post}}+\epsilon_{i,t}\)</span>. In this case, <span class="math inline">\(\beta_{post} = \sum_s \omega_s \tau_s\)</span>. So even if all the <span class="math inline">\(\tau_s\)</span> are positive, negative weights <span class="math inline">\(\omega_s\)</span> could lead to a negative <span class="math inline">\(\beta\)</span>! In particular, longer-run treatment effects will often receive negative weights. Thus, for example, it is possible that the effect of Medicaid expansion on insurance coverage is positive and grows over time since the expansion, and yet <span class="math inline">\(\color{red}{\beta_{post}}\)</span> <span class="math inline">\(Y_{i,t} =\alpha_i +\phi_t +D_{i,t}\color{red}{\beta_{post}}+\epsilon_{i,t}\)</span> will be negative. More generally, if treatment effects vary across both time and units, then <span class="math inline">\(\tau_{i,t}(g)\)</span> may get negative weight in the TWFE estimand for some combinations of <span class="math inline">\(t\)</span> and <span class="math inline">\(g\)</span>.</p></li>
<li><p>Goodman-Bacon provides some helpful intuition to understand this phenomenon. He shows that <span class="math inline">\(\hat{\beta}_{post}\)</span> can be written as a convex weighted average of differences-in-differences comparisons between pairs of units and time periods in which one unit changed its treatment status and the other did not. Counterintuitively, however, this decomposition includes difference-in-differences that use as a ‘‘control’’ group units who were treated in earlier periods. For example, in 2016, a state that first expanded Medicaid in 2014 might be used as the ‘‘control group’’ for a state that first adopted Medicaid in 2016. Hence, an early-treated unit can get negative weights if it appears as a ‘‘control’’ for many later-treated units. This decomposition further highlights that βpost may not be a sensible estimand when treatment effects differ across either units or time, because of its inclusion of these ‘‘forbidden comparisons’’</p></li>
<li><p>This decomposition makes clear that the static OLS coefficient <span class="math inline">\(\hat{\beta}_{post}\)</span> is not aggregating natural comparisons of units, and thus will not produce a sensible estimand when there is arbitrary heterogeneity. When treatment effects are homogeneous — i.e.&nbsp;<span class="math inline">\(\tau_i\)</span>,<span class="math inline">\(t(g) \equiv \tau\)</span> — the negative weights on <span class="math inline">\(\tau\)</span> for some observations cancel out the positive weights for other observations, and thus <span class="math inline">\(\beta_{post}\)</span> recovers the causal effect under a suitable generalization of parallel trends.</p></li>
</ul>
<p>Several recent papers introduce diagnostic approaches for understanding the extent of the aggregation issues under staggered treatment timing, with a focus on the static specification (5). de Chaisemartin and D’Haultfoeuille (2020) propose reporting the number/fraction of group-time ATTs that receive negative weights, as well as the degree of heterogeneity in treatment effects that would be necessary for the estimated treatment effect to have the ‘‘wrong sign’’. Goodman-Bacon (2021) proposes reporting the weights that ˆβpost places on the different 2-group, 2-period difference-in-differences, which allows one to evaluate how much weight is being placed on ‘‘forbidden’’ comparisons of already-treated units and how removing the comparisons would change the estimate. Jakiela (2021) proposes evaluating both whether TWFE places negative weights on some treated units and whether the data rejects the constant treatment effects assumption.</p>
<ol type="1">
<li>Negative results:TWFE OLS doesn’t give us what we want with treatment effect heterogeneity</li>
<li>New estimators: perform better under treatment effect heterogeneity</li>
</ol>
<p>See e.g., <a href="http://fmwww.bc.edu/repec/frsug2022/France22_dHaultfoeuille.pdf" class="uri">http://fmwww.bc.edu/repec/frsug2022/France22_dHaultfoeuille.pdf</a> and <a href="https://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say." class="uri">https://friosavila.github.io/playingwithstata/main_didmany.html#:\~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say.</a></p>
<p>I also explain why the negative weights occur: when already-treated units act as controls, changes in their treatment effects over time get subtracted from the DD estimate. This negative weighting only arises when treatment effects vary over time, in which case it typically biases regression DD estimates away from the sign of the true treatment effect. This does not imply a failure of the underlying design, but it does caution against the use of a single-coefficient two-way fixed effects specification to summarize time-varying effects.</p>
<ul>
<li>(Steve note - they are subtracted because they’re on the right hand side of the equation!) https://www.nber.org/system/files/working_papers/w25018/w25018.pdf</li>
</ul>
<p><a href="https://stats.stackexchange.com/questions/529447/when-does-the-weight-of-dd-estimator-become-negative" class="uri">https://stats.stackexchange.com/questions/529447/when-does-the-weight-of-dd-estimator-become-negative</a></p>
<p>“We show that they estimate weighted sums of the average treatment effects (ATE) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive.”</p>
<p>“Almost 20% of empirical articles published in the AER between 2010 and 2012 use regressions with groups and period fixed effects to estimate treatment effects. In this paper, we show that under a common trends assumption, those regressions estimate weighted sums of the treatment effect in each group and period. The weights may be negative: in one application, we find that almost 50% of the weights are negative. The negative weights are an issue when the treatment effect is heterogeneous, between groups or over time. Then, one could have that the treatment’s coefficient in those regressions is negative while the treatment effect is positive in every group and time period.” See Clement de Chaisemartin and Xavier D’Haultfoeuille’s 2019 paper for more details.</p>
</section>
</section>
<section id="new-estimators" class="level3">
<h3 class="anchored" data-anchor-id="new-estimators">New Estimators</h3>
<section id="further-reading" class="level4">
<h4 class="anchored" data-anchor-id="further-reading">Further Reading</h4>
<ul>
<li><p>DiD with continuous/multi-valued treatments. Callaway, Goodman-Bacon and Sant’Anna (2021)</p></li>
<li><p>When is DiD sensitive to functional form assumptions? Roth and Sant’Anna (2022a)</p></li>
<li><p>What types of selection models are compatible with parallel trends? Ghanem, Sant’Anna and Wüthrich (2022)</p></li>
<li><p>How to incorporate Machine Learning into DiD? Chang (2020)</p></li>
<li><pre><code>What if we have multiple treatments?</code></pre>
<p>de Chaisemartin and D’Haultfœuille (2022)</p></li>
<li><p><a href="https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/" class="uri">https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/</a> for general diff-in-diff</p></li>
<li><p><a href="https://arxiv.org/pdf/2107.02637.pdf" class="uri">https://arxiv.org/pdf/2107.02637.pdf</a> for continuous treatment</p></li>
<li><p><a href="https://pjakiela.github.io/TWFE/TWFE-2021-03-24.pdf" class="uri">https://pjakiela.github.io/TWFE/TWFE-2021-03-24.pdf</a> diagnostics for two way fixed effects</p></li>
<li><p><a href="https://github.com/Mixtape-Sessions/Advanced-DID/blob/main/Slides/03-Violations.tex" class="uri">https://github.com/Mixtape-Sessions/Advanced-DID/blob/main/Slides/03-Violations.tex</a> for Jonathan Roth’s slides</p></li>
</ul>
</section>
</section>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<p>Stuff to add to this when I get around to it!</p>
<section id="question" class="level2">
<h2 class="anchored" data-anchor-id="question">Question</h2>
<p>You accidentally double every observation, what happens to beta?</p>
<p>You end up with <span class="math inline">\(\beta = 1/2 \times (X'X)^{-1}XY\)</span></p>
<p>Suppose you accidentally append the data to itself, what happens?</p>
<p>Coefficient remains the same.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-15_f43530a6fed3cff1ff0676ce33fb6e54">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> x <span class="sc">+</span> x</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">=</span> x <span class="sc">+</span> x <span class="sc">+</span> x</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">50</span> <span class="sc">+</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">4</span>)</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>doub_y <span class="ot">=</span> <span class="fu">c</span>(y,y)</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>doub_x <span class="ot">=</span> <span class="fu">c</span>(x,x)</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y<span class="sc">~</span>x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
     50.051        0.987  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y<span class="sc">~</span>x2)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x2)

Coefficients:
(Intercept)           x2  
    50.0512       0.4935  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y<span class="sc">~</span>x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x3)

Coefficients:
(Intercept)           x3  
     50.051        0.329  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(doub_y <span class="sc">~</span> doub_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = doub_y ~ doub_x)

Coefficients:
(Intercept)       doub_x  
     50.051        0.987  </code></pre>
</div>
</div>
</section>
<section id="best-linear-predictor" class="level2">
<h2 class="anchored" data-anchor-id="best-linear-predictor">Best Linear Predictor</h2>
<p>We’re going to show that, for a population, <span class="math inline">\(\beta\)</span> is the best linear predictor for y in the mean-squared error sense (i.e, it has the lowest mean squared error).</p>
<ul>
<li><a href="http://www.its.caltech.edu/~mshum/stats/natural2.pdf" class="uri">http://www.its.caltech.edu/~mshum/stats/natural2.pdf</a></li>
<li><a href="http://prob140.org/fa18/textbook/chapters/Chapter_25/02_Best_Linear_Predictor" class="uri">http://prob140.org/fa18/textbook/chapters/Chapter_25/02_Best_Linear_Predictor</a></li>
</ul>
</section>
<section id="dealing-with-data" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-data">Dealing with Data</h2>
<p>Three types of extreme values</p>
<p>1 Outlier: extreme in the y direction</p>
<p>2 Leverage point: extreme in one x direction</p>
<p>3 Influence point: extreme in both directions</p>
<p>Is the data corrupted? I Fix the observation (obvious data entry errors) I Remove the observation I Be transparent either way</p>
<p>Is the outlier part of the data generating process? I Transform the dependent variable (log(y)) I Use a method that is robust to outliers (robust regression)</p>
<p><a href="https://scholar.princeton.edu/sites/default/files/bstewart/files/lecture9handout.pdf" class="uri">https://scholar.princeton.edu/sites/default/files/bstewart/files/lecture9handout.pdf</a></p>
<p>Logistic Regression <a href="https://bookdown.org/egarpor/SSS2-UC3M/logreg-assumps.html" class="uri">https://bookdown.org/egarpor/SSS2-UC3M/logreg-assumps.html</a></p>
</section>
<section id="bias-varience-tradeoff" class="level2">
<h2 class="anchored" data-anchor-id="bias-varience-tradeoff">Bias Varience Tradeoff</h2>
<p><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="uri">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></p>
</section>
<section id="l1-and-l2-regularization-in-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="l1-and-l2-regularization-in-linear-regression">L1 and L2 Regularization in Linear Regression</h2>
<p><a href="https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf" class="uri">https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf</a> A general, HUGELY IMPORTANT problem for all machine learning algorithms • We can find a hypothesis that predicts perfectly the training data but does not generalize well to new data</p>
<p>See <a href="https://uc-r.github.io/regularized_regression" class="uri">https://uc-r.github.io/regularized_regression</a></p>
<p>Regularization is a technique that allows</p>
<p>With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection.</p>
<p>Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model.</p>
<p>However, elastic nets, and regularization models in general, still assume linear relationships between the features and the target variable.</p>
</section>
<section id="the-math" class="level2">
<h2 class="anchored" data-anchor-id="the-math">The Math</h2>
<p>The objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P).</p>
<p>I.e. in a regression we minimize the sum of squared errors (SSE) as in <span class="math inline">\(\text{min}{e'e} = \text{min}{(Y-X\beta)'(Y-X\beta)} \rightarrow \beta = (X'X)^{-1}X'Y\)</span></p>
<p>Imagine, instead we add a penalty term to the minimization problem that constrains the coefficients and progressively shrink them towards zero.</p>
<p><span class="math display">\[\text{Min{SSE - P}}\]</span> What values can P take? There are two main options - L1 and L2:<br>
- L1 - or LASSO <span class="math inline">\(P = \lambda \sum_{j=1}^{p}\)</span> |<span class="math inline">\(B_j\)</span>| - L2 - or Ridge Regression <span class="math inline">\(P = \lambda \sum_{j=1}^{p}B_j^2\)</span></p>
<p>In both cases <span class="math inline">\(\lambda\)</span> is a tuning parameter that helps to control our model from over-fitting to the training data.</p>
<div class="cell" data-hash="OLS-published-version_cache/html/unnamed-chunk-16_b5d2496f2184873de4a37a9a776a0ffa">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regularization</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge Regression in R</span></span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load libraries, get data &amp; set</span></span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a><span class="co"># seed for reproducibility </span></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)    </span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-7</code></pre>
</div>
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)   </span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(psych)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'psych'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:modelsummary':

    SD</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:arm':

    logit, rescale, sim</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:ggplot2':

    %+%, alpha</code></pre>
</div>
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"mtcars"</span>)</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Center y, X will be standardized </span></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="co"># in the modelling function</span></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> <span class="fu">tibble</span>() <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(mpg) <span class="sc">%&gt;%</span> </span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>            <span class="fu">scale</span>(<span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>) </span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(y)</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> <span class="fu">tibble</span>() <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>mpg)</span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(X)</span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform 10-fold cross-validation to select lambda</span></span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>lambdas_to_try <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting alpha = 0 implements ridge regression</span></span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, </span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a>                      <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb131-19"><a href="#cb131-19" aria-hidden="true" tabindex="-1"></a>                      <span class="at">standardize =</span> <span class="cn">TRUE</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb131-20"><a href="#cb131-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb131-21"><a href="#cb131-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot cross-validation results</span></span>
<span id="cb131-22"><a href="#cb131-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge_cv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Best cross-validated lambda</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>lambda_cv <span class="ot">&lt;-</span> ridge_cv<span class="sc">$</span>lambda.min</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit final model, get its sum of squared</span></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="co"># residuals and multiple R-squared</span></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>model_cv <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_cv,</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>y_hat_cv <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_cv, X)</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a>ssr_cv <span class="ot">&lt;-</span> <span class="fu">t</span>(y <span class="sc">-</span> y_hat_cv) <span class="sc">%*%</span> (y <span class="sc">-</span> y_hat_cv)</span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>rsq_ridge_cv <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat_cv)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a><span class="co"># selecting lambda based on the information</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(X)</span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a>aic <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a>bic <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (lambda <span class="cf">in</span> <span class="fu">seq</span>(lambdas_to_try)) {</span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Run model</span></span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a>                  <span class="at">lambda =</span> lambdas_to_try[lambda], </span>
<span id="cb132-20"><a href="#cb132-20" aria-hidden="true" tabindex="-1"></a>                  <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb132-21"><a href="#cb132-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb132-22"><a href="#cb132-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract coefficients and residuals (remove first </span></span>
<span id="cb132-23"><a href="#cb132-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># row for the intercept)</span></span>
<span id="cb132-24"><a href="#cb132-24" aria-hidden="true" tabindex="-1"></a>  betas <span class="ot">&lt;-</span> <span class="fu">as.vector</span>((<span class="fu">as.matrix</span>(<span class="fu">coef</span>(model))[<span class="sc">-</span><span class="dv">1</span>, ]))</span>
<span id="cb132-25"><a href="#cb132-25" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">&lt;-</span> y <span class="sc">-</span> (X_scaled <span class="sc">%*%</span> betas)</span>
<span id="cb132-26"><a href="#cb132-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb132-27"><a href="#cb132-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute hat-matrix and degrees of freedom</span></span>
<span id="cb132-28"><a href="#cb132-28" aria-hidden="true" tabindex="-1"></a>  ld <span class="ot">&lt;-</span> lambdas_to_try[lambda] <span class="sc">*</span> <span class="fu">diag</span>(<span class="fu">ncol</span>(X_scaled))</span>
<span id="cb132-29"><a href="#cb132-29" aria-hidden="true" tabindex="-1"></a>  H <span class="ot">&lt;-</span> X_scaled <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X_scaled) <span class="sc">%*%</span> X_scaled <span class="sc">+</span> ld) <span class="sc">%*%</span> <span class="fu">t</span>(X_scaled)</span>
<span id="cb132-30"><a href="#cb132-30" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">&lt;-</span> <span class="fu">tr</span>(H)</span>
<span id="cb132-31"><a href="#cb132-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb132-32"><a href="#cb132-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute information criteria</span></span>
<span id="cb132-33"><a href="#cb132-33" aria-hidden="true" tabindex="-1"></a>  aic[lambda] <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X_scaled) <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">t</span>(resid) <span class="sc">%*%</span> resid) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> df</span>
<span id="cb132-34"><a href="#cb132-34" aria-hidden="true" tabindex="-1"></a>  bic[lambda] <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X_scaled) <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">t</span>(resid) <span class="sc">%*%</span> resid) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> df <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">nrow</span>(X_scaled))</span>
<span id="cb132-35"><a href="#cb132-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb132-36"><a href="#cb132-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb132-37"><a href="#cb132-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot information criteria against tried values of lambdas</span></span>
<span id="cb132-38"><a href="#cb132-38" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(lambdas_to_try), aic, <span class="at">col =</span> <span class="st">"orange"</span>, <span class="at">type =</span> <span class="st">"l"</span>,</span>
<span id="cb132-39"><a href="#cb132-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">190</span>, <span class="dv">260</span>), <span class="at">ylab =</span> <span class="st">"Information Criterion"</span>)</span>
<span id="cb132-40"><a href="#cb132-40" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">log</span>(lambdas_to_try), bic, <span class="at">col =</span> <span class="st">"skyblue3"</span>)</span>
<span id="cb132-41"><a href="#cb132-41" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"orange"</span>, <span class="st">"skyblue3"</span>), </span>
<span id="cb132-42"><a href="#cb132-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"AIC"</span>, <span class="st">"BIC"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-16-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal lambdas according to both criteria</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>lambda_aic <span class="ot">&lt;-</span> lambdas_to_try[<span class="fu">which.min</span>(aic)]</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>lambda_bic <span class="ot">&lt;-</span> lambdas_to_try[<span class="fu">which.min</span>(bic)]</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit final models, get their sum of </span></span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a><span class="co"># squared residuals and multiple R-squared</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>model_aic <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_aic, </span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>y_hat_aic <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_aic, X)</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>ssr_aic <span class="ot">&lt;-</span> <span class="fu">t</span>(y <span class="sc">-</span> y_hat_aic) <span class="sc">%*%</span> (y <span class="sc">-</span> y_hat_aic)</span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>rsq_ridge_aic <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat_aic)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>model_bic <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_bic, </span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a>y_hat_bic <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_bic, X)</span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>ssr_bic <span class="ot">&lt;-</span> <span class="fu">t</span>(y <span class="sc">-</span> y_hat_bic) <span class="sc">%*%</span> (y <span class="sc">-</span> y_hat_bic)</span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a>rsq_ridge_bic <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat_bic)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a><span class="co"># The higher the lambda, the more the </span></span>
<span id="cb133-20"><a href="#cb133-20" aria-hidden="true" tabindex="-1"></a><span class="co"># coefficients are shrinked towards zero.</span></span>
<span id="cb133-21"><a href="#cb133-21" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb133-22"><a href="#cb133-22" aria-hidden="true" tabindex="-1"></a>              <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb133-23"><a href="#cb133-23" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(res, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span>
<span id="cb133-24"><a href="#cb133-24" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, </span>
<span id="cb133-25"><a href="#cb133-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">colnames</span>(X), <span class="at">cex =</span> .<span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="OLS-published-version_files/figure-html/unnamed-chunk-16-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="whats-your-estimand" class="level2">
<h2 class="anchored" data-anchor-id="whats-your-estimand">What’s your estimand</h2>
<p>“In every quantitative paper we read, every quantitative talk we attend, and every quantitative article we write, we should all ask one question: what is the estimand? The estimand is the object of inquiry—it is the precise quantity about which we marshal data to draw an inference.”</p>
<p>“Our framework stands in contrast to the currently dominant mode of quantitative inquiry: hypotheses about regression coefficients. That mode of inquiry defines the research goal inside a particular statistical model. If your research goal is a coefficient of a particular model, then you are committed to that model: it becomes impossible to reason about other approaches to achieve the goal. By contrast, we advocate a statement of the goal outside the statistical model—like an average causal effect or a population mean—which opens the door to alternative estimation procedures that could answer the research question under more credible assumptions.”</p>
<p>“We introduce a term for the goal stated outside the model—the theoretical estimand—which has two components. The first is a unit-specific quantity, which could be a realized outcome (whether person i is employed), a potential outcome (whether person i would be employed if they received job training), or a difference in potential outcomes (the effect of job training on the employment of person i). It could also be a potential outcome that would be realized under intervention of more than one variable (whether person i would be employed if they received job training and childcare), thus unlocking numerous new causal questions. The unit-specific quantity clarifies whether the research goal is causal, and if so, what counterfactual intervention is being considered. The second component of the theoretical estimand is the target population: over whom or what do we aggregate that unit-specific quantity? The unit-specific quantity and target population combine to define the theoretical estimand: the thing we would like to know if we had data for the full population in all factual or counterfactual worlds of interest. A paper may have multiple theoretical estimands.”</p>
<p>“Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g., a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not. These identification assumptions can be conveyed in a Directed Acyclic Graph (DAG). Finally, one chooses an estimation strategy to learn the empirical estimand (e.g., a regression model). We use the general term “estimands” to refer to both the theoretical and the empirical estimands.”</p>
<div id="fig-dataflow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="theory-to-estimation.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Data Analysis Workflow</figcaption>
</figure>
</div>
<p>“Too often, research papers involve pages of rich theory followed by pages of procedures applied to data, with a vague link between the two. The theoretical and empirical estimand fill the void by precisely stating both the theoretical quantity we would like to know and the empirical quantity that our procedures are most directly designed to approximate.”</p>
<p>“ A clear statement of the target population allows a researcher to clarify which approach they are taking.”</p>
<p>“The IV design offers strong causal identification at a cost: the estimated causal effect is an average not over the full population, but only over the subpopulation of compliers whose treatment status is causally affected by the instrument (Imbens and Angrist 1994).”</p>
<p>“A lack of common support arises whenever some subpopulation defined by a confounder (e.g., terrorists) contains no treated units or no untreated units (e.g., those on probation or not). Common support problems leave researchers three options. They can argue that the feasible subpopulation—those with covariates at which both treated and control units are observed—is theoretically interesting (a leap at the link between theory and the theoretical estimand); they can argue that the feasible subpopulation is informative about the broader population (a leap between the theoretical and empirical estimand); or, they can lean heavily on a parametric model and extrapolate what is observed in the feasible subpopulation to what they think would happen in the space beyond common support (a leap in estimation). As in experiments and IV, there is no free lunch. A statement of the target population is an opportunity for authors to put the difficulty in the pages of the article and clarify how they address it.”</p>
<p>“To summarize, the theoretical estimand states the study aim in precise terms involving a unit-specific quantity aggregated over a target population. The theoretical estimand exists outside of any statistical model and liberates us to make complex research questions precise. Descriptive estimands can be stated even if some of the population would refuse all survey attempts or is structurally missing from administrative records. Causal estimands can be stated in terms of counterfactuals we could never observe. In contrast to the constraints of regression coefficients, a theoretical estimand allows us to formalize the quantity most relevant to theory.”</p>
<p><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<p>“ Instead of thinking about estimating the parameters of a model, we must think of the estimation algorithm as a tool to estimate the unknown components (e.g., conditional means) that appear in the empirical estimand. Doing so allows empirical evidence to inform the choice of an estimation strategy. Conceptual argument is central to the statement of the theoretical and empirical estimands, but selection of an estimation strategy can be largely data-driven.”</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>“Econometric identification really means just one thing: model parameters or features being uniquely determined from the observable population that generates the data.” See more at <a href="http://fmwww.bc.edu/EC-P/wp957.pdf" class="uri">http://fmwww.bc.edu/EC-P/wp957.pdf</a>. Cyrus Samii defines it as “Identification refers generally to sufficiency for drawing a conclusion given the type of data that are available. [shorter link]<a href="https://uc76a733439785d112bbe0d38b98.dl.dropboxusercontent.com/cd/0/inline2/CQ61-3qehcE2gmowHjic11s015sJF6D1WJfI_poSUzkCRE9YxB8Db8l7Rj5D84svlwKIp9XQiqQ67gDu_T0hr8r8sqnKnFxqOxIdIm8M8Zt-aR2kauFd8_K9CxTtL7c9_M1kaikQZEj2XZ69YLdJE8yaQe-J5r7PROGzFXSAWWiJ5cCdo2ByxR8QZV3DjE2I4xRknw1LPeSwtIKBrJpiNQf7DU-sydqctqTQPUv6wqN4r1VPZF1Uvmip2evOt9dqZDp77xD1kxEsQV-AXjBrigqltjSVYYq2caOQELnzuIfov_TlNLPKRnW4lFoVEj5UccjZQLxlsMrJLUx0C1ZFQ4wsyZFnuPDyo0WQPtu8HQ-bpxE7fvBmTP3JeaWErj-Txmk/file" class="uri">https://uc76a733439785d112bbe0d38b98.dl.dropboxusercontent.com/cd/0/inline2/CQ61-3qehcE2gmowHjic11s015sJF6D1WJfI_poSUzkCRE9YxB8Db8l7Rj5D84svlwKIp9XQiqQ67gDu_T0hr8r8sqnKnFxqOxIdIm8M8Zt-aR2kauFd8_K9CxTtL7c9_M1kaikQZEj2XZ69YLdJE8yaQe-J5r7PROGzFXSAWWiJ5cCdo2ByxR8QZV3DjE2I4xRknw1LPeSwtIKBrJpiNQf7DU-sydqctqTQPUv6wqN4r1VPZF1Uvmip2evOt9dqZDp77xD1kxEsQV-AXjBrigqltjSVYYq2caOQELnzuIfov_TlNLPKRnW4lFoVEj5UccjZQLxlsMrJLUx0C1ZFQ4wsyZFnuPDyo0WQPtu8HQ-bpxE7fvBmTP3JeaWErj-Txmk/file</a>”<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say." class="uri">https://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say.</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf" class="uri">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf" class="uri">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This section mostly stolen from <a href="https://diff.healthpolicydatascience.org" class="uri">https://diff.healthpolicydatascience.org</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>See <a href="#sec-terms">Section&nbsp;3.2</a> for more detail<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>See Lundberg, I., Johnson, R. and Stewart, B.M., 2021. What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), pp.532-565.<a href="https://www.rebeccajohnson.io/files/asr_estimands_pdf.pdf" class="uri">https://www.rebeccajohnson.io/files/asr_estimands_pdf.pdf</a>. This article has a discussion about how to pick estimands.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>The problem, however, is that you don’t know the true real-world value of the estimand. You only have the estimate. Which, as I discussed above, is a function of the estimand, bias, and noise (i.e.&nbsp;<span class="math inline">\(\text{estimate} = \text{estimand} + \text{bias} + \text{noise}\)</span>).<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf#:~:text=Our%20starting%20point%20in%20Section%202%20is,not%20receive%20the%20treatment%20in%20either%20period." class="uri">https://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf#:~:text=Our%20starting%20point%20in%20Section%202%20is,not%20receive%20the%20treatment%20in%20either%20period.</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>This is the like Medicare spending in a state that if they didn’t expand Medicaid when they did expand Medicaid.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Note, however, there’s nothing that stops an analyst from making a stupid decision like using TX or CA as a control group.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><a href="https://diff.healthpolicydatascience.org" class="uri">https://diff.healthpolicydatascience.org</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>This proof is shamelessly stolen from <a href="https://diff.healthpolicydatascience.org" class="uri">https://diff.healthpolicydatascience.org</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>This applies to unconditional expectations too so <span class="math inline">\(\mathbb{E}[x+y]=\mathbb{E}[x]+\mathbb{E}[y]\)</span><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>This imposes that (a) all units have the same treatment effect, and (b) the treatment has the same effect regardless of how long it has been since treatment started. This would impose that the effect of Medicaid expansion on insurance coverage is the same both across states and across time. Then, under a suitable generalization of the parallel trends assumption and no anticipation assumption, the population regression coefficient <span class="math inline">\(\beta_post\)</span> from is equal to <span class="math inline">\(\tau\)</span>.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p><a href="https://github.com/Mixtape-Sessions/Advanced-DID/?tab=readme-ov-file" class="uri">https://github.com/Mixtape-Sessions/Advanced-DID/?tab=readme-ov-file</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Causal inference theory also clarifies the important distinction between a confounder and a collider. A confounder is a variable that causes both the exposure and the outcome. By contrast, a “collider” is a variable that is caused by at least two other variables (the causing variables “collide” in the collider). For example, if quality of life is affected by both smoking (exposure) and lung cancer (outcome), quality of life would be a collider and not a confounder for the association between smoking and lung cancer. This distinction is important, because methods designed to correct for confounding (e.g., regression analysis) can introduce bias if they are applied to colliders. For this reason, bias of this kind is termed “collider bias” (CB).<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>