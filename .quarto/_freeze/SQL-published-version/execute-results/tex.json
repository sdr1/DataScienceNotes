{
  "hash": "fea9a557a46b3567c4bdbd84ad06f8db",
  "result": {
    "markdown": "---\ntitle: \"SQL\"\nauthor: \"Steven Rashin\"\ndate: \"2024-04-22\"\nexecute: \n  cache: true\nformat:\n  html:\n    page-layout: full\n    toc: true\n    grid:\n      sidebar-width: 350px\n    theme: cosmo\n  pdf:\n    documentclass: scrreprt\n    include-in-header: \n      text: |\n        \\usepackage{makeidx}\n        \\makeindex\n    include-after-body: \n      text: |\n        \\printindex\n        website:\n---\n\n\n\n## Preliminaries\n\nFirst we're going to create a few datasets that we'll use to show what SQL can do\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/intro_1af08adfa69b9cff1337ce948dd2502c'}\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.4     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nhere() starts at /Users/stevenrashin/Documents/GitHub/DataScienceNotes\n\nhere() starts at /Users/stevenrashin/Documents/GitHub/DataScienceNotes\n```\n:::\n:::\n\n::: {.cell hash='SQL-published-version_cache/pdf/create data_230e178462a29bc86a84f51afb79c80a'}\n\n```{.r .cell-code}\nsample_data <- tibble(\n  id = 1:1000,\n  x1 = rnorm(1000, 0, 1),\n  x2 = rnorm(1000, 10, 15),\n  y = 3 * x1 + 4 * x2 + rnorm(1000, 0, 10),\n  g = rbinom(1000, size = 1, prob = 0.3)\n)\nsample_data2 <- tibble(\n  id = 1:1000,\n  x3 = rnorm(1000, 15, 30),\n  x4 = rnorm(1000, 20, 5),\n  y2 = 10 * x3 + 40 * x4 + rnorm(1000, 0, 40),\n  g2 = rbinom(1000, size = 1, prob = 0.3)\n)\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbWriteTable(conn = mydb, name = \"sample\", value = sample_data, overwrite = T)\ndbWriteTable(conn = mydb, name = \"other_sample\", value = sample_data2, overwrite = T)\n\n#### Try to load in postgresql - doesn't currently work\n\n# https://caltechlibrary.github.io/data-carpentry-R-ecology-lesson/05-r-and-databases.html\n# https://jtr13.github.io/cc21fall2/how-to-integrate-r-with-postgresql.html\n# https://solutions.posit.co/connections/db/databases/postgresql/\n#https://medium.com/geekculture/a-simple-guide-on-connecting-rstudio-to-a-postgresql-database-9e35ccdc08be\n```\n:::\n\n::: {.cell hash='SQL-published-version_cache/pdf/connect to sql_8e1a20b6ab0e48e002ae222db10f73ad'}\n\n```{.r .cell-code}\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbListTables(mydb) # returns a list of tables in your database\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"other_sample\" \"sample\"      \n```\n:::\n:::\n\n\n\n## Basic Syntax\n\nThe basic syntax is you `SELECT` variables `FROM` a database.\n\n$$\\underbrace{\\text{SELECT }}_{\\text{Select vars}} \\underbrace{\\text{*}}_{\\text{* is all variables}}$$\n\n$$\\underbrace{\\text{FROM }}_{\\text{from where}} \\underbrace{\\text{db\\_name}}_{\\text{name}}$$\n\nSee e.g.,\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-1_815698e12e6aff3a76602b25c8683593'}\n\n```{.sql .cell-code}\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\nSELECT *\nFROM sample\n```\n:::\n\n\n\nYou can run these commands in r using the following syntax. Since you can't run the sql commands in quarto without compiling, I've used this method to check that my sql commands actually work.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/r-to-sql_476bbacb635cba5b99c441b1eb164826'}\n\n```{.r .cell-code}\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select *\n  from \"sample\"\n  limit 10\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id           x1            x2         y g\n1   1  0.207999813 -10.762582603 -49.00510 0\n2   2 -0.778179625  16.485806479  51.53265 0\n3   3  0.002494996  -0.004209537  -6.57579 0\n4   4 -0.182489887 -19.844823245 -65.65121 0\n5   5 -0.535157187   6.138696214  16.69592 0\n6   6 -1.458613033  15.348222676  75.93028 0\n7   7 -0.881421480  -4.384157211 -23.42625 0\n8   8 -0.490183765   7.783408046  46.70411 0\n9   9 -0.784921390  25.043418192  99.61142 0\n10 10  0.998697075   6.644689810  41.48158 0\n```\n:::\n:::\n\n\n\nThis can be modified (obviously!). Suppose you need two variables, `x1` and `x2`.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-2_ab73d319a9d8c8f550fa93c6d2da86f6'}\n\n```{.sql .cell-code}\nSELECT x1, x2\nFROM sample\n```\n\n\n\n\nTable: Displaying records 1 - 10\n\n|         x1|          x2|\n|----------:|-----------:|\n|  0.2079998| -10.7625826|\n| -0.7781796|  16.4858065|\n|  0.0024950|  -0.0042095|\n| -0.1824899| -19.8448232|\n| -0.5351572|   6.1386962|\n| -1.4586130|  15.3482227|\n| -0.8814215|  -4.3841572|\n| -0.4901838|   7.7834080|\n| -0.7849214|  25.0434182|\n|  0.9986971|   6.6446898|\n:::\n\n\n\nHere's a more advanced query where we select rows where `var_1` $> 10$\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-3_e9fb554e7e20a024e134675ad163d12d'}\n\n```{.sql .cell-code}\nSELECT x1, x2\nFROM sample\nWHERE x2 >= 10\n```\n\n\n\n\nTable: Displaying records 1 - 10\n\n|         x1|       x2|\n|----------:|--------:|\n| -0.7781796| 16.48581|\n| -1.4586130| 15.34822|\n| -0.7849214| 25.04342|\n|  0.2605532| 23.62786|\n| -0.3214749| 10.09783|\n| -0.0862071| 22.05707|\n|  0.8537062| 34.13348|\n|  0.0561365| 12.32119|\n|  0.5253064| 10.87483|\n|  0.7212095| 11.57365|\n:::\n\n\n\nOften you need summaries or operations by group `var_1` $> 10$:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-4_3da53c5dc09ddf5cbc454795499e7af5'}\n\n```{.sql .cell-code}\nSELECT avg(var1)\nFROM db_name\nGROUP BY group_var\nHAVING var2 > 0\n```\n:::\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-5_d27b877cc1e3fc925e727b4307af046d'}\n\n```{.r .cell-code}\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select avg(\"x1\")\n  from \"sample\"\n  group by \"g\"\n  having x2 >= 0\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   avg(\"x1\")\n1 0.05455595\n```\n:::\n:::\n\n\n\nWhat is the difference? `HAVING` applies to groups as a whole whereas `WHERE` applies to individual rows. If you have both, the `WHERE` clause is applied first, the `GROUP BY` clause is second - so only the individual rows that meet the `WHERE` clause are grouped. The `HAVING` clause is then applied to the output. Then only the groups that meet the `HAVING` condition will appear.\n\nSuppose you need both:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/having-eg_e49c82a9a3f6aded52aa70dfe2e1aa75'}\n\n```{.sql .cell-code}\nSELECT AVG(var_3)\nFROM db_name\nWHERE var1 >= 10\nGROUP BY group_var\nHAVING var_2 > 5\n```\n:::\n\n\n\n\nAbove you'll get the average of variable 3 from $db\\_name$ only for the individual rows where $var\\_1$ is greater than 10 grouped by group_var where, within the groups their associated $var\\_2$ value is greater than 5.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-6_29a44183f691431361dc8369f13b9746'}\n\n```{.r .cell-code}\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select avg(\"y\")\n  from \"sample\"\n  where \"x1\" >= 3 \n  group by \"g\"\n  having x2 >= 0\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  avg(\"y\")\n1 65.86343\n```\n:::\n:::\n\n\n\n## Data types\n\nHere are a few common data types. For a full list go to <https://www.postgresql.org/docs/current/datatype.html>.\n\n| Data Type | What does it do?                          |\n|-----------|-------------------------------------------|\n| int       | signed four-byte integer                  |\n| numeric   | exact number. use when dealing with money |\n| varchar   | variable-length character string          |\n| time      | time of day (no time zone)                |\n| timestamp | date and time (no time zone)              |\n| date      | calendar date (year, month, day)          |\n\n: Data Types {.striped .hover }\n\nFor a technical discussion of the difference between float4 and float8 see this post: <https://stackoverflow.com/questions/16889042/postgresql-what-is-the-difference-between-float1-and-float24>.\n\n## NULLS\n\nUse `IS NOT NULL` to get rid of nulls.  Usually used after the `WHERE` clause.\n\n## Aliasing\n\nSometimes you need to alias variables. This is especially necessary when merging as you can overwrite columns that have the same name that aren't explicitly part of the merge.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-7_b3ce8fe3aca0891495185d9aaa9f1f5c'}\n\n```{.sql .cell-code}\nSELECT var AS new_var_name\nFROM ...\n```\n:::\n\n\n\nYou can also alias data frames - this is useful when you have multiple data frames.  \n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-8_cd91b3311a146c972904f303100edf8e'}\n\n```{.sql .cell-code}\nSELECT var AS new_var_name\nFROM df1 a \n```\n:::\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-9_d859ac5416c93288aaf48e8a0b5a3e67'}\n\n```{.r .cell-code}\nmydb <- dbConnect(RSQLite::SQLite(), \"sample.sqlite\")\n\ndbGetQuery(mydb,'\n  select \"g\" as \"group\", ROUND(avg(\"y\"),2) as \"new_average\"\n  from \"sample\" \"b\"\n  group by \"g\"\n  having \"x2\" >= 0\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  group new_average\n1     1       38.56\n```\n:::\n:::\n\n\n\n\n## Converting Data Types\n\nSometimes the data is in one format and you need it in another. You can use `CAST` to do this\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-10_c5acaabbd1c62cef3720f04cd8354e41'}\n\n```{.sql .cell-code}\nCAST(variable AS int/numeric/varcar/time)\n```\n:::\n\n\n\nSometimes you need to get rid of nulls, to do that use `COALESCE`\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-11_a2a0b1a62af3218d1dadf83af70b8cef'}\n\n```{.sql .cell-code}\nCOALESCE(variable, 0)\n```\n:::\n\n\n\n## Extracting\n\nA lot of times when dealing with dates you'll need a range or only part of the information given. To extract this data, you need the command `extract`.\n\nThis extracts a year from a date:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-12_55e78315d4a92db9ad2e9abd97593383'}\n\n```{.sql .cell-code}\nEXTRACT(Year from date_var)\n```\n:::\n\n\n\nThis extracts an epoch (i.e. the time difference) between the end date and the start date:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-13_7da189925cc576af238240ab42bb13ea'}\n\n```{.sql .cell-code}\nEXTRACT(EPOCH from endvar-startvar)\n```\n:::\n\n\n\nSuppose, however, that you only want the days in the epoch. That's surprisingly easy with the following code:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-14_4b49f0f0ca9a3feb34f896b86611e195'}\n\n```{.sql .cell-code}\nEXTRACT(Day from endvar-startvar)\n```\n:::\n\n\n\n## Aggregate Functions {#sec-aggregate}\n\nNote that in the table below all of the functions EXCEPT count ignore null values.\n\n| Aggregate Fcn          | What does it do?                                                                                                                                         |\n|:-------------------------|:---------------------------------------------|\n| `MIN()`                | returns the smallest value within the selected column                                                                                                    |\n| `MAX()`                | returns the largest value within the selected column                                                                                                     |\n| `COUNT()`              | returns the number of rows in a set                                                                                                                      |\n| `SUM()`                | returns the total sum of a numerical column                                                                                                              |\n| `AVG()`                | returns the *mean* value of a numerical column. Getting the median requires `PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY var1)`. See below for more info |\n| `GREATEST(var1, var2)` | Greatest rowwise among var1 and var2                                                                                                                     |\n| `LEAST(var1, var2)`    | Least rowwise among var1 and var2                                                                                                                        |\n\n: Aggregate Types {.striped .hover tbl-colwidths=\"\\[15,85\\]\"}\n\nBefore we go on, a brief digression on getting the median. For reasons known only to the creators of SQL, getting the median is fantastically difficult. Suppose you want the median as a decimal rounded to the second significant digit. You'd need to write `ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY var1)::DECIMAL, 2)`.[^1]\n\n[^1]: If you ask why, I'll give you my favorite coding answer.\n\nThe functions `MIN()` to `AVG()` operate globally but sometimes you need the biggest/smallest out of a single record (i.e. locally/within a row). You can do that with `GREATEST()` and `LEAST()`. Note that these also work on characters. An example will help clarify.\n\nSuppose you want to see the number of flights between a pair of cities (e.g. Austin and Dallas) but you don't care about where the plane begins. In this case the command `CONCAT(departure_city, '-' arrival_city)`[^2] will create **2** separate entries for Austin-Dallas and Dallas-Austin which is not what the question asked for. So you have to use `GREATEST` and `LEAST`.\n\n[^2]: See @sec-text for more details\n\nGreatest gets the highest alphabetically/greatest date/number out of a variable PER RECORD (i.e per row). Max gets the most **OVER ALL RECORDS**. Why does this difference matter? Suppose Abilene and Amarillo are in the data. Then if you used `MAX()` every row would be in the Abilene and Amarillo group.\n\nGoing back to our Dallas/Austin example, `GREATEST(departure_city, arrival_city)` would give us `Austin` and `LEAST(departure_city, arrival_city)` gives us `Dallas` in a row with a flight from Austin to Dallas. In a row with a flight from Austin to London the command would give us `Austin` and `London`. So to combine these to create a unique ID, we could type `CONCAT(GREATEST(destination_location, source_location),'-',LEAST(destination_location, source_location))` and that would give us `Austin-Dallas` whenever the these two cities appeared in destination location and source location.\n\n### Percentiles\n\nAs you could see from above, extracting the median is difficult. If you want a bunch of percentiles, the problem is even worse.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/percentiles_bd842cafe427136658fe76064ee6f290'}\n\n```{.sql .cell-code}\nSELECT \nUNNEST(array[0, 0.25, 0.5, 0.75, 1]) AS percentile, \nUNNEST(PERCENTILE_CONT(array[0, 0.25, 0.5, 0.75, 1]) within group (order by var1)) <---- Note that unnest wraps the whole thing!  \nFROM ...\n\n```\n:::\n\n\n\nYou have two options for calculating percentiles `PERCENTILE_CONT` and `PERCENTILE_DISC`. `PERCENTILE_CONT` will interpolate values while `PERCENTILE_DISC` will give you values only from your data. Suppose you have 2,3,4,5. PERCENTILE_CONT would give you 3.5, PERCENTILE_DISC gives you 3.\n\n## Merging\n\nThere are four types of joins:\n\n-   INNER JOIN/JOIN\n    -   Joins all common records between tables\n-   LEFT JOIN\n    -   Joins the matching records in the right frame (i.e. the one after the LEFT JOIN clause) with all the records in the left frame (i.e. the one after the FROM clause)\n-   RIGHT JOIN\n    -   Joins all of the records in the right frame (i.e. the one after the RIGHT JOIN clause) with all the matching records in the left frame (i.e. the one after the FROM clause)\n-   FULL JOIN\n    -   All the records in both frames\n\nThese joins are all on some variable.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/join_6b771785460e39839f27440b66bcf0e1'}\n\n```{.sql .cell-code}\nSELECT a.var1, a.var2, a.id, b.var3, b.var4, b.id1 <---- note the aliases up here.  This is good practice to show where you're getting each variable from\nFROM df1 a <---- alias dataframe 1 as a\nINNER JOIN df2 b <----- alias dataframe 2 as b\nON a.id = b.id\n```\n:::\n\n\n\nYou can use `OR` in the join to join on either value.  `AND` works too.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/join2_ddf5899e9564b4a568054fcaf895095f'}\n\n```{.sql .cell-code}\nSELECT a.var1, a.var2, a.id, a.id2, b.var3, b.var4, b.id1, b.alt_id <---- note the aliases up here.  This is good practice to show where you're getting each variable from\nFROM df1 a <---- alias dataframe 1 as a\nINNER JOIN df2 b <----- alias dataframe 2 as b\nON a.id = b.id OR a.var1 = b.alt_id <---- this joins if EITHER is true\n```\n:::\n\n\n\n\n-   UNION\n    -   Concatenates queries. Does not allow duplicates\n    \n\n\n::: {.cell hash='SQL-published-version_cache/pdf/union-example_fcba6c63894219ffc5198c82d6d155b8'}\n\n```{.sql .cell-code}\nSELECT ...\nFROM ...\nUNION\nSELECT ...\nFROM ...\n```\n:::\n\n\n    \n-   UNION ALL\n    -   Concatenates queries. Allows duplicates\n\n\n\n## Window Functions\n\nSuppose you need to do something within a window like find all flights within 3 days. Here you need a window function. The basic syntax is as follows:\n\n$\\underbrace{\\dots}_{\\text{Some fcn}} \\text{OVER} ($\n\n$\\hspace{0.5cm}\\underbrace{\\text{PARTITION BY} {\\color{blue}{\\text{var1}}}}_{\\text{group by }{\\color{blue}{\\text{var1}}}}$\n\n$\\hspace{0.5cm}\\underbrace{\\text{ORDER BY} {\\color{green}{\\text{var2}}}}_{\\text{order by }{\\color{green}{\\text{var2}}}}$ $) \\text{ AS newvar}$\n\nIn addition to the functions in @sec-aggregate, here are a bunch of useful functions. For a more comprehensive list, go to <https://www.postgresql.org/docs/current/functions-window.html>\n\n| Window Function | What does it do?                                                                                                                             |\n|:-------------------------|:---------------------------------------------|\n| `lag()`         | lags the data 1, lag(2) would lag 2 rows                                                                                                     |\n| `lead()`        | opposite of lag()                                                                                                                            |\n| `rank()`        | ranks rows. Suppose you have two rows tied for first, the rankings would go 1,1,3                                                            |\n| `dense_rank()`  | ranks rows without skipping.Suppose you have two rows tied for first, the rankings would go 1,1,2                                            |\n| `ntile()`       | splits the data into n groups, indexed by an integer, as equally as possible. `ntile(4)` for example, gives us quartiles if ordered properly |\n| `cume_dist()`   | cumulative distribution                                                                                                                      |\n\n: Window Functions {.striped .hover tbl-colwidths=\"\\[15,85\\]\"}\n\n### Bounding window functions\n\nSometimes you need to search within a certain **window** in a group as opposed to within an entire group. Suppose we wanted a moving average within the last three years. We could do that by properly bounding our query. The bounds come after the `ORDER BY` clause.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-15_c7f68335b6216137b58a6d2d0805f025'}\n\n```{.sql .cell-code}\nSELECT AVG(var1) OVER  <---- give us the mean of variable 1\n (  \n PARTITION BY country <---- group by country\n ORDER BY year desc <---- order by year descending\n ROWS BETWEEN 2 PRECEDING AND CURRENT ROW <---- gives us the last 3 years.  includes the current row.\n)\nFROM df1\n```\n:::\n\n\n\nWe can be fairly creative with the bounds using the following building blocks:\n\n| Bounds                        | What does it do?                                                    |\n|:-------------------------|:---------------------------------------------|\n| `n PRECEDING`                 | 2 Preceding gives us the 2 prior rows not including the current row |\n| `UNBOUND PRECEDING`           | All rows up to but not including the current row                    |\n| `CURRENT ROW`                 | Just the current row                                                |\n| `n PRECEDING AND CURRENT ROW` | n preceding and the current row                                     |\n| `n FOLLOWING`                 | n following but not including the current row                       |\n| `UNBOUND FOLLOWING`           | n preceding and the current row                                     |\n\n## Conditionals\n\nSometimes you don't just need an average, you need a conditional average or sum. This can be done with the `FILTER` command\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-16_ac1114566c7e59a860f444e856f1ca87'}\n\n```{.sql .cell-code}\nsum(var) FILTER(WHERE ...) AS ...\n```\n:::\n\n\n\nSometimes you need to create a new variable based on the values of other variables. You do that with `CASE WHEN`\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-17_3921551026ac7526a7f1e9fe5552cff7'}\n\n```{.sql .cell-code}\nCASE \n  WHEN [condition] THEN [result]\n  WHEN [condition2] THEN [result2]\n  ELSE [result3] END AS new_conditional_variable  \n```\n:::\n\n\n\nNote that the `CASE WHEN ...` can be used in the `GROUP BY` command to create groups.\n\n## Dates\n\nDates are difficult to deal with. You just have to memorize these commands.\n\n| Function                                               | What does it do?              |\n|:-------------------------|:---------------------------------------------|\n| `MAKE_DATE(year,month,day)`                            | makes dates                   |\n| `MAKE_TIMESTAMP(year,month,day, hour, minute, second)` | makes timestamps              |\n| `MAKE_INTERVAL(year,month,day, hour, minute, second)`  | makes intervals               |\n| `DATE(datetime_var)`                                   | extracts dates from datetimes |\n\n: Datetime Functions {.striped .hover tbl-colwidths=\"\\[55,45\\]\"}\n\nThere are variations. Suppose you wanted to find all processes that lasted less than 10 days. You could use the command `MAKE_INTERVAL(days <= 10)`\n\n## Common Table Expressions {#sec-cte}\n\nSometimes you need to create a separate table that you can then extract data from to avoid conflicts like using a window function in the `WHERE` clause. You do this with a common table expression (CTE).\n\nThe basic syntax is as follows:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/cte-plain_64d3962d28c51184f7a573a83f2ec7e7'}\n\n```{.sql .cell-code}\nWITH cte1 AS (\nSELECT ...\nFROM db1\n)\n\nSELECT ...\nFROM ... (likely db1 or db2)\n```\n:::\n\n\n\nYou're not limited to one CTE, you can have multiple if you want:\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/ctwx2_10a2296315d1581c7d67ecb22599c128'}\n\n```{.sql .cell-code}\nWITH RECURSIVE cte1 AS (\nSELECT ...\nFROM db1\n), <---- need the parenthesis and comma here otherwise you get an error!\ncte2 AS (\nSELECT ...\nFROM db2\n)\n\nSELECT ...\nFROM ... (likely db1 or db2)\n```\n:::\n\n\n\nBelow is an an example of using CTEs to avoid a window function in the `WHERE` clause.[^3] The query is from a problem that asks you to find the second longest flight between two cities. This is a bit of a tricky problem because it requries a window function, concatentaion, ordering text strings, and a common table expression.\n\n[^3]: See <https://www.interviewquery.com/questions/second-longest-flight> for full problem details\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-18_37c137cd48c7e3ea0a058ea005b4b10c'}\n\n```{.sql .cell-code}\nWITH tmp AS (SELECT \nid, \ndestination_location,\nsource_location,\nflight_start,\nflight_end,\ndense_rank() OVER (\n    PARTITION BY CONCAT(GREATEST(destination_location, source_location),'.',LEAST(destination_location, source_location))\n    ORDER BY extract(epoch from flight_end - flight_start) desc\n) AS flight_duration\nFROM flights)\n\nSELECT id, destination_location, source_location, flight_start, flight_end\nFROM tmp\nwhere flight_duration = 2\norder by id\n```\n:::\n\n\n\nLet's go through this, starting with the structure. Here we're creating a common table expression called cte and then using it\n\n$\\text{WITH } {\\color{red}{\\text{tmp}}} \\text{ AS (} \\leftarrow \\text{Create CTE called } \\color{red}{\\text{tmp}}$\\\n$\\hspace{1cm}\\text{SELECT} \\dots \\leftarrow \\text{Standard SQL commands in here}$\\\n$\\hspace{1cm}\\text{FROM} \\dots$\\\n$) \\leftarrow \\text{Close out CTE}$\n\n$\\text{SELECT} \\dots$\\\n$\\text{FROM } {\\color{red}{\\text{tmp}}} \\leftarrow \\text{Use } {\\color{red}{\\text{tmp}}} \\text{ as a normal table}$\n\nThe other tricky bit is the window function. Here we're using a dense rank (i.e. ranking everything sequentially so if two are tied for first the ranks are 1,1,2) by city pair (that's the variable after partition by) ordering those groups by flight time descending.\n\n## Subqueries\n\nSuppose you need a one off query within a query. You can use a sub query! The basic syntax is below. You can put them anywhere. For example, here's one in the `FROM` clause. Note that you need to alias your subqueries or else they'll fail\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-19_47445db811e3a01f7ecae75e591fc534'}\n\n```{.sql .cell-code}\nSELECT ...\nFROM (\n  SELECT *\n  FROM db1\n) AS db2\n```\n:::\n\n\n\nYou could, however, do one in the `SELECT` clause if you wanted\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-20_5e865b643c7eb9f6cfc5109f99b64f11'}\n\n```{.sql .cell-code}\nSELECT (\n  SELECT ...\n  FROM ...\n)\nFROM db1\n```\n:::\n\n\n\nSo when to use common table expressions versus subtables? Common table expressions are, generally, preferred because they're more readable and can be used multiple times. See, e.g., below, where we havethe same query as @sec-cte but with a subquery instead of a common table expression.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/unnamed-chunk-21_19a45f237868e26153ed7a3021ffe076'}\n\n```{.sql .cell-code}\n\nSELECT \nid, \ndestination_location,\nsource_location,\nflight_start,\nflight_end\nFROM (\n    SELECT \n    *,  \ndense_rank() OVER (\n    PARTITION BY CONCAT(GREATEST(destination_location, source_location),'.',LEAST(destination_location, source_location))\n    ORDER BY extract(epoch from flight_end - flight_start) desc) AS duration_rank\n    FROM flights\n    ) AS subquery\n\nWHERE duration_rank = 2\norder by id\n```\n:::\n\n\n\n### Subquery Joins\n\nSometimes you need to do something to a database before joining it. Here a subquery join is helpful\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/sub-q-join_3451250f63667f9ce695e6b2cd841672'}\n\n```{.sql .cell-code}\nSELECT user_id, var1, var2 \nFROM  db1\nLEFT JOIN \n    ( <---- begin subquery \n        SELECT id, var3, var4\n        FROM db2\n        WHERE ... <-----  you're using a subquery because you need to do something so I've included the where in here \n    ) AS a <--- end and alias subquery \nON db1.user_id = a.id <---- join on subquery name!\n```\n:::\n\n\n\n## Text {#sec-text}\n\nSometimes you're faced with a task where you have to concatenate (i.e. join) or split variables. I've seen this problem when trying to match pairs of cities for flights when the question only cares about the pair and not the ordering.\n\n$\\text{CONCAT(var1, var2)}$\n\n$\\underbrace{\\text{SPLIT\\_PART}}_{\\text{split apart}}(\\underbrace{{\\color{red}{\\text{var}}, {\\color{blue}{\\text{'.'}}}, {\\color{green}{\\text{1}}}}}_{\\text{split apart}{\\color{red}{\\text{ var}}} \\text{ on}{\\color{blue}{\\text{ a period}}} \\text{ taking }{\\color{green}{\\text{the first instance}}} })$\n\n## Random Sample\n\nThe method below selects 10% of the data.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/random-sample_fad427201f3f3b5e829e394ae476e4b9'}\n\n```{.sql .cell-code}\nSELECT ...\nFROM ...\nTABLESAMPLE BERNOULLI(10)\n```\n:::\n\n\n\n## Summary Statistics\n\nGetting summary statistics is a pain the ass. So here is some code that will do it for a variable called $a$ in database $t1$. The difficulty of the code is that it requires two separate common table expressions. Here's a sketch of what that looks like. Note that adding RECURSIVE to the with query allows you to select from db1 in the db2 query.\n\n\n\n::: {.cell hash='SQL-published-version_cache/pdf/show-cte-x2-again_18dd7ef8a417e0bdcb1575ef47fa671f'}\n\n```{.sql .cell-code}\nWITH RECURSIVE cte1 AS (\nSELECT ...\nFROM db1\n),\ncte2 AS (\nSELECT ...\nFROM db2\n)\n\nSELECT ...\nFROM ... (likely db1 or db2)\n```\n:::\n\n::: {.cell hash='SQL-published-version_cache/pdf/summary-stats_380d1fb2c41a4a44f59644cc0a115492'}\n\n```{.sql .cell-code}\nWITH RECURSIVE\nsummary_stats AS\n(\n SELECT \n  ROUND(AVG(a), 2) AS mean,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY a) AS median,\n  MIN(a) AS min,\n  MAX(a) AS max,\n  MAX(a) - MIN(a) AS range,\n  ROUND(STDDEV(a), 2) AS standard_deviation,\n  ROUND(VARIANCE(a), 2) AS variance,\n  PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY a) AS q1,\n  PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY a) AS q3\n   FROM t1\n),\nrow_summary_stats AS\n(\nSELECT \n 1 AS id, \n 'mean' AS statistic, \n mean AS value \n  FROM summary_stats\nunion all\nSELECT \n 2, \n 'median', \n median \n  FROM summary_stats\nUNION\nSELECT \n 3, \n 'minimum', \n min \n  FROM summary_stats\nUNION\nSELECT \n 4, \n 'maximum', \n max \n  FROM summary_stats\nUNION\nSELECT \n 5, \n 'range', \n range \n  FROM summary_stats\nUNION\nSELECT \n 6, \n 'standard deviation', \n standard_deviation \n  FROM summary_stats\nUNION\nSELECT \n 7, \n 'variance', \n variance \n  FROM summary_stats\nUNION\nSELECT \n 9, \n 'Q1', \n q1 \n  FROM summary_stats\nUNION\nSELECT \n 10, \n 'Q3', \n q3 \n  FROM summary_stats\nUNION\nSELECT \n 11, \n 'IQR', \n (q3 - q1) \n  FROM summary_stats\nUNION\nSELECT \n 12, \n 'skewness', \n ROUND(3 * (mean - median)::NUMERIC / standard_deviation, 2) AS skewness \n  FROM summary_stats\n)\nSELECT * \n FROM row_summary_stats\n  ORDER BY id;\n```\n:::\n\n\n\n## Execution Order\n\n1.  JOIN\n    -   If no JOIN, then we start at FROM\n2.  FROM\n3.  WHERE\n4.  GROUP BY\n5.  HAVING\n6.  SELECT\n7.  DISTINCT\n8.  ORDER BY\n9.  LIMIT/OFFSET\n\n### More Efficient Code\n\n-   <https://nodeteam.medium.com/how-to-optimize-postgresql-queries-226e6ff15f72>\n-   Properly index\\* columns used in WHERE and JOIN conditions.\n    -   Index will create a pointer to the actual rows in the specified table.\n    -   <https://www.postgresql.org/docs/current/sql-createindex.html>\n-   Use appropriate data types and avoid unnecessary data type conversions.\n-   Limit the use of SELECT \\* and only retrieve the columns you need.\n-   Minimize the use of subqueries and consider JOIN alternatives.\n-   Monitor and analyze query performance using database-specific tools and profiling.\"\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}